


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <!-- ========== Start: SEO & Schema Enhancement ========== -->

    <!-- Title: Clear and specific to the page's function -->
    <title>Post-Quantum Cryptography - Course Content | CipherHall</title>

    <!-- Meta Description: Explains the page's purpose for users who might land here directly -->
    <meta name="description" content="Access the complete curriculum for the Post-Quantum Cryptography course. Log in to track your progress, complete lessons, and take quizzes at CipherHall." />

    <!-- Robots Tag: CRITICAL. This tells search engines not to index this page. -->

    
    <!-- Canonical URL: A self-referencing link to indicate this is the original version of this page's content. -->
    <link rel="canonical" href="https://www.cipherhall.com/courses/post-quantum-cryptography.html" />

    <!-- Schema Markup: Provides context about the course, even if the page isn't indexed. -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Course",
      "name": "Post-Quantum Cryptography - Complete Learning Roadmap",
      "description": "An in-depth course on the future of cryptography, from the fundamentals of quantum computing to the design and analysis of quantum-resistant algorithms.",
      "provider": {
        "@type": "Organization",
        "name": "CipherHall",
        "sameAs": "https://www.cipherhall.com"
      },
      "hasCourseInstance": {
        "@type": "CourseInstance",
        "courseMode": "Online",
        "instructor": {
          "@type": "Person",
          "name": "Dr. Anya Sharma"
        }
      }
    }
    </script>
    <!-- ========== End: SEO & Schema Enhancement ========== -->

    <!-- Favicon -->
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon.png" />
    <link rel="shortcut icon" href="/favicon.png" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon.png" />

    <!-- Fonts, CSS, and JS -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&family=Exo+2:wght@700;800;900&display=swap" rel="stylesheet" />
    <script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js@2.39.7/dist/umd/supabase.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css" />
    <link rel="stylesheet" href="assets/css/coursepages.css">
</head>
  <body>
    <!-- Loading Screen -->
    <div id="loadingScreen" class="loading-screen">
      <div class="loader-icon">
        <i class="fas fa-graduation-cap"></i>
      </div>
      <div class="loader-text">Loading Course Content...</div>
    </div>

    <!-- Sidebar Overlay for Mobile -->
    <div class="sidebar-overlay" id="sidebarOverlay"></div>

    <!-- Music Player (fixed at top right) -->
    <div class="header-controls">
      <div class="music-dropdown" id="music-dropdown">
        <button class="btn btn-secondary">
          <span id="music-button-text">STUDY MUSIC</span>
          <i id="music-icon" class="fa-solid fa-music"></i>
        </button>
        <div class="music-dropdown-content" id="music-dropdown-content">
          <a
            href="#"
            data-src="https://www.learningcontainer.com/wp-content/uploads/2020/02/Kalimba.mp3"
            >1. Coffee Shop Vibes</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-16.mp3"
            >2. City Lights Lofi</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-15.mp3"
            >3. Mellow Thoughts</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-13.mp3"
            >4. Rainy Mood</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-10.mp3"
            >5. Time Alone</a
          >
          <a href="#" id="stop-music-link"
            ><i class="fa-solid fa-stop-circle"></i> Stop Music</a
          >
        </div>
      </div>

      <button class="btn btn-secondary" id="resetProgressBtn">
        <i class="fas fa-redo"></i>
        Reset Progress
      </button>
    </div>

    <!-- Auth Modal -->
    <div id="authModal" class="modal" style="display: none">
      <div class="modal-overlay"></div>
      <div class="modal-content">
        <span class="close" id="closeAuthModal">&times;</span>
        <div class="auth-tabs">
          <button id="tabSignIn" class="auth-tab active">Sign In</button>
          <button id="tabSignUp" class="auth-tab">Sign Up</button>
        </div>
        <div
          id="authLoader"
          style="display: none; text-align: center; padding: 2rem"
        >
          <i
            class="fas fa-spinner fa-spin fa-2x"
            style="color: var(--color-green)"
          ></i>
          <p style="margin-top: 0.5rem">Authenticating...</p>
        </div>
        <div id="signInForm" class="auth-form">
          <h2>Sign In to CipherHall</h2>
          <button id="googleSignIn" class="btn btn-google">
            <i class="fab fa-google"></i> Continue with Google
          </button>
          <div class="divider"><span>or</span></div>
          <form id="emailSignInForm">
            <input
              type="email"
              id="signInEmail"
              placeholder="Email Address"
              required
            />
            <input
              type="password"
              id="signInPassword"
              placeholder="Password"
              required
            />
            <button type="submit" class="btn btn-primary">Sign In</button>
          </form>
        </div>
        <div id="signUpForm" class="auth-form" style="display: none">
          <h2>Join CipherHall</h2>
          <button id="googleSignUp" class="btn btn-google">
            <i class="fab fa-google"></i> Continue with Google
          </button>
          <div class="divider"><span>or</span></div>
          <form id="emailSignUpForm">
            <input
              type="text"
              id="signUpName"
              placeholder="Full Name"
              required
            />
            <input
              type="email"
              id="signUpEmail"
              placeholder="Email Address"
              required
            />
            <input
              type="password"
              id="signUpPassword"
              placeholder="Password (min. 8 characters)"
              required
            />
            <button type="submit" class="btn btn-secondary">
              Create Account
            </button>
          </form>
        </div>
        <div id="authMessage"></div>
      </div>
    </div>

    <!-- Sidebar -->
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <div class="course-info">
          <h1 class="course-title" id="courseTitle">Loading...</h1>
          <div class="course-progress">
            <div class="progress-header">
              <span class="progress-label">Course Progress</span>
              <span class="progress-percentage" id="courseProgressPercent"
                >0%</span
              >
            </div>
            <div class="progress-bar">
              <div
                class="progress-fill"
                id="courseProgressFill"
                style="width: 0%"
              ></div>
            </div>
            <div class="progress-stats">
              <span id="completedLessons">0</span>
              <span id="totalLessons">0</span>
            </div>
          </div>
          <a href="/dashboard.html" class="back-to-dashboard">
            <i class="fas fa-arrow-left"></i>
            Back to Dashboard
          </a>
        </div>
      </div>

      <nav class="lesson-nav" id="lessonNav">
        <!-- Lessons will be loaded dynamically -->
      </nav>
    </aside>

    <!-- Main Content -->
    <main class="main-content">
      <header class="content-header">
        <div class="lesson-header">
          <div class="lesson-header-left">
            <span class="lesson-number" id="currentLessonNumber">1</span>
            <h1 class="lesson-header-title" id="currentLessonTitle">
              Loading...
            </h1>
          </div>
          <div class="lesson-actions">
            <button class="mbtn btn-primary" id="menuToggle">
              <i class="fas fa-bars"></i>
            </button>
          </div>
        </div>
      </header>

      <div class="content-body" id="contentBody">
        <div class="lesson-content" id="lessonContent">
          <!-- Lesson content will be loaded dynamically -->
        </div>
      </div>

      <footer class="lesson-navigation">
        <div class="nav-info" id="navInfo">Lesson 1 of 10</div>
        <div class="nav-controls">
          <button class="btn btn-secondary" id="prevLessonBtn" disabled>
            <i class="fas fa-chevron-left"></i>
            Previous
          </button>
          <button class="btn btn-primary" id="nextLessonBtn" disabled>
            Next
            <i class="fas fa-chevron-right"></i>
          </button>
        </div>
      </footer>
    </main>

    <!-- Achievement Notification -->
    <div id="achievementNotification" class="achievement-notification">
      <div class="notification-header">
        <div class="notification-icon" id="notificationIcon">
          <i class="fas fa-trophy"></i>
        </div>
        <div class="notification-content">
          <h3 id="notificationTitle">Achievement Unlocked!</h3>
          <p id="notificationDescription">Description here...</p>
        </div>
      </div>
      <div class="notification-reward" id="notificationReward">
        <i class="fas fa-coins"></i>
        <span id="notificationPoints">+100 Points</span>
      </div>
    </div>

    <!-- Toast Notification -->
    <div id="toast" class="toast">
      <span id="toastMessage"></span>
    </div>

    <script>
      // =====================================================
      // SYNCHRONIZED COURSE SYSTEM WITH DASHBOARD INTEGRATION
      // =====================================================

      // Supabase Configuration - Replace with your config
      const supabaseUrl = "https://lzcmzulemfubjaksoqor.supabase.co";
      const supabaseKey =
        "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imx6Y216dWxlbWZ1Ympha3NvcW9yIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTUzMzM5MDgsImV4cCI6MjA3MDkwOTkwOH0.crhPH4YWtRsr1BJTpAQcmPbWSpwIWRbzoI4sRb2_fPI";
      const supabase = window.supabase.createClient(supabaseUrl, supabaseKey);

      // =====================================================
      // COURSE DATA STRUCTURE - REPLACE WITH YOUR COURSE JSON
      // =====================================================
      const COURSE_DATA = 
{
    "id": "post-quantum-cryptography",
    "title": "Post-Quantum Cryptography - Complete Learning Roadmap",
    "description": "An in-depth course on the future of cryptography, from the fundamentals of quantum computing to the design and analysis of quantum-resistant algorithms.",
    "category": "cryptography",
    "difficulty": "Advanced",
    "duration": "120 hours",
    "instructor": "Dr. Anya Sharma",
    "lessons": [
        {
            "id": "lesson-1",
            "title": "Quantum Computing Fundamentals",
            "duration": "90 min",
            "objectives": [
                "Understand the basic principles of quantum mechanics relevant to cryptography",
                "Define qubits, superposition, and entanglement",
                "Learn the function of quantum gates and the structure of quantum circuits",
                "Get a high-level overview of key quantum algorithms",
                "Grasp the concept of quantum advantage and its complexity implications"
            ],
            "content": {
                "overview": "This lesson introduces the fundamental concepts of quantum computing that are essential for understanding its threat to modern cryptography. We will move beyond classical bits to the world of qubits, superposition, and quantum gates, laying the groundwork for analyzing quantum algorithms.",
                "sections": [
                    {
                        "title": "Quantum Bits (Qubits) and Quantum States",
                        "content": "<p>A classical bit is in a state of either 0 or 1. A <strong>qubit</strong> (quantum bit) is a quantum system that can exist in a <strong>superposition</strong> of both states simultaneously. Its state can be represented as α|0⟩ + β|1⟩, where α and β are complex numbers representing probability amplitudes.</p><h3>Key Concepts:</h3><ul><li><strong>Superposition:</strong> The ability of a qubit to be in multiple states at once. This is the source of a quantum computer's massive parallelism.</li><li><strong>Entanglement:</strong> A quantum mechanical phenomenon in which the quantum states of two or more qubits are linked. The state of one entangled qubit cannot be described independently of the state of the others, even when separated by large distances.</li><li><strong>Measurement:</strong> When we measure a qubit in superposition, its state collapses to a definite 0 or 1, with a probability determined by its amplitudes.</li></ul>",
                       
                    },
                    {
                        "title": "Quantum Gates and Quantum Circuits",
                        "content": "<p>Just as classical computers use logic gates (AND, OR, NOT) to manipulate bits, quantum computers use <strong>quantum gates</strong> to manipulate qubits. These gates are represented by unitary matrices and are reversible, unlike many classical gates.</p><h3>Common Quantum Gates:</h3><ul><li><strong>Hadamard Gate (H):</strong> Puts a qubit into an equal superposition of |0⟩ and |1⟩.</li><li><strong>Pauli Gates (X, Y, Z):</strong> Analogous to classical bit-flip and phase-flip operations.</li><li><strong>CNOT Gate (Controlled-NOT):</strong> A two-qubit gate that is crucial for creating entanglement.</li></ul><p>A <strong>quantum circuit</strong> is a sequence of quantum gates applied to a register of qubits. It represents a quantum computation.</p>",
                        "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRdn3m3M4D2Igjm0ZeUElC5-fWU6wH8Kky7tA&s"
                    },
                    {
                        "title": "Quantum Advantage and Computational Complexity",
                        "content": "<p><strong>Quantum advantage</strong> (or supremacy) refers to the demonstrated ability of a programmable quantum device to solve a problem that no classical computer can solve in any feasible amount of time. It's important to note that quantum computers are not faster for *all* problems.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Complexity Classes</strong></div><p>Classical computers are good at problems in the complexity class <strong>P</strong>. The class of problems that quantum computers can solve efficiently is called <strong>BQP</strong> (Bounded-error Quantum Polynomial time). BQP is believed to be larger than P, but it does not include all hard classical problems (like NP-complete problems). The problems relevant to cryptography, like factoring, happen to be in BQP but are not believed to be in P.</p></div>",
                       
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Creating a Bell State (Entanglement) with Qiskit",
                        "language": "python",
                        "code": "from qiskit import QuantumCircuit, Aer, execute\n\n# Create a quantum circuit with 2 qubits and 2 classical bits\nqc = QuantumCircuit(2, 2)\n\n# 1. Apply a Hadamard gate to the first qubit to create superposition\nqc.h(0)\n\n# 2. Apply a CNOT gate to entangle the qubits\n# The first qubit is the control, the second is the target\nqc.cx(0, 1)\n\n# 3. Measure the qubits\nqc.measure([0,1], [0,1])\n\n# Execute the circuit on a simulator\nbackend = Aer.get_backend('qasm_simulator')\nresult = execute(qc, backend, shots=1024).result()\ncounts = result.get_counts()\n\n# The result will be approximately 50% '00' and 50% '11', but never '01' or '10'\nprint(counts)"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is superposition in the context of a qubit?",
                        "options": [
                            "The qubit is physically larger than a classical bit.",
                            "The qubit can be in a state of 0, 1, or both simultaneously.",
                            "The qubit can communicate faster than the speed of light.",
                            "The qubit has collapsed to a definite state."
                        ],
                        "correct": 1,
                        "explanation": "Superposition is a core principle of quantum mechanics that allows a qubit to exist in a combination of its basis states (|0⟩ and |1⟩) at the same time, enabling quantum parallelism."
                    },
                    {
                        "id": 2,
                        "question": "Which quantum gate is primarily used to create superposition from a basis state?",
                        "options": [
                            "X Gate",
                            "CNOT Gate",
                            "Hadamard Gate",
                            "Z Gate"
                        ],
                        "correct": 2,
                        "explanation": "The Hadamard gate is a fundamental single-qubit gate that maps the basis states |0⟩ and |1⟩ to an equal superposition of both, and vice versa."
                    },
                    {
                        "id": 3,
                        "question": "The class of problems that can be solved efficiently by a quantum computer is known as...?",
                        "options": [
                            "P",
                            "NP",
                            "BQP",
                            "EXP"
                        ],
                        "correct": 2,
                        "explanation": "BQP (Bounded-error Quantum Polynomial time) is the complexity class that captures the computational power of a quantum computer."
                    }
                ]
            }
        },
        {
            "id": "lesson-2",
            "title": "Shor's Algorithm and RSA/ECC Threats",
            "duration": "75 min",
            "objectives": [
                "Understand the mechanism of Shor's algorithm in detail",
                "Explain how period-finding is used to break the discrete logarithm problem",
                "Analyze the threat timeline for RSA factorization",
                "Describe the vulnerability of Elliptic Curve Cryptography (ECC)",
                "Learn how to estimate the quantum resources required for an attack"
            ],
            "content": {
                "overview": "Shor's algorithm is the primary reason quantum computers pose an existential threat to modern public-key cryptography. This lesson provides a deep dive into how this powerful algorithm works and why it can efficiently solve the integer factorization and discrete logarithm problems that underpin the security of RSA and ECC.",
                "sections": [
                    {
                        "title": "Shor's Algorithm Detailed Analysis",
                        "content": "<p>Shor's algorithm is a quantum algorithm for integer factorization. Its power comes from using quantum parallelism to efficiently solve a related problem: <strong>period-finding</strong>.</p><h3>The High-Level Steps:</h3><ol><li>Given a number N to factor, pick a random number 'a' coprime to N.</li><li>Find the period 'r' of the function f(x) = a^x mod N. This is the hard part that a quantum computer does efficiently using the Quantum Fourier Transform.</li><li>If 'r' is even, compute the greatest common divisor (GCD) of (a^(r/2) ± 1) and N.</li><li>The result of the GCD calculation will be a non-trivial factor of N with high probability.</li></ol><p>The quantum computer's ability to find the period 'r' in polynomial time is what makes the entire process feasible, whereas a classical computer would take exponential time.</p>",
                        
                    },
                    {
                        "title": "Threat to RSA and ECC",
                        "content": "<p>The security of today's most common public-key cryptosystems relies on the presumed difficulty of two problems:</p><ul><li><strong>RSA Security:</strong> Relies on the difficulty of factoring large numbers. Shor's algorithm solves this directly.</li><li><strong>ECC Security:</strong> Relies on the difficulty of the Elliptic Curve Discrete Logarithm Problem (ECDLP). A variant of Shor's algorithm can also solve ECDLP efficiently.</li></ul><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Total Breakdown</strong></div><p>This means that a sufficiently powerful quantum computer would completely break the security of nearly all public-key infrastructure used today, including the ECDSA signatures that secure Bitcoin and Ethereum transactions, and the RSA encryption that secures much of the internet via TLS.</p></div>",
                        "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT26vnC2FETQSw62Da5xFjyVNZ4vyAFUkd15w&s"
                    },
                    {
                        "title": "Quantum Resource Requirements Estimation",
                        "content": "<p>While Shor's algorithm is theoretically proven, building a quantum computer large and stable enough to run it is a massive engineering challenge. This is known as a <strong>fault-tolerant quantum computer</strong>.</p><p>Estimates vary, but breaking a 2048-bit RSA key would require a quantum computer with millions of physical qubits to implement the necessary quantum error correction. While current quantum computers have only hundreds or a few thousand (often noisy) qubits, progress is rapid. The key security question is not *if* such a machine can be built, but *when*.</p>",
                     
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Shor's Algorithm Classical Post-Processing (Python)",
                        "language": "python",
                        "code": "import math\n\n# This simulates the classical part of Shor's algorithm AFTER\n# the quantum computer has found the period 'r'.\n\ndef classical_post_processing(N, a, r):\n    if r % 2 != 0:\n        print(\"Period is odd, try again.\")\n        return None\n\n    base = pow(a, r // 2)\n    if (base + 1) % N == 0:\n        print(\"Trivial factor found, try again.\")\n        return None\n\n    factor1 = math.gcd(base + 1, N)\n    factor2 = math.gcd(base - 1, N)\n    return factor1, factor2\n\n# Example: Factor N=15. Let's say we picked a=7 and the quantum\n# computer found the period of 7^x mod 15 to be r=4.\nN = 15\na = 7\nr = 4 # Found by quantum period-finding\n\nfactors = classical_post_processing(N, a, r)\nprint(f\"Factors of {N} are: {factors}\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Shor's algorithm provides an efficient quantum solution to which two mathematical problems?",
                        "options": [
                            "Sorting and Searching",
                            "Integer Factorization and the Discrete Logarithm Problem",
                            "Matrix Multiplication and System Simulation",
                            "Traveling Salesman and Knapsack Problem"
                        ],
                        "correct": 1,
                        "explanation": "These two problems form the foundation of RSA and ECC security, respectively. Shor's ability to solve them efficiently is what makes it a threat."
                    },
                    {
                        "id": 2,
                        "question": "What is the specific, computationally hard task that a quantum computer performs within Shor's algorithm?",
                        "options": [
                            "Calculating the greatest common divisor (GCD).",
                            "Finding the period of a modular exponentiation function.",
                            "Picking a random number.",
                            "Multiplying two large numbers."
                        ],
                        "correct": 1,
                        "explanation": "Period-finding is the core quantum component of Shor's algorithm. It uses the Quantum Fourier Transform to efficiently find the period, a task that is intractable for classical computers."
                    },
                    {
                        "id": 3,
                        "question": "The security of the ECDSA signatures used in Bitcoin and Ethereum relies on the difficulty of which problem?",
                        "options": [
                            "Integer Factorization",
                            "The Elliptic Curve Discrete Logarithm Problem (ECDLP)",
                            "The Traveling Salesman Problem",
                            "The Learning With Errors (LWE) Problem"
                        ],
                        "correct": 1,
                        "explanation": "ECDLP is the problem that a variant of Shor's algorithm can solve, which would allow an attacker to derive a private key from a public key on these blockchains."
                    }
                ]
            }
        },
        {
            "id": "lesson-3",
            "title": "Grover's Algorithm and Symmetric Crypto Impact",
            "duration": "60 min",
            "objectives": [
                "Understand the mechanism of Grover's search algorithm",
                "Analyze the square root speedup it provides",
                "Assess the impact of this speedup on symmetric cryptography (e.g., AES)",
                "Explain the resulting requirement to double key lengths for equivalent security",
                "Calculate the effect on hash function security"
            ],
            "content": {
                "overview": "While Shor's algorithm is a devastating, targeted attack on public-key crypto, Grover's algorithm is a more general-purpose quantum search algorithm that also has significant, though less catastrophic, implications for symmetric cryptography. This lesson explores how Grover's algorithm works and what it means for the security of AES and SHA-256.",
                "sections": [
                    {
                        "title": "Grover's Algorithm Mechanism",
                        "content": "<p>Grover's algorithm is a quantum algorithm that finds a specific entry in an unstructured database or search space. A classical computer would have to check, on average, N/2 entries to find the target in a database of size N. Grover's algorithm can find it in approximately √N (square root of N) steps.</p><p>It works by repeatedly applying an operation, called the Grover iterator, that amplifies the probability amplitude of the desired state while decreasing the amplitudes of all other states. After about √N iterations, a measurement will yield the correct state with very high probability.</p>",
                       
                    },
                    {
                        "title": "Impact on Symmetric Cryptography",
                        "content": "<p>The most basic attack against a symmetric cipher like AES is a brute-force key search. For a key of length 'k' bits, there are 2^k possible keys. Classically, an attacker would have to try, on average, half of them (2^(k-1)) to find the correct key.</p><p>A quantum computer running Grover's algorithm could search this key space in √2^k = 2^(k/2) steps. This means that Grover's algorithm effectively halves the security level (in bits) of any symmetric cipher. For example, AES-128, which has a 128-bit security level against classical attacks, only has a 64-bit security level against a quantum attack.</p><div class='info-box tip'><div class='info-box-header'><i class='fas fa-lightbulb'></i><strong>The Mitigation: Double the Key Length</strong></div><p>The standard defense against this is straightforward: to maintain a 128-bit security level in the post-quantum era, we must use a cipher with a 256-bit key. AES-256 provides 256 bits of classical security, which is reduced to 128 bits of quantum security (√2^256 = 2^128), which is still considered completely secure.</p></div>",
                        "image": "https://www.researchgate.net/publication/362794835/figure/fig5/AS:11431281170444758@1687775381508/Effect-of-Shors-and-Grovers-algorithms-on-Cryptography-Algorithm.png"
                    },
                    {
                        "title": "Impact on Hash Functions",
                        "content": "<p>Grover's algorithm also affects the collision resistance of hash functions. Finding a collision for a hash function with an n-bit output takes about 2^(n/2) classical computations (due to the birthday attack). Grover's algorithm can speed this up to approximately 2^(n/3) quantum computations.</p><p>For SHA-256 (n=256), the classical collision resistance is about 2^128. The quantum resistance is about 2^85. While this is a significant reduction, a security level of 85 bits is still considered very high and likely secure for the foreseeable future. Therefore, hash functions like SHA-256 are considered more resilient to quantum attacks than symmetric ciphers.</p>",
                        
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What kind of speedup does Grover's algorithm provide for an unstructured search problem of size N?",
                        "options": [
                            "Logarithmic (log N)",
                            "Linear (N)",
                            "Quadratic (N^2)",
                            "Square Root (√N)"
                        ],
                        "correct": 3,
                        "explanation": "Grover's algorithm provides a quadratic speedup over the best possible classical algorithm for unstructured search, reducing the complexity from O(N) to O(√N)."
                    },
                    {
                        "id": 2,
                        "question": "What is the impact of Grover's algorithm on an AES-128 key search?",
                        "options": [
                            "It has no impact.",
                            "It breaks it instantly.",
                            "It reduces the effective security level from 128 bits to 64 bits.",
                            "It reduces the effective security level from 128 bits to 127 bits."
                        ],
                        "correct": 2,
                        "explanation": "A brute-force key search is an unstructured search problem. Grover's algorithm reduces the number of operations from 2^128 to √2^128 = 2^64, effectively halving the bit security."
                    },
                    {
                        "id": 3,
                        "question": "What is the recommended mitigation to maintain a 128-bit security level for symmetric encryption against quantum attacks?",
                        "options": [
                            "Stop using symmetric encryption.",
                            "Use AES-128 twice.",
                            "Use a symmetric cipher with a 256-bit key, like AES-256.",
                            "Use a symmetric cipher with a 64-bit key."
                        ],
                        "correct": 2,
                        "explanation": "Using a 256-bit key provides a quantum security level of 128 bits (2^256 -> 2^128), which is the currently accepted standard for long-term security."
                    }
                ]
            }
        },
        {
            "id": "lesson-4",
            "title": "Post-Quantum Cryptography Landscape",
            "duration": "75 min",
            "objectives": [
                "Define Post-Quantum Cryptography (PQC)",
                "Understand the goals and process of the NIST PQC standardization competition",
                "Identify the main mathematical families of PQC algorithms",
                "Analyze the security assumptions behind these families",
                "Discuss the trade-offs between performance, key size, and security"
            ],
            "content": {
                "overview": "With the threat of quantum computers established, the race is on to develop and standardize new cryptographic algorithms that are resistant to quantum attacks. This lesson provides a high-level overview of the Post-Quantum Cryptography (PQC) landscape, the major families of algorithms, and the NIST process that is selecting the next generation of cryptographic standards.",
                "sections": [
                    {
                        "title": "What is Post-Quantum Cryptography?",
                        "content": "<p><strong>Post-Quantum Cryptography (PQC)</strong>, also called quantum-resistant cryptography, refers to cryptographic algorithms (typically public-key algorithms) that are thought to be secure against cryptanalytic attack by both classical and quantum computers.</p><p>Crucially, PQC algorithms are designed to run on classical computers we use today. They are not 'quantum cryptography' (like QKD), but rather classical algorithms whose security is based on mathematical problems that are believed to be hard even for a quantum computer to solve.</p>",
                       
                    },
                    {
                        "title": "The NIST PQC Standardization Process",
                        "content": "<p>Since 2016, the U.S. National Institute of Standards and Technology (NIST) has been running a public competition to select and standardize one or more PQC algorithms.</p><h3>The Process:</h3><ol><li><strong>Call for Proposals (2016):</strong> NIST invited the global cryptographic community to submit candidate algorithms.</li><li><strong>Rounds of Analysis:</strong> Over several years and multiple rounds, the submitted algorithms have been subjected to intense public scrutiny by researchers worldwide. Weaknesses have been found, and many candidates have been eliminated.</li><li><strong>Selection and Standardization (2022-Present):</strong> NIST has selected a primary set of algorithms for standardization (for both key exchange and signatures) and is continuing to analyze others for future consideration.</li></ol><p>This open and transparent process aims to build global consensus and confidence in the final standardized algorithms.</p>",
                        "image": "https://media.licdn.com/dms/image/v2/D5612AQGvygmFRjcOTQ/article-cover_image-shrink_720_1280/B56Zc0ZEojGoAI-/0/1748930679353?e=2147483647&v=beta&t=th3dlm3wAGs20ltUI5uM0owSTtuqzeSHX1USXP7usrQ"
                    },
                    {
                        "title": "Mathematical Families of PQC",
                        "content": "<p>Most PQC candidates fall into one of several families, each based on a different hard mathematical problem:</p><ul><li><strong>Lattice-based Cryptography:</strong> Based on the difficulty of finding the shortest vector in a high-dimensional geometric lattice. This family is the most promising and includes the primary NIST selections (Kyber, Dilithium).</li><li><strong>Code-based Cryptography:</strong> Based on the difficulty of decoding a general linear error-correcting code. McEliece is the oldest example.</li><li><strong>Multivariate Cryptography:</strong> Based on the difficulty of solving systems of multivariate polynomial equations.</li><li><strong>Hash-based Signatures:</strong> Based only on the security of cryptographic hash functions. SPHINCS+ is the primary example.</li><li><strong>Isogeny-based Cryptography:</strong> Based on the difficulty of finding a path between two supersingular elliptic curves. (This family has recently faced significant cryptanalytic challenges).</li></ul><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>No Silver Bullet</strong></div><p>Each family has different performance trade-offs. For example, some have very large key sizes, while others have slower signing operations. The diversity of mathematical approaches is a key strategic goal, so that if a weakness is found in one family, the entire PQC ecosystem is not compromised.</p></div>",
                       
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Post-Quantum Cryptography (PQC) algorithms are designed to run on...?",
                        "options": [
                            "Quantum computers only",
                            "Specialized hardware only",
                            "Classical computers we use today",
                            "Mainframe computers"
                        ],
                        "correct": 2,
                        "explanation": "PQC algorithms are classical algorithms designed to be secure against future quantum threats. They do not require a quantum computer to run."
                    },
                    {
                        "id": 2,
                        "question": "What is the primary goal of the NIST PQC standardization process?",
                        "options": [
                            "To design a new quantum computer.",
                            "To select one or more quantum-resistant algorithms through a public, transparent process of analysis.",
                            "To find a faster way to break RSA.",
                            "To create a new cryptocurrency."
                        ],
                        "correct": 1,
                        "explanation": "The NIST process is a multi-year, collaborative effort to vet and standardize PQC algorithms to ensure a secure migration away from vulnerable public-key systems."
                    },
                    {
                        "id": 3,
                        "question": "Which family of PQC algorithms is considered the most promising and has been selected by NIST for primary standardization?",
                        "options": [
                            "Isogeny-based",
                            "Code-based",
                            "Multivariate",
                            "Lattice-based"
                        ],
                        "correct": 3,
                        "explanation": "Lattice-based cryptography has emerged as the leading contender due to its strong security proofs and relatively good all-around performance, leading to the selection of algorithms like Kyber and Dilithium."
                    }
                ]
            }
        },
        {
            "id": "lesson-5",
            "title": "Lattice-Based Cryptography Foundations",
            "duration": "90 min",
            "objectives": [
                "Understand the mathematical concept of a lattice",
                "Learn about lattice basis reduction algorithms like LLL",
                "Define the Shortest Vector Problem (SVP)",
                "Define the Closest Vector Problem (CVP)",
                "Appreciate the computational hardness of these lattice problems"
            ],
            "content": {
                "overview": "Lattice-based cryptography is the foundation for the most promising and now standardized post-quantum algorithms. This lesson introduces the fundamental mathematical object at its core—the lattice—and the computationally hard problems associated with it that provide its security.",
                "sections": [
                    {
                        "title": "Lattice Theory Fundamentals",
                        "content": "<p>In this context, a <strong>lattice</strong> is a regular, grid-like set of points in a multi-dimensional space. More formally, it is the set of all integer linear combinations of a set of basis vectors.</p><p>A key property of lattices is that they can have many different bases. A 'good' basis consists of short, nearly orthogonal vectors, while a 'bad' basis consists of long, non-orthogonal vectors. It is easy to go from a good basis to a bad one, but it is computationally very hard to go from a bad basis back to a good one. This asymmetry is the foundation of lattice-based cryptography.</p>",
                        
                    },
                    {
                        "title": "Hard Lattice Problems: SVP and CVP",
                        "content": "<p>The security of lattice-based cryptography relies on several problems that are believed to be hard for both classical and quantum computers.</p><h3>Shortest Vector Problem (SVP):</h3><p>Given an arbitrary basis for a lattice, find the non-zero lattice vector with the smallest Euclidean norm (i.e., the lattice point closest to the origin). Finding the exact shortest vector is extremely hard in high dimensions.</p><h3>Closest Vector Problem (CVP):</h3><p>Given an arbitrary basis for a lattice and a target vector that is *not* in the lattice, find the lattice vector that is closest to the target vector. This is also extremely hard.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Average-Case vs. Worst-Case Hardness</strong></div><p>One of the most attractive features of lattice cryptography is that its security can often be based on the *worst-case* hardness of these problems. This means the cryptosystem is secure as long as the underlying problem is hard to solve for *any* input, not just a specific subset. This provides a very strong security guarantee.</p></div>",
                        "image": "https://www.redhat.com/rhdc/managed-files/styles/wysiwyg_full_width/private/lattice-based-cryptography.png.webp?itok=ktUcRMmc"
                    },
                    {
                        "title": "Lattice Basis Reduction (LLL Algorithm)",
                        "content": "<p>Since finding the exact shortest vector is too hard, cryptanalysts use 'basis reduction' algorithms to find an approximately short vector. The most famous of these is the Lenstra–Lenstra–Lovász (LLL) algorithm.</p><p>LLL takes a 'bad' basis (long, skewed vectors) as input and outputs a 'better' basis (shorter, more orthogonal vectors). While it doesn't solve SVP exactly, its ability to find shortish vectors is the primary tool used in lattice cryptanalysis. The security of a lattice-based scheme depends on choosing parameters (like the dimension of the lattice) such that the approximations provided by LLL and its more powerful successors are not good enough to break the system.</p>",
                       
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Lattice Visualization (Conceptual Python with SageMath)",
                        "language": "python",
                        "code": "# This requires a mathematical library like SageMath to run\n# import numpy as np\n# from sage.all import V, plot\n\n# Define two 'bad' basis vectors\nb1 = vector([6, 1])\nb2 = vector([5, 1])\n\n# Create the lattice from the basis matrix\nB = matrix([b1, b2])\nL = Lattice(B)\n\n# Use LLL to find a 'good' basis\nB_reduced = L.LLL()\n\n# B_reduced will contain shorter, more orthogonal vectors, e.g., [[1, 0], [-1, 1]]\nprint(\"Original Basis Matrix:\\n\", B)\nprint(\"Reduced Basis Matrix:\\n\", B_reduced)\n\n# Visualizing this would show that the reduced basis vectors\n# form a much more natural 'grid' for the same set of points."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the Shortest Vector Problem (SVP)?",
                        "options": [
                            "Finding the shortest path between two cities.",
                            "Finding the non-zero lattice point closest to the origin.",
                            "Finding the shortest basis for a lattice.",
                            "Finding the shortest way to write a vector."
                        ],
                        "correct": 1,
                        "explanation": "SVP is a fundamental hard problem in lattice theory. Given a basis, the goal is to find the shortest possible non-zero vector that can be formed by an integer combination of the basis vectors."
                    },
                    {
                        "id": 2,
                        "question": "What is the primary purpose of the LLL algorithm in cryptanalysis?",
                        "options": [
                            "To encrypt data.",
                            "To solve SVP exactly.",
                            "To take a 'bad' lattice basis and find a 'better' one with shorter, more orthogonal vectors.",
                            "To create quantum-resistant signatures."
                        ],
                        "correct": 2,
                        "explanation": "LLL is a basis reduction algorithm. It's a key tool for attackers, as it helps them find weaknesses by finding approximately short vectors. Cryptographers design systems with parameters large enough to resist LLL."
                    },
                    {
                        "id": 3,
                        "question": "What is a major theoretical advantage of many lattice-based cryptosystems?",
                        "options": [
                            "They are very easy to implement.",
                            "They have the smallest key sizes.",
                            "Their security can be based on the worst-case hardness of the underlying problems.",
                            "They are the fastest PQC algorithms."
                        ],
                        "correct": 2,
                        "explanation": "The connection between average-case security (needed for a cryptosystem) and worst-case hardness (of the underlying math problem) is a powerful feature, giving strong confidence in their security."
                    }
                ]
            }
        },
        {
            "id": "lesson-6",
            "title": "Learning With Errors (LWE) Problem",
            "duration": "75 min",
            "objectives": [
                "Define the Learning With Errors (LWE) problem",
                "Understand its variants like Ring-LWE and Module-LWE",
                "Analyze the role and importance of the error distribution",
                "Grasp the concept of security reductions from lattice problems",
                "Learn about the process of selecting secure parameters"
            ],
            "content": {
                "overview": "The Learning With Errors (LWE) problem is a cornerstone of modern lattice-based cryptography and the foundation for the NIST-selected PQC standard, Kyber. This lesson explains what the LWE problem is, why it's believed to be hard, and how it is used to build secure cryptosystems.",
                "sections": [
                    {
                        "title": "LWE Problem Definition",
                        "content": "<p>The LWE problem is about solving a system of noisy linear equations. Imagine you are given a set of equations of the form <strong>b ≈ A · s</strong>, where <strong>A</strong> and <strong>b</strong> are known, but <strong>s</strong> is a secret vector. The '≈' means that the equations are not exact; a small, random 'error' has been added to each one.</p><p><strong>The Challenge:</strong> Find the secret vector <strong>s</strong>. Without the errors, this would be a simple linear algebra problem. With the addition of even small, random errors, the problem becomes computationally very hard to solve.</p>",
                        
                    },
                    {
                        "title": "Security Reductions",
                        "content": "<p>The reason we believe LWE is hard is because it has been proven to be at least as hard as the worst-case instances of standard lattice problems like SVP. This is called a <strong>security reduction</strong>.</p><p>This means that if someone were to find an efficient algorithm to solve the average-case LWE problem, they could use it as a subroutine to build an efficient algorithm to solve the hardest, worst-case lattice problems. Since decades of research have failed to find such an algorithm for lattice problems, we are confident that LWE is also hard.</p>",
                        "image": "https://image3.slideserve.com/7044569/slide4-l.jpg"
                    },
                    {
                        "title": "Variants: Ring-LWE and Module-LWE",
                        "content": "<p>The basic LWE problem can be inefficient, as the matrix <strong>A</strong> can be very large. To improve performance, structured variants were introduced.</p><ul><li><strong>Ring-LWE (RLWE):</strong> Instead of a random matrix, RLWE uses a matrix with algebraic structure derived from polynomial rings. This allows for much smaller key sizes and faster computations (using techniques like the Number Theoretic Transform).</li><li><strong>Module-LWE (MLWE):</strong> A generalization of RLWE that provides a middle ground, offering better security and flexibility than RLWE while still being much more efficient than plain LWE. The NIST-selected KEM, Kyber, is based on MLWE.</li></ul><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>The Role of Errors</strong></div><p>The small errors are crucial. They are used to mask the secret information during cryptographic operations. In an encryption scheme, the error is added during encryption, and the legitimate user with the secret key is able to remove the error during decryption, while an attacker cannot.</p></div>",
                        
                    }
                ],
                "codeExamples": [
                    {
                        "title": "LWE Instance Generation (Python)",
                        "language": "python",
                        "code": "import numpy as np\n\n# LWE parameters\nn = 10  # dimension of the secret\nm = 20  # number of samples\nq = 97  # a prime modulus\n\n# 1. Generate a secret vector 's'\ns = np.random.randint(0, q, n)\n\n# 2. Generate the public matrix 'A'\nA = np.random.randint(0, q, (m, n))\n\n# 3. Generate a small error vector 'e'\n# Errors are typically drawn from a discrete Gaussian distribution\ne = np.random.randint(-1, 2, m)\n\n# 4. Compute b = (A.s + e) mod q\nb = (np.dot(A, s) + e) % q\n\n# The LWE problem: Given A and b, find s.\nprint(\"Public Matrix A (first 5 rows):\\n\", A[:5])\nprint(\"\\nPublic Vector b:\\n\", b)\nprint(\"\\nSecret Vector s (for reference):\\n\", s)"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the core idea of the Learning With Errors (LWE) problem?",
                        "options": [
                            "Learning from mistakes in code.",
                            "Solving a system of linear equations that contains small, random errors.",
                            "Finding errors in a data transmission.",
                            "Encrypting data with errors."
                        ],
                        "correct": 1,
                        "explanation": "LWE's hardness comes from the fact that adding a small amount of random noise to a system of linear equations makes it computationally very difficult to find the original secret solution."
                    },
                    {
                        "id": 2,
                        "question": "What does a 'security reduction' from SVP to LWE tell us?",
                        "options": [
                            "LWE is easier than SVP.",
                            "LWE is a type of lattice.",
                            "If you can solve LWE efficiently, you can also solve the hardest instances of SVP, which implies LWE is also hard.",
                            "The two problems are unrelated."
                        ],
                        "correct": 2,
                        "explanation": "This formal proof connects the security of the LWE problem to the long-standing, well-studied hardness of worst-case lattice problems, providing strong confidence in LWE's security."
                    },
                    {
                        "id": 3,
                        "question": "Why were structured variants like Ring-LWE and Module-LWE developed?",
                        "options": [
                            "They are easier to understand.",
                            "To provide better performance and smaller key sizes compared to plain LWE.",
                            "They are more resistant to quantum attacks.",
                            "They do not require errors."
                        ],
                        "correct": 1,
                        "explanation": "The algebraic structure in Ring-LWE and Module-LWE allows for much more compact and computationally efficient cryptographic schemes, making them practical for real-world applications. Kyber, the NIST standard, is based on Module-LWE."
                    }
                ]
            }
        },
        {
            "id": "lesson-7",
            "title": "NTRU and Ring-Based Systems",
            "duration": "75 min",
            "objectives": [
                "Understand the fundamentals of the NTRU cryptosystem",
                "Learn the basics of polynomial ring arithmetic",
                "Analyze the process of NTRU parameter selection",
                "Explore the security and common attacks against NTRU",
                "Recognize modern NTRU variants and improvements"
            ],
            "content": {
                "overview": "NTRU (N-th degree Truncated polynomial Ring Units) is one of the oldest and most studied lattice-based cryptosystems. It uses arithmetic in a ring of polynomials to achieve efficient encryption and decryption. This lesson explores the mechanics of NTRU and its place in the PQC landscape.",
                "sections": [
                    {
                        "title": "NTRU Cryptosystem Fundamentals",
                        "content": "<p>NTRU is a public-key cryptosystem that, like other lattice schemes, involves 'small' (secret) polynomials and 'large' (public) polynomials. Its security is related to finding a short vector in a particular type of lattice.</p><h3>The Core Idea:</h3><ul><li><strong>Private Key:</strong> Two small, secret polynomials, <strong>f</strong> and <strong>g</strong>.</li><li><strong>Public Key:</strong> The polynomial <strong>h = g / f</strong>, computed in a specific polynomial ring. The division 'hides' the smallness of <strong>f</strong> and <strong>g</strong>.</li><li><strong>Encryption:</strong> A message (another small polynomial) is encrypted using the public key <strong>h</strong> and a random small polynomial.</li><li><strong>Decryption:</strong> The owner of the private key <strong>f</strong> can use it to 'undo' the multiplication by <strong>h</strong> and recover the original message.</li></ul>"
                        
                    },
                    {
                        "title": "Polynomial Ring Arithmetic",
                        "content": "<p>The operations in NTRU (addition, multiplication, division) are not performed on regular numbers, but on polynomials. These operations are performed modulo two things:<ul><li>A polynomial modulus, typically <strong>X^N - 1</strong>, which means the degree of the polynomials never exceeds N-1.</li><li>An integer modulus, <strong>q</strong>, which means the coefficients of the polynomials are kept within a certain range.</li></ul>This algebraic structure, a 'truncated polynomial ring', allows for very efficient computation, especially when N is a power of 2, as it enables the use of FFT-like algorithms for multiplication.</p>",
                        "image": "https://www.mdpi.com/electronics/electronics-14-01358/article_deploy/html/images/electronics-14-01358-g001.png"
                    },
                    {
                        "title": "Security Analysis and Attacks",
                        "content": "<p>The security of NTRU rests on the difficulty of recovering the private key <strong>(f, g)</strong> from the public key <strong>h</strong>. This is equivalent to solving a shortest vector problem on a lattice constructed from <strong>h</strong>.</p><p>An attacker can use lattice reduction algorithms like LLL or BKZ on this 'NTRU lattice' to try and recover the secret key. Therefore, the security of an NTRU instance depends on choosing parameters (N, q, and the distribution of the small polynomials) such that the lattice dimension is high enough to resist these attacks.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>NTRU in the NIST Competition</strong></div><p>Several strong candidates in the NIST PQC competition were based on NTRU, including the finalist NTRUEncrypt. While Kyber (based on MLWE) was ultimately chosen as the primary standard for KEMs, NTRU-based schemes are still considered highly secure and efficient alternatives.</p></div>"
                      
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What are the private and public keys in the NTRU cryptosystem made of?",
                        "options": [
                            "Large prime numbers",
                            "Elliptic curves",
                            "Polynomials",
                            "Error-correcting codes"
                        ],
                        "correct": 2,
                        "explanation": "NTRU's core operations are based on arithmetic in a ring of polynomials. The private key consists of two 'small' polynomials, and the public key is their quotient."
                    },
                    {
                        "id": 2,
                        "question": "Breaking NTRU is mathematically related to which hard problem?",
                        "options": [
                            "Factoring large numbers",
                            "Solving a system of noisy linear equations (LWE)",
                            "Finding a short vector in a specific type of lattice (the NTRU lattice)",
                            "Calculating a discrete logarithm"
                        ],
                        "correct": 2,
                        "explanation": "An attacker can construct a lattice from the public key 'h', and finding a short vector in this lattice is equivalent to recovering the private key. Thus, its security is based on the SVP."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary tool an attacker would use to try and break NTRU?",
                        "options": [
                            "Shor's algorithm",
                            "Grover's algorithm",
                            "Lattice basis reduction algorithms like LLL or BKZ",
                            "A classical brute-force search"
                        ],
                        "correct": 2,
                        "explanation": "Lattice reduction algorithms are the primary method of cryptanalysis against NTRU, as they are designed to find the short vectors that constitute the secret key."
                    }
                ]
            }
        },
        {
            "id": "lesson-8",
            "title": "Kyber Key Encapsulation Mechanism",
            "duration": "75 min",
            "objectives": [
                "Understand the specification of the CRYSTALS-Kyber algorithm",
                "Recognize its security foundation in the Module-LWE problem",
                "Describe the three core operations: Key Generation, Encapsulation, and Decapsulation",
                "Analyze the role of the Fujisaki-Okamoto transformation in achieving high security",
                "Learn about key implementation considerations for performance and security"
            ],
            "content": {
                "overview": "CRYSTALS-Kyber is a key encapsulation mechanism (KEM) chosen by NIST as the primary standard for post-quantum public-key encryption. This lesson breaks down how Kyber works, from its Module-LWE foundation to the process of encapsulating a shared secret that can be used for symmetric encryption.",
                "sections": [
                    {
                        "title": "Kyber Algorithm Specification",
                        "content": "<p>Kyber is not a public-key encryption scheme in the traditional sense (where you encrypt a message directly). It is a <strong>Key Encapsulation Mechanism (KEM)</strong>. The goal of a KEM is for two parties to agree on a shared secret key, which can then be used with a secure symmetric cipher (like AES-256) to encrypt data.</p><h3>The KEM Process:</h3><ol><li><strong>KeyGen:</strong> Alice runs the Key Generation algorithm to create a public key (pk) and a private key (sk).</li><li><strong>Encaps:</strong> Bob wants to communicate with Alice. He runs the Encapsulation algorithm with Alice's public key (pk). This produces two outputs: a shared secret (ss) and a ciphertext (ct). Bob uses 'ss' for his encryption and sends 'ct' to Alice.</li><li><strong>Decaps:</strong> Alice receives the ciphertext (ct). She runs the Decapsulation algorithm with her private key (sk) and the received ciphertext. This recovers the exact same shared secret (ss).</li></ol>"
                    },
                    {
                        "title": "Security from Module-LWE",
                        "content": "<p>The security of Kyber is based on the hardness of the <strong>Module Learning With Errors (MLWE)</strong> problem. The public key in Kyber is essentially an MLWE instance (a matrix <strong>A</strong> and a vector <strong>b = A·s + e</strong>). The secret key is the small vector <strong>s</strong> and the small error vector <strong>e</strong>.</p><p>An attacker who only has the public key (A, b) cannot find the secret key 's' because solving MLWE is computationally hard. However, the owner of the secret key 's' can use it to decrypt/decapsulate messages that have been encrypted with the corresponding public key.</p>",
                        "image": "https://www.researchgate.net/publication/371388643/figure/fig2/AS:11431281390847500@1745297107906/Key-encapsulation-mechanism-overview.tif"
                    },
                    {
                        "title": "Implementation and Performance",
                        "content": "<p>Kyber was chosen by NIST in part due to its excellent all-around performance. It has relatively small public keys and ciphertexts compared to many other PQC candidates, and the key generation, encapsulation, and decapsulation operations are all very fast.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>IND-CCA2 Security</strong></div><p>To achieve the highest level of security (IND-CCA2, which protects against chosen-ciphertext attacks), Kyber applies a transformation known as the Fujisaki-Okamoto (FO) transform. This involves hashing various inputs and using them to derive keys and check for consistency, which prevents an attacker from subtly modifying ciphertexts to learn information about the secret key. This transformation is a critical part of the final, standardized algorithm.</p></div>"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary purpose of a Key Encapsulation Mechanism (KEM) like Kyber?",
                        "options": [
                            "To sign digital documents.",
                            "To encrypt a large message directly with a public key.",
                            "To securely establish a shared secret key between two parties, which is then used for symmetric encryption.",
                            "To prove one's identity."
                        ],
                        "correct": 2,
                        "explanation": "A KEM is a modern approach to public-key cryptography. Instead of encrypting the data itself, it encrypts a random key, which is then used with a highly efficient symmetric cipher like AES. This is called a hybrid encryption scheme."
                    },
                    {
                        "id": 2,
                        "question": "The security of the CRYSTALS-Kyber algorithm is based on the hardness of which mathematical problem?",
                        "options": [
                            "Integer Factorization",
                            "The Discrete Logarithm Problem",
                            "The Module Learning With Errors (MLWE) Problem",
                            "The Multivariate Quadratic (MQ) Problem"
                        ],
                        "correct": 2,
                        "explanation": "Kyber is a prime example of a lattice-based cryptosystem whose security is derived from the well-studied hardness of a structured variant of the Learning With Errors problem."
                    },
                    {
                        "id": 3,
                        "question": "In the Kyber KEM process, the 'Encapsulation' step produces which two outputs?",
                        "options": [
                            "A public key and a private key.",
                            "A signature and a message.",
                            "A shared secret and a ciphertext.",
                            "An encrypted message and a decryption key."
                        ],
                        "correct": 2,
                        "explanation": "The encapsulator uses the recipient's public key to generate a shared secret for their own use, and a ciphertext which they send to the recipient. The recipient then uses their private key on the ciphertext to derive the same shared secret."
                    }
                ]
            }
        },
        {
            "id": "lesson-9",
            "title": "CRYSTALS-DILITHIUM Signatures",
            "duration": "75 min",
            "objectives": [
                "Understand the specification of the CRYSTALS-DILITHIUM signature scheme",
                "Learn about the 'Fiat-Shamir with Aborts' technique for building signatures",
                "Recognize its security foundation in Module-LWE",
                "Analyze the security proof methodology",
                "Discuss parameter optimization and performance"
            ],
            "content": {
                "overview": "Alongside Kyber for key exchange, NIST selected CRYSTALS-DILITHIUM as the primary standard for post-quantum digital signatures. This lesson explores how Dilithium uses the same underlying Module-LWE problem to create a secure and efficient signature scheme, focusing on the clever cryptographic construction that makes it possible.",
                "sections": [
                    {
                        "title": "Dilithium Signature Scheme",
                        "content": "<p>Dilithium is a digital signature algorithm. Like any signature scheme, it consists of three algorithms:</p><ol><li><strong>KeyGen:</strong> Generates a public key (pk) for verifying signatures and a private key (sk) for creating them.</li><li><strong>Sign:</strong> Takes a message and the private key (sk) as input and produces a signature (sig).</li><li><strong>Verify:</strong> Takes the message, the signature (sig), and the public key (pk) as input. It outputs 'valid' if the signature was created by the owner of the corresponding private key, and 'invalid' otherwise.</li></ol><p>The public and private keys in Dilithium are, like Kyber, derived from the Module-LWE problem.</p>"
                    },
                    {
                        "title": "Fiat-Shamir with Aborts",
                        "content": "<p>Signatures are proofs of knowledge (proving you know the secret key). A common way to build a signature scheme from a hard problem is to use an interactive 'identification protocol' and make it non-interactive using the Fiat-Shamir transformation.</p><p>Dilithium uses a clever variant of this called <strong>Fiat-Shamir with Aborts</strong>. During the signing process, a signer generates some secret values and a public 'commitment'. A challenge is then derived by hashing the message and the commitment. The signer then uses the challenge and their secret key to compute the rest of the signature. However, if the secret values generated are too large, they might leak information about the secret key. To prevent this, the signing algorithm 'aborts' and retries with new random values until a signature with 'small' enough values is produced. This is called <strong>rejection sampling</strong>.</p>",
                        "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQZvIH8E_DscTG6xXsUSSq3uRNCSCEAYDwRw&s"
                    },
                    {
                        "title": "Security and Performance",
                        "content": "<p>The security of Dilithium relies on the hardness of MLWE and another lattice problem called the Short Integer Solution (SIS) problem. The use of rejection sampling is crucial; without it, an attacker could analyze the distribution of values in many signatures to slowly learn bits of the private key.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Performance</strong></div><p>Dilithium has one of the smallest signature sizes among all the NIST PQC candidates, which is a major advantage for applications like blockchains where signature data must be stored permanently. It is also very fast for both signing and verifying, making it a strong, all-around choice for a primary standard.</p></div>"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "CRYSTALS-DILITHIUM is the NIST standard for which cryptographic primitive?",
                        "options": [
                            "Symmetric Encryption",
                            "Key Encapsulation Mechanism (KEM)",
                            "Digital Signature Algorithm",
                            "Hash Function"
                        ],
                        "correct": 2,
                        "explanation": "Dilithium is designed for creating and verifying digital signatures in a post-quantum world, complementing Kyber which is for key establishment."
                    },
                    {
                        "id": 2,
                        "question": "What is 'rejection sampling' in the context of Dilithium's signing process?",
                        "options": [
                            "Rejecting invalid signatures.",
                            "The process of aborting and retrying the signing operation if the generated random values are too large and might leak information.",
                            "A method for sampling random numbers.",
                            "A way to reduce the size of the signature."
                        ],
                        "correct": 1,
                        "explanation": "This 'Fiat-Shamir with Aborts' technique is a key part of Dilithium's design, ensuring that the output signatures have a specific statistical distribution that does not reveal any information about the secret key."
                    },
                    {
                        "id": 3,
                        "question": "Like Kyber, Dilithium's security is based on the hardness of which family of problems?",
                        "options": [
                            "Code-based problems",
                            "Multivariate problems",
                            "Lattice-based problems (like MLWE and SIS)",
                            "Isogeny-based problems"
                        ],
                        "correct": 2,
                        "explanation": "Both of the primary NIST CRYSTALS standards (Kyber and Dilithium) are built on the strong foundations of structured lattice problems, specifically Module-LWE."
                    }
                ]
            }
        },
        {
            "id": "lesson-10",
            "title": "Code-Based Cryptography Foundations",
            "duration": "75 min",
            "objectives": [
                "Understand the basics of error-correcting code theory",
                "Learn about linear codes and syndrome decoding",
                "Grasp the computational hardness of the Syndrome Decoding Problem (SDP)",
                "Describe the core principles of code-based cryptography",
                "Analyze the pros and cons of code-based schemes"
            ],
            "content": {
                "overview": "Code-based cryptography is one of the oldest and most trusted families of post-quantum cryptography. Based on the theory of error-correcting codes, its security rests on a simple but hard problem: decoding a message that has been intentionally corrupted with too many errors. This lesson introduces the foundational concepts of this venerable PQC family.",
                "sections": [
                    {
                        "title": "Error-Correcting Code Theory",
                        "content": "<p>Error-correcting codes are used everywhere in digital communications to detect and correct errors that occur during transmission. The core idea is to add redundancy to a message.</p><p>A message (a k-bit vector) is encoded into a longer <strong>codeword</strong> (an n-bit vector) using a generator matrix <strong>G</strong>. If this codeword is corrupted by a small number of bit-flip errors during transmission, a receiver can still recover the original message because the valid codewords are 'far apart' from each other in the n-dimensional space.</p>"
                      
                    },
                    {
                        "title": "The Syndrome Decoding Problem (SDP)",
                        "content": "<p>The process of correcting errors is called decoding. For a special class of codes called <strong>linear codes</strong>, this is easy if you know the structure of the code and the number of errors is small. The process involves calculating a 'syndrome' which gives a clue about the location of the errors.</p><p>However, if you are given a linear code and a received word, and asked to find the *closest* codeword (i.e., correct the errors) without knowing the code's special structure, the problem becomes computationally very hard. This is the <strong>Syndrome Decoding Problem (SDP)</strong>, and it is NP-complete. The security of code-based cryptography relies on the hardness of SDP.</p>",
                        "image": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSSDm-vDsj-guVNegw4eo2f3WMdOKsGhVXKiA&s"
                    },
                    {
                        "title": "Principles of Code-Based Cryptography",
                        "content": "<p>The core idea behind code-based cryptosystems like McEliece is to use a code that has a secret, efficient decoding algorithm (a 'trapdoor'), but appears to be a general, unstructured code to the public.</p><ul><li><strong>Private Key:</strong> The generator matrix <strong>G</strong> of a structured code (like a Goppa code) that is easy to decode, plus some scrambling matrices.</li><li><strong>Public Key:</strong> A scrambled version of <strong>G</strong> that looks like a random linear code with no discernible structure.</li><li><strong>Encryption:</strong> A message is encoded using the public key, and then a fixed number of errors are intentionally added.</li><li><strong>Decryption:</strong> The owner of the private key can 'un-scramble' the public key, use their secret trapdoor to efficiently correct the errors, and recover the message. An attacker, who only sees a general SDP instance, cannot.</li></ul><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Key Size Problem</strong></div><p>The main drawback of most code-based systems is their very large public key size, which can be hundreds of kilobytes or even megabytes. This has been a barrier to their widespread adoption.</p></div>"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "The security of code-based cryptography primarily relies on the hardness of which problem?",
                        "options": [
                            "Factoring large numbers",
                            "The Syndrome Decoding Problem (SDP)",
                            "Finding the shortest vector in a lattice",
                            "Finding a path between two isogenies"
                        ],
                        "correct": 1,
                        "explanation": "SDP is the problem of finding the closest codeword to a given received word for a general linear code, which is known to be computationally very difficult."
                    },
                    {
                        "id": 2,
                        "question": "What is the 'trapdoor' in a code-based cryptosystem like McEliece?",
                        "options": [
                            "A secret weakness in the algorithm.",
                            "The public key.",
                            "Secret knowledge about the structure of the code that allows for efficient error correction (decoding).",
                            "The error vector."
                        ],
                        "correct": 2,
                        "explanation": "The private key contains the secret structure (e.g., that it's a Goppa code) which enables the legitimate user to decode easily, while the public key hides this structure, presenting a hard problem to an attacker."
                    },
                    {
                        "id": 3,
                        "question": "What is the most significant practical disadvantage of many code-based PQC schemes?",
                        "options": [
                            "They are too slow.",
                            "They have not been studied for very long.",
                            "They have very large public keys.",
                            "They are vulnerable to quantum attacks."
                        ],
                        "correct": 2,
                        "explanation": "The large public keys (often megabytes in size for classic McEliece) make them impractical for many bandwidth-constrained applications, although newer variants have made significant improvements."
                    }
                ]
            }
        },
        {
            "id": "lesson-11",
            "title": "McEliece Cryptosystem",
            "duration": "75 min",
            "objectives": [
                "Understand the algorithm of the classic McEliece cryptosystem",
                "Learn the role and properties of Goppa codes",
                "Analyze the key generation, encryption, and decryption processes",
                "Explore the history of security analysis and attacks against McEliece",
                "Discuss parameter selection and key size trade-offs"
            ],
            "content": {
                "overview": "The McEliece cryptosystem, developed in 1978, is one of the oldest public-key cryptosystems and a leading candidate for post-quantum security. It has withstood decades of cryptanalysis. This lesson provides a detailed look at the original McEliece scheme, its use of Goppa codes, and why it has remained secure for so long.",
                "sections": [
                    {
                        "title": "Classic McEliece Algorithm",
                        "content": "<p>The McEliece cryptosystem is the archetypal code-based PQC scheme. Its design has remained largely unchanged since its invention.</p><h3>The Setup:</h3><ol><li><strong>Key Generation:</strong> Alice chooses a generator matrix <strong>G</strong> for a specific type of code called a binary Goppa code, which has an efficient decoding algorithm. She also chooses a random permutation matrix <strong>P</strong> and a non-singular scramble matrix <strong>S</strong>. Her private key is (G, S, P). Her public key is <strong>G' = S · G · P</strong>. G' looks like the generator matrix of a random linear code.</li><li><strong>Encryption:</strong> Bob wants to send a message <strong>m</strong> to Alice. He encodes it as <strong>c' = m · G'</strong> and adds a random error vector <strong>e</strong> of a specific weight 't'. The ciphertext is <strong>y = c' + e</strong>.</li><li><strong>Decryption:</strong> Alice receives <strong>y</strong>. She computes <strong>y' = y · P⁻¹</strong>. She then uses her secret Goppa decoding algorithm to decode <strong>y'</strong> back to <strong>m · S</strong>, correcting the errors. Finally, she computes <strong>m = (m · S) · S⁻¹</strong> to recover the message.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Goppa Codes",
                        "content": "<p>The choice of code family is critical to McEliece's security. Robert McEliece originally proposed using <strong>Goppa codes</strong>, a family of algebraic codes. Goppa codes are powerful because:<ul><li>They are a large family, making it hard to guess the secret code.</li><li>They have an efficient decoding algorithm (the trapdoor), like Patterson's algorithm, that can correct up to 't' errors.</li><li>They are difficult to distinguish from a random linear code, which is essential for hiding the trapdoor.</li></ul>The difficulty of distinguishing a scrambled Goppa code from a random code is the foundation of the system's security.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Security and Parameter Selection",
                        "content": "<p>The security of McEliece has been remarkably stable. The best known attacks are still forms of <strong>Information Set Decoding (ISD)</strong>, a general algorithm for solving the Syndrome Decoding Problem. While ISD algorithms have improved over the years, their complexity remains exponential.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Choosing Parameters</strong></div><p>To set up a secure McEliece system, one chooses parameters (the code length 'n', dimension 'k', and number of errors 't') such that the complexity of the best ISD attack is at a desired security level (e.g., 2^128). This typically results in very large public keys. The Classic McEliece submission to NIST, for a 128-bit security level, has a public key of over 250 kilobytes.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the purpose of the scramble matrix 'S' and permutation matrix 'P' in the McEliece key generation?",
                        "options": [
                            "To make the encryption faster.",
                            "To reduce the size of the key.",
                            "To hide the secret structure of the Goppa code, making the public key look like a random linear code.",
                            "To correct errors during transmission."
                        ],
                        "correct": 2,
                        "explanation": "These matrices act as a disguise. They transform the structured Goppa code generator matrix 'G' into a public key 'G'' that appears unstructured, thus hiding the trapdoor decoding algorithm."
                    },
                    {
                        "id": 2,
                        "question": "What kind of code family did the original McEliece cryptosystem propose to use?",
                        "options": [
                            "Hamming Codes",
                            "Reed-Solomon Codes",
                            "Goppa Codes",
                            "Turbo Codes"
                        ],
                        "correct": 2,
                        "explanation": "Goppa codes were chosen because they possess an efficient decoding algorithm (the trapdoor) and are hard to distinguish from random codes, making them ideal for this cryptographic construction."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary class of algorithms used to attack the McEliece cryptosystem?",
                        "options": [
                            "Lattice Reduction Algorithms",
                            "Information Set Decoding (ISD) algorithms",
                            "Shor's Algorithm",
                            "Grover's Algorithm"
                        ],
                        "correct": 1,
                        "explanation": "ISD represents the best known general-purpose attack strategy against code-based cryptosystems. Security parameters for McEliece are chosen specifically to ensure the workload for ISD is computationally infeasible."
                    }
                ]
            }
        },
        {
            "id": "lesson-12",
            "title": "BIKE and QC-MDPC Codes",
            "duration": "75 min",
            "objectives": [
                "Understand the theory of Quasi-Cyclic codes",
                "Learn the mechanism of BIKE (Bit Flipping Key Encapsulation)",
                "Describe the construction of QC-MDPC codes",
                "Analyze the concept of decoding failures and their cryptographic impact",
                "Explore performance optimization techniques for these codes"
            ],
            "content": {
                "overview": "While Classic McEliece is highly secure, its large key size is a major drawback. Modern code-based proposals, like the NIST finalist BIKE, use structured codes like Quasi-Cyclic Moderate-Density Parity-Check (QC-MDPC) codes to drastically reduce key sizes. This lesson explores the innovations that make code-based PQC practical for modern applications.",
                "sections": [
                    {
                        "title": "Quasi-Cyclic (QC) Codes",
                        "content": "<p>A key reason for McEliece's large key size is that the public generator matrix is essentially random. <strong>Quasi-Cyclic codes</strong> introduce structure that allows the key to be represented much more compactly. A QC code is one where a cyclic shift of any codeword is still 'close' to being a valid codeword.</p><p>This structure means that instead of storing an entire large matrix, you only need to store its first row. All other rows can be generated from it by cyclic shifts. This property drastically reduces the size of the public key, making the system much more practical.</p>",
                        "image": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "BIKE and QC-MDPC Codes",
                        "content": "<p><strong>BIKE (Bit Flipping Key Encapsulation)</strong> was a finalist in the NIST PQC competition. It is a KEM based on <strong>QC-MDPC</strong> codes. MDPC stands for Moderate-Density Parity-Check code.</p><ul><li><strong>Parity-Check Matrix:</strong> Instead of a generator matrix, these codes are defined by a parity-check matrix <strong>H</strong>. A vector <strong>c</strong> is a codeword if <strong>H · cᵀ = 0</strong>.</li><li><strong>Moderate-Density:</strong> The matrix <strong>H</strong> is sparse, meaning it contains very few 1s and mostly 0s.</li><li><strong>Decoding:</strong> Simple and fast 'bit-flipping' decoders can be used. These decoders iteratively identify and flip bits that are likely to be errors until a valid codeword is found.</li></ul><p>BIKE leverages the compact representation of QC codes and the simplicity of MDPC decoding to create an efficient KEM.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Decoding Failure Analysis",
                        "content": "<p>A unique characteristic of schemes like BIKE is the possibility of <strong>decoding failures</strong>. The simple bit-flipping decoders are probabilistic and are not guaranteed to correct all error patterns, even if the number of errors is within the theoretical limit. This means that, with a very small probability, a legitimate user might fail to decrypt a ciphertext correctly.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Cryptographic Implications</strong></div><p>This probability, called the Decoding Failure Rate (DFR), must be made negligibly small (e.g., less than 2⁻¹²⁸) by choosing the system parameters carefully. Furthermore, cryptographic protocols using these schemes must be designed to be secure even if an attacker can cause a decoding failure to occur. The Fujisaki-Okamoto transformation used in KEMs like BIKE is designed to prevent these failures from leaking any information about the secret key.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary advantage of using Quasi-Cyclic (QC) codes over the random-looking codes in Classic McEliece?",
                        "options": [
                            "They are more secure.",
                            "They allow for a much more compact key representation, drastically reducing key sizes.",
                            "They can correct more errors.",
                            "They are easier to decode."
                        ],
                        "correct": 1,
                        "explanation": "The structure of QC codes allows the entire public key matrix to be generated from just its first row, which leads to a massive reduction in the size of the public key."
                    },
                    {
                        "id": 2,
                        "question": "The BIKE cryptosystem is based on which family of codes?",
                        "options": [
                            "Goppa Codes",
                            "Reed-Solomon Codes",
                            "QC-MDPC (Quasi-Cyclic Moderate-Density Parity-Check) Codes",
                            "Lattice Codes"
                        ],
                        "correct": 2,
                        "explanation": "BIKE uses the combination of QC structure for small keys and the simple decoding algorithms of MDPC codes to build an efficient post-quantum KEM."
                    },
                    {
                        "id": 3,
                        "question": "What is a 'decoding failure' in the context of a cryptosystem like BIKE?",
                        "options": [
                            "An attacker successfully breaking the system.",
                            "The system running out of memory during decryption.",
                            "A rare event where the legitimate user fails to correctly decrypt a valid ciphertext due to the probabilistic nature of the decoder.",
                            "A user entering the wrong password."
                        ],
                        "correct": 2,
                        "explanation": "The simple, high-speed decoders used for MDPC codes are not perfect. There is a tiny, but non-zero, chance they will fail. The system's parameters must be chosen to make this probability negligible, and the protocol must be designed to remain secure even when such failures happen."
                    }
                ]
            }
        },
        {
            "id": "lesson-13",
            "title": "Multivariate Cryptography Foundations",
            "duration": "75 min",
            "objectives": [
                "Understand the concept of a multivariate polynomial system",
                "Define the MQ (Multivariate Quadratic) problem",
                "Learn about Gröbner basis attacks as a primary cryptanalysis tool",
                "Explore the high-level design of Oil and Vinegar signature schemes",
                "Get an overview of Hidden Field Equations (HFE)"
            ],
            "content": {
                "overview": "Multivariate cryptography builds its security on the difficulty of solving a system of multivariate polynomial equations over a finite field. This is a well-known hard problem. This lesson introduces the basics of this PQC family, including the MQ problem and the design principles behind schemes like Oil and Vinegar.",
                "sections": [
                    {
                        "title": "The MQ Problem",
                        "content": "<p>Imagine you have a set of 'm' quadratic equations with 'n' variables. For example:</p><p><code>y1 = 3x1² + 2x1x2 + 5x2² + x1 + 4</code><br/><code>y2 = x1x2 + 7x2² + 6x1 + 2x2</code></p><p>The <strong>Multivariate Quadratic (MQ) Problem</strong> is: given the public polynomials (the equations) and the outputs (y1, y2), find the inputs (x1, x2) that satisfy the system. While this looks simple for two variables, solving such a system is proven to be NP-hard in the general case, making it a good candidate for building cryptography.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Trapdoor Construction",
                        "content": "<p>The core idea of multivariate cryptography is to design a system of multivariate equations that looks random and hard to solve to the public, but contains a secret 'trapdoor' that allows the owner of the private key to solve it easily.</p><h3>General Approach:</h3><ol><li>Choose a central map <strong>F</strong>, which is a system of quadratic equations that is easy to invert (this is the trapdoor).</li><li>Choose two invertible affine maps, <strong>S</strong> and <strong>T</strong> (these are part of the secret key).</li><li>The public key is the composition <strong>P = S ∘ F ∘ T</strong>. This composition makes the public system of equations look like a random, unstructured MQ instance, hiding the easily-invertible structure of <strong>F</strong>.</li></ol><p>To sign a message or decrypt, the user applies the inverse maps <strong>S⁻¹</strong> and <strong>T⁻¹</strong> to bypass the hard problem and use the easy inverse of <strong>F</strong>.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Oil and Vinegar Schemes",
                        "content": "<p><strong>Oil and Vinegar</strong> is one of the oldest and most famous multivariate signature schemes. It constructs the central map <strong>F</strong> with a special structure. The variables are split into two groups: 'oil' variables and 'vinegar' variables.</p><p>The equations are constructed such that if you fix the 'vinegar' variables to random values, the equations become a simple system of *linear* equations in the 'oil' variables, which is very easy to solve. This process of fixing vinegar variables and solving for oil variables allows for efficient signature generation.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Signature Size</strong></div><p>A key advantage of many multivariate signature schemes is that they can have very short signature sizes. However, they often suffer from very large public keys, similar to code-based schemes.</p></div>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "The security of multivariate cryptography is based on the hardness of what problem?",
                        "options": [
                            "Factoring integers",
                            "Solving a system of multivariate polynomial equations over a finite field (the MQ problem).",
                            "Finding a short vector in a lattice.",
                            "Decoding a random linear code."
                        ],
                        "correct": 1,
                        "explanation": "The MQ problem is a well-known NP-hard problem, and it forms the security foundation for this entire family of PQC algorithms."
                    },
                    {
                        "id": 2,
                        "question": "What is the general 'trapdoor' principle in multivariate cryptography?",
                        "options": [
                            "There is no trapdoor.",
                            "The system uses a secret, easily invertible central map that is hidden by composing it with two other secret linear maps.",
                            "The trapdoor is a secret prime number.",
                            "The trapdoor is a small error vector."
                        ],
                        "correct": 1,
                        "explanation": "The core design is to create a public key that looks like a random, hard-to-solve MQ instance, while the private key contains the 'unscrambled' easy-to-solve central map."
                    },
                    {
                        "id": 3,
                        "question": "What is a primary advantage of many multivariate signature schemes?",
                        "options": [
                            "They have the smallest public keys.",
                            "They are the fastest to verify.",
                            "They can produce very short signatures.",
                            "They are the easiest to implement."
                        ],
                        "correct": 2,
                        "explanation": "Compared to other PQC families like hash-based or some lattice-based signatures, multivariate schemes can often produce signatures of a much smaller size, which is advantageous for on-chain applications."
                    }
                ]
            }
        },
        {
            "id": "lesson-14",
            "title": "Rainbow Signature Scheme",
            "duration": "60 min",
            "objectives": [
                "Understand the specification of the Rainbow signature algorithm",
                "Learn about its Unbalanced Oil and Vinegar structure",
                "Describe the layer construction methodology",
                "Analyze the security framework and historical attacks",
                "Discuss implementation optimization and performance"
            ],
            "content": {
                "overview": "Rainbow is a multivariate digital signature scheme that was a finalist in the NIST PQC competition. It builds upon the Oil and Vinegar design to create a more efficient scheme. This lesson explores the layered structure of Rainbow, its performance characteristics, and the cryptanalytic break that led to its removal from consideration as a standard.",
                "sections": [
                    {
                        "title": "Rainbow Algorithm Specification",
                        "content": "<p>Rainbow is a direct extension of the Oil and Vinegar signature scheme. It enhances the efficiency of the basic idea by creating multiple layers of Oil and Vinegar equations.</p><h3>Unbalanced Oil and Vinegar (UOV):</h3><p>The first step is to use an 'unbalanced' structure, meaning there are more vinegar variables than oil variables. This allows for smaller signatures and is a building block for Rainbow.</p><h3>Layered Construction:</h3><p>Rainbow's key innovation is to stack these UOV layers. To sign a message, the signer starts by choosing random values for the vinegar variables in the first layer. This allows them to solve for the first layer's oil variables. These oil variables then become the vinegar variables for the second layer, and the process repeats until all variables are solved. This layered approach allows for a more compact and efficient system compared to a single large UOV scheme.</p>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Performance and Characteristics",
                        "content": "<p>Rainbow was a strong contender in the NIST process due to its exceptional performance characteristics:</p><ul><li><strong>Extremely Fast Signing and Verification:</strong> The simple linear algebra operations made it one of the fastest signature schemes.</li><li><strong>Very Short Signatures:</strong> It produced some of the shortest signatures of any PQC candidate.</li></ul><p>However, these benefits came at the cost of a very large public key, similar to other multivariate and code-based schemes. The public key for Rainbow could be hundreds of kilobytes.</p>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Cryptanalytic Break",
                        "content": "<p>Despite years of security, in early 2022, a new and powerful key recovery attack against Rainbow was discovered. The attack was able to break the parameter sets submitted to NIST over a weekend using a standard laptop.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>A Lesson in Public Cryptanalysis</strong></div><p>The break of Rainbow is a perfect example of why the NIST open competition model is so valuable. It subjected the algorithm to years of intense, global scrutiny. While it is unfortunate that Rainbow was broken, it is far better to discover this weakness during the analysis phase than after it has been deployed in critical systems. The attack demonstrated a fundamental weakness in the layered Oil and Vinegar structure, leading NIST to decide not to standardize Rainbow.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "The Rainbow signature scheme is a layered version of which earlier multivariate design?",
                        "options": [
                            "Hidden Field Equations (HFE)",
                            "Oil and Vinegar",
                            "McEliece",
                            "NTRU"
                        ],
                        "correct": 1,
                        "explanation": "Rainbow's core innovation was to stack multiple 'Unbalanced Oil and Vinegar' (UOV) schemes in layers to improve efficiency and key size."
                    },
                    {
                        "id": 2,
                        "question": "What were the two main performance advantages of the Rainbow signature scheme?",
                        "options": [
                            "Small public keys and fast signing",
                            "Small public keys and small signatures",
                            "Fast signing/verification and small signatures",
                            "Fast verification and small public keys"
                        ],
                        "correct": 2,
                        "explanation": "Rainbow was extremely attractive due to its very high speed and very short signatures, but this came at the cost of a very large public key."
                    },
                    {
                        "id": 3,
                        "question": "What was the ultimate fate of Rainbow in the NIST PQC competition?",
                        "options": [
                            "It was selected as the primary standard for signatures.",
                            "It is still being considered in the 4th round.",
                            "A practical key recovery attack was discovered, and NIST decided not to standardize it.",
                            "It was withdrawn by its designers."
                        ],
                        "correct": 2,
                        "explanation": "In 2022, a new attack was found that completely broke the submitted parameter sets, demonstrating a fundamental weakness. This highlights the success of the public review process in weeding out insecure algorithms."
                    }
                ]
            }
        },
        {
            "id": "lesson-15",
            "title": "Hash-Based Signatures Fundamentals",
            "duration": "75 min",
            "objectives": [
                "Understand the concept of a one-time signature (OTS) scheme like WOTS",
                "Learn how Merkle trees are used to create a many-time signature scheme",
                "Analyze the specifications of XMSS and its multi-tree variant",
                "Differentiate between stateful and stateless hash-based signatures",
                "Grasp the concept of forward security"
            ],
            "content": {
                "overview": "Hash-based signatures are unique in the PQC landscape because their security relies only on the security of the underlying hash function (like SHA-256). They do not depend on novel or less-understood mathematical problems. This lesson explores the building blocks of hash-based signatures, from one-time signatures to the Merkle trees that make them practical.",
                "sections": [
                    {
                        "title": "One-Time Signatures (OTS)",
                        "content": "<p>A one-time signature scheme, like the Lamport signature or its improvement WOTS (Winternitz OTS), allows you to sign exactly one message securely. The private key consists of a set of random numbers. The public key is derived by hashing these numbers. To sign a message, you reveal some of the private key numbers based on the bits of the message hash. An observer can verify the signature by hashing the revealed numbers to see if they match the public key.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>One-Time Use Only!</strong></div><p>Revealing parts of the private key for one signature means you cannot use it again. If you were to sign a second message with the same key, an attacker could forge signatures for other messages. This limitation is why OTS schemes are only a building block.</p></div>",
                        "image": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Merkle Signature Scheme",
                        "content": "<p>To overcome the one-time limitation, Ralph Merkle invented a way to combine many OTS keys into a single many-time public key. This is the <strong>Merkle Signature Scheme</strong>.</p><h3>How it Works:</h3><ol><li>Generate a large number (e.g., 2^20) of OTS key pairs.</li><li>The public key of each OTS pair becomes a 'leaf' in a Merkle tree.</li><li>The root of this Merkle tree becomes the single, public key for the entire scheme.</li><li>To sign a message, you use the *next available* OTS private key (e.g., key #1). The signature consists of the OTS signature itself, plus the corresponding OTS public key and the 'authentication path' (the sibling hashes needed to reconstruct the Merkle root).</li><li>A verifier uses the authentication path to confirm that the OTS public key is a valid leaf in the tree, then verifies the OTS signature.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Stateful vs. Stateless Signatures",
                        "content": "<p>The Merkle Signature Scheme is <strong>stateful</strong>. The signer *must* keep track of which OTS key was used last and never, ever reuse one. This 'state' management is a major practical challenge. If the state is copied (e.g., restoring a wallet from backup) and an OTS key is reused, the security of the entire scheme collapses.</p><p><strong>XMSS</strong> is a standardized stateful hash-based signature scheme. To address the statefulness problem, more complex <strong>stateless</strong> schemes were developed. These schemes, like SPHINCS+, use various clever techniques to avoid the need for the signer to maintain state, but this comes at the cost of larger signature sizes and slower performance.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the security foundation of hash-based signatures?",
                        "options": [
                            "The hardness of factoring",
                            "The hardness of lattice problems",
                            "The security (e.g., collision resistance) of the underlying cryptographic hash function",
                            "The security of multivariate equations"
                        ],
                        "correct": 2,
                        "explanation": "This is their most attractive feature. Their security is very well understood and relies only on standard, time-tested hash functions like SHA-256."
                    },
                    {
                        "id": 2,
                        "question": "What is the major security limitation of a One-Time Signature (OTS) scheme?",
                        "options": [
                            "The signatures are too large.",
                            "Each private key can only be used to sign a single message securely.",
                            "They are very slow.",
                            "They are not resistant to quantum attacks."
                        ],
                        "correct": 1,
                        "explanation": "Signing a message reveals part of the private key, so reusing it for another message allows an attacker to forge signatures. This is why they are called 'one-time'."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary challenge of using a 'stateful' hash-based signature scheme like XMSS?",
                        "options": [
                            "The public key is too large.",
                            "The signer must securely manage the state to ensure a one-time key is never reused.",
                            "They are not quantum-resistant.",
                            "The signatures are too short."
                        ],
                        "correct": 1,
                        "explanation": "The requirement to perfectly manage the state (e.g., the index of the last used key) is a significant practical burden. A simple error like restoring a backup and reusing an old key can lead to a catastrophic security failure."
                    }
                ]
            }
        },
        {
            "id": "lesson-16",
            "title": "SPHINCS+ Stateless Signatures",
            "duration": "75 min",
            "objectives": [
                "Understand the high-level design of the SPHINCS+ algorithm",
                "Learn the role of Few-Time Signature (FTS) schemes like FORS",
                "Describe the hypertree construction used in SPHINCS+",
                "Analyze the trade-offs that make SPHINCS+ stateless",
                "Discuss its selection by NIST as a standard"
            ],
            "content": {
                "overview": "SPHINCS+ is a stateless hash-based signature scheme that was selected by NIST as a standard for PQC signatures. It solves the dangerous 'state management' problem of earlier hash-based designs, making them safe for real-world use. This lesson explores the clever, multi-layered tree structure that makes SPHINCS+ possible.",
                "sections": [
                    {
                        "title": "SPHINCS+ Algorithm Design",
                        "content": "<p>The core challenge for a stateless scheme is: how do you select a unique one-time key to sign a message, without keeping track of which keys you've already used? SPHINCS+ solves this by using the message itself to select the key.</p><p>However, allowing an attacker to choose which key to use is dangerous. SPHINCS+ gets around this with a complex, multi-layered structure of Merkle trees, called a <strong>hypertree</strong>. A single public key at the top of the hypertree certifies a large number of trees below it. The message hash is used to select a random path down through this structure, ultimately selecting a unique one-time key at the very bottom that has never been used before.</p>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "FORS: Forest of Random Subsets",
                        "content": "<p>The one-time signature scheme at the bottom of the SPHINCS+ hypertree is a special <strong>Few-Time Signature (FTS)</strong> scheme called FORS (Forest of Random Subsets). It's more robust than a standard OTS like WOTS.</p><p>In FORS, the private key is a large set of numbers. The hash of the message to be signed is split into chunks, and each chunk is used as an index to select one of the private key numbers to reveal. The signature consists of these revealed numbers and their corresponding authentication paths in a set of Merkle trees (a 'forest'). This is more resilient against attackers who might try to find messages that hash to the same indices.</p>",
                        "image": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Why SPHINCS+? Security and Trade-offs",
                        "content": "<p>NIST selected SPHINCS+ as a standard alongside DILITHIUM for a key reason: <strong>security diversity</strong>. DILITHIUM's security is based on the hardness of lattice problems, which are relatively new and complex. SPHINCS+'s security is based only on the hardness of well-understood cryptographic hash functions.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Conservative Choice</strong></div><p>If a future breakthrough (classical or quantum) were to break lattice-based cryptography, SPHINCS+ would remain secure. It acts as a crucial backup. This high confidence in its security comes at a cost: SPHINCS+ signatures are much larger and the signing process is significantly slower than DILITHIUM. It is intended for use cases where security confidence is paramount and performance is a secondary concern.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary advantage of SPHINCS+ being a 'stateless' hash-based signature scheme?",
                        "options": [
                            "It has smaller signatures than stateful schemes.",
                            "It is faster than stateful schemes.",
                            "It eliminates the need for the signer to securely manage state, preventing accidental key reuse.",
                            "It uses a more secure hash function."
                        ],
                        "correct": 2,
                        "explanation": "State management is the Achilles' heel of stateful schemes. By being stateless, SPHINCS+ is much safer to use in practice, as it's resilient to situations like restoring a wallet from a backup."
                    },
                    {
                        "id": 2,
                        "question": "How does SPHINCS+ determine which one-time key to use for signing a particular message?",
                        "options": [
                            "It uses a counter that increases with every signature.",
                            "It uses a portion of the message's hash to deterministically select a key path in its hypertree structure.",
                            "It asks the user to choose a key.",
                            "It uses the same key every time."
                        ],
                        "correct": 1,
                        "explanation": "The deterministic selection based on the message content is the core of its stateless design. The large hypertree structure ensures that the probability of two different messages selecting the same one-time key is negligible."
                    },
                    {
                        "id": 3,
                        "question": "Why did NIST choose to standardize SPHINCS+ in addition to the lattice-based DILITHIUM?",
                        "options": [
                            "Because SPHINCS+ is faster.",
                            "To provide security diversity; SPHINCS+ relies only on the security of hash functions, which is a different assumption than lattices.",
                            "Because SPHINCS+ has smaller keys.",
                            "Because it was required by law."
                        ],
                        "correct": 1,
                        "explanation": "NIST's strategy is to not put all its eggs in one basket. By standardizing algorithms from different mathematical families, they hedge against the risk of a future breakthrough that might break an entire class of problems (like lattices)."
                    }
                ]
            }
        },
        {
            "id": "lesson-17",
            "title": "Isogeny-Based Cryptography",
            "duration": "75 min",
            "objectives": [
                "Understand the basics of elliptic curve isogenies",
                "Learn about supersingular elliptic curves",
                "Describe the SIDH (Supersingular Isogeny Diffie-Hellman) protocol",
                "Analyze the 'isogeny walk' and the hardness of the isogeny path problem",
                "Discuss the post-quantum security assumptions of this family"
            ],
            "content": {
                "overview": "Isogeny-based cryptography is a fascinating branch of post-quantum cryptography that uses maps between elliptic curves, called isogenies, to build its security. For a time, it was a leading PQC candidate due to its small key sizes. This lesson explores the elegant mathematics behind isogeny-based protocols like SIDH.",
                "sections": [
                    {
                        "title": "Elliptic Curve Isogenies",
                        "content": "<p>In standard elliptic curve cryptography (ECC), we operate on a single, fixed elliptic curve. In isogeny-based cryptography, we work with a whole graph of elliptic curves.</p><p>An <strong>isogeny</strong> is a special kind of map between two elliptic curves that preserves their group structure. You can think of it as a bridge or a path connecting one curve to another. The security of these schemes is based on the fact that while it's easy to 'walk' along an isogeny path from a starting curve to an ending curve, it is computationally very hard to find the specific path that was taken, given only the start and end curves. This is the <strong>isogeny path problem</strong>.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Supersingular Isogeny Diffie-Hellman (SIDH)",
                        "content": "<p>SIDH was the most prominent isogeny-based protocol, designed as a key exchange mechanism similar to classic Diffie-Hellman.</p><h3>The Analogy:</h3><ul><li>In classic DH, Alice and Bob start with a public number 'g'. Alice chooses a secret 'a', Bob chooses a secret 'b'. Alice computes g^a, Bob computes g^b. They exchange these, and can both compute the shared secret g^(ab).</li><li>In SIDH, Alice and Bob start on a public supersingular elliptic curve. Alice chooses a secret isogeny (her private key) and uses it to 'walk' to a new curve, which she sends to Bob. Bob does the same. They exchange their new curves and can use their secret isogenies to compute a shared secret curve.</li></ul>The key advantage of SIDH was its incredibly small key sizes, the smallest of any PQC family.",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Recent Cryptanalytic Developments",
                        "content": "<p>SIDH was a finalist in the NIST competition and was seen as a very promising alternative to lattices. However, in the summer of 2022, a major cryptanalytic breakthrough was announced. A new classical algorithm was discovered that could efficiently solve the mathematical problem underlying SIDH's security.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>SIDH is Broken</strong></div><p>The attack was devastating and completely broke the SIDH protocol. This was another major success for the NIST public review process, as it uncovered a fundamental flaw before the protocol was standardized and deployed. While this specific protocol is broken, research into other forms of isogeny-based cryptography continues, but it has been set back significantly.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is an isogeny in the context of this cryptographic family?",
                        "options": [
                            "A type of elliptic curve.",
                            "A special map or path between two elliptic curves.",
                            "A private key.",
                            "A type of attack."
                        ],
                        "correct": 1,
                        "explanation": "An isogeny is a structured map from one elliptic curve to another. The security of these schemes is based on the difficulty of finding the specific isogeny (the path) given only the starting and ending curves."
                    },
                    {
                        "id": 2,
                        "question": "What was the most significant practical advantage of the SIDH protocol?",
                        "options": [
                            "It was very fast.",
                            "It was easy to implement.",
                            "It had the smallest key and ciphertext sizes of all PQC families.",
                            "It was based on the security of hash functions."
                        ],
                        "correct": 2,
                        "explanation": "SIDH's extremely compact keys made it very attractive for applications where bandwidth and storage are at a premium, which is why it was a leading candidate for a long time."
                    },
                    {
                        "id": 3,
                        "question": "What happened to the SIDH protocol during the NIST PQC competition?",
                        "options": [
                            "It was selected as a standard.",
                            "It was withdrawn by its designers.",
                            "A powerful new classical attack was discovered that completely broke the protocol.",
                            "It was found to be vulnerable to Grover's algorithm."
                        ],
                        "correct": 2,
                        "explanation": "In 2022, a devastating attack demonstrated a fundamental weakness in SIDH's mathematical foundations. This led to its removal from consideration and highlighted the value of public cryptanalysis."
                    }
                ]
            }
        },
        {
            "id": "lesson-18",
            "title": "SIKE Key Encapsulation",
            "duration": "60 min",
            "objectives": [
                "Understand the SIKE protocol specification",
                "Learn how SIDH was transformed into a KEM",
                "Describe the role of the Fujisaki-Okamoto transformation",
                "Analyze implementation considerations",
                "Review the cryptanalytic developments that broke the protocol"
            ],
            "content": {
                "overview": "SIKE (Supersingular Isogeny Key Encapsulation) was the specific submission to the NIST PQC competition that was based on the SIDH protocol. It was a full-fledged Key Encapsulation Mechanism (KEM) designed for real-world use. This lesson examines the structure of SIKE and serves as a case study in how even the most promising cryptographic proposals can fall to new research.",
                "sections": [
                    {
                        "title": "SIKE Protocol Specification",
                        "content": "<p>SIKE was the instantiation of the SIDH key exchange protocol as a KEM. It took the core SIDH idea and wrapped it in the necessary transformations to achieve the high level of security (IND-CCA2) required for a modern standard.</p><p>Like Kyber, it consisted of three algorithms: KeyGen, Encaps, and Decaps. The goal was to produce a shared secret that could be used for hybrid encryption. For several years, it was a 4th round 'alternate' candidate in the NIST process, valued for its unique mathematical foundation and small key sizes.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Transformation and Implementation",
                        "content": "<p>To turn the interactive SIDH key exchange into a secure non-interactive KEM, SIKE employed a variant of the <strong>Fujisaki-Okamoto (FO) transformation</strong>. This is a standard cryptographic technique that uses hashing and specific formatting to convert a basic public-key encryption scheme into one that is secure against chosen-ciphertext attacks.</p><p>The implementation of SIKE was highly complex, involving difficult mathematics on supersingular elliptic curves. This complexity was both a barrier to entry for developers and a rich area for academic research.</p>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The SIKE-breaking Attack",
                        "content": "<p>The attack that broke SIDH/SIKE, discovered by Wouter Castryck and Thomas Decru, was a surprise to many in the cryptographic community. It was not a quantum attack, but a purely classical mathematical attack that exploited properties of the underlying algebraic geometry that were previously not thought to be useful.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>A Case Study in Public Review</strong></div><p>The story of SIKE is a powerful testament to the scientific process. It was a brilliant and beautiful idea that attracted many talented researchers. For years, it held up against all known attacks. But the open, collaborative, and adversarial nature of public academic review ultimately uncovered a flaw. This process, while sometimes resulting in the 'death' of a promising algorithm, is exactly how we build confidence in the ones that survive, like Kyber and Dilithium.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What was SIKE?",
                        "options": [
                            "A digital signature algorithm.",
                            "A new type of blockchain.",
                            "A Key Encapsulation Mechanism (KEM) based on the SIDH protocol, submitted to the NIST competition.",
                            "A symmetric encryption algorithm."
                        ],
                        "correct": 2,
                        "explanation": "SIKE was the specific KEM proposal that formalized and standardized the underlying SIDH key exchange idea for the NIST PQC process."
                    },
                    {
                        "id": 2,
                        "question": "What kind of attack broke the SIKE protocol?",
                        "options": [
                            "A quantum attack using Shor's algorithm.",
                            "A side-channel attack.",
                            "A purely classical, mathematical attack that solved the underlying isogeny problem.",
                            "A fault injection attack."
                        ],
                        "correct": 2,
                        "explanation": "The attack was a major breakthrough in classical cryptanalysis, using advanced mathematics to solve the core problem that was believed to be hard. It did not require a quantum computer."
                    },
                    {
                        "id": 3,
                        "question": "What does the story of SIKE's failure demonstrate about the NIST PQC process?",
                        "options": [
                            "The process is flawed and cannot be trusted.",
                            "The process works as intended, subjecting candidates to intense public scrutiny to find flaws before standardization.",
                            "The process is too slow.",
                            "The process only favors lattice-based cryptography."
                        ],
                        "correct": 1,
                        "explanation": "Finding such a significant break during the public analysis phase is a sign of success, not failure. It prevented a vulnerable algorithm from being deployed in critical infrastructure and strengthened our confidence in the algorithms that survived the same level of scrutiny."
                    }
                ]
            }
        },
        {
            "id": "lesson-19",
            "title": "Symmetric Cryptography in Quantum Era",
            "duration": "60 min",
            "objectives": [
                "Perform a quantum security analysis of the AES cipher",
                "Understand the key length recommendations for post-quantum security",
                "Explore the design principles for post-quantum symmetric schemes",
                "Analyze the quantum security of authenticated encryption (AEAD) modes",
                "Revisit the quantum resistance of major hash functions"
            ],
            "content": {
                "overview": "While public-key cryptography requires a complete replacement, the impact of quantum computers on symmetric cryptography is less severe but still significant. This lesson revisits symmetric ciphers like AES and hash functions like SHA-256 to provide a clear analysis of their quantum security and the straightforward steps needed to ensure they remain safe.",
                "sections": [
                    {
                        "title": "AES Quantum Security Analysis",
                        "content": "<p>The primary quantum threat to symmetric block ciphers like AES is Grover's algorithm, which provides a quadratic speedup for brute-force key search.</p><ul><li><strong>AES-128:</strong> Has a 128-bit key. A classical brute-force attack takes 2^128 operations. A quantum attack using Grover's algorithm takes √2^128 = 2^64 operations. A 64-bit security level is considered breakable and is NOT secure for long-term use.</li><li><strong>AES-256:</strong> Has a 256-bit key. A classical attack takes 2^256 operations. A quantum attack takes √2^256 = 2^128 operations. A 128-bit security level is considered secure against all known attacks, including quantum ones.</li></ul><div class='info-box tip'><div class='info-box-header'><i class='fas fa-lightbulb'></i><strong>The Recommendation</strong></div><p>For this reason, NIST recommends that for data requiring long-term security, systems should migrate from AES-128 to AES-256 to ensure quantum resistance.</p></div>",
                        "image": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum Resistance of Hash Functions",
                        "content": "<p>Hash functions are generally more resistant to quantum attacks than symmetric ciphers.</p><ul><li><strong>Pre-image Resistance:</strong> Finding an input that hashes to a given output 'y' for an n-bit hash function takes 2^n classical work. Grover's algorithm reduces this to 2^(n/2) quantum work. For SHA-256, this is 2^128, which is secure.</li><li><strong>Collision Resistance:</strong> Finding two inputs that hash to the same output takes 2^(n/2) classical work (birthday attack). Quantum attacks provide a smaller speedup, to roughly 2^(n/3). For SHA-256, this is 2^(256/3) ≈ 2^85, which is still considered secure.</li></ul><p>Because of this inherent strength, major hash functions like SHA-256 and SHA-3 are believed to be quantum-resistant at their standard output lengths.</p>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Authenticated Encryption (AEAD)",
                        "content": "<p>Modern cryptographic protocols do not just use encryption; they use <strong>Authenticated Encryption with Associated Data (AEAD)</strong> schemes, such as AES-GCM. These modes provide both confidentiality (the message is secret) and integrity/authenticity (the message cannot be undetectably tampered with).</p><p>The security of these modes depends on the underlying block cipher. As long as the block cipher is secure (i.e., we use AES-256), the AEAD constructions like GCM are also expected to remain secure in a post-quantum world.</p>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "To maintain a 128-bit security level against quantum attacks, which version of AES should be used?",
                        "options": [
                            "AES-128",
                            "AES-192",
                            "AES-256",
                            "AES is not quantum-resistant at all."
                        ],
                        "correct": 2,
                        "explanation": "Grover's algorithm reduces the security of AES-256 from 256 bits to 128 bits (√2^256 = 2^128), which is the target security level for long-term protection."
                    },
                    {
                        "id": 2,
                        "question": "Is SHA-256 considered secure against quantum attacks?",
                        "options": [
                            "No, it is completely broken by Shor's algorithm.",
                            "No, it is completely broken by Grover's algorithm.",
                            "Yes, while Grover's algorithm reduces its collision resistance, the remaining security level is still considered sufficient.",
                            "It is unknown if it is secure."
                        ],
                        "correct": 2,
                        "explanation": "The quantum attacks on hash functions provide less dramatic speedups than on public-key crypto. The consensus is that SHA-256 and SHA-3 are quantum-resistant and do not need to be replaced, although using larger output sizes provides a greater safety margin."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary quantum threat against symmetric cryptography?",
                        "options": [
                            "Shor's Algorithm",
                            "Grover's Algorithm speeding up key search",
                            "Lattice reduction attacks",
                            "Side-channel attacks"
                        ],
                        "correct": 1,
                        "explanation": "Grover's algorithm offers a generic quadratic speedup for search problems, and a brute-force key search is a type of search problem. This is the main impact quantum computers have on symmetric ciphers."
                    }
                ]
            }
        },
        {
            "id": "lesson-20",
            "title": "Hybrid Cryptographic Systems",
            "duration": "75 min",
            "objectives": [
                "Understand the rationale behind classical-quantum hybrid approaches",
                "Learn how to develop a crypto-agile migration strategy",
                "Analyze the backward compatibility requirements for a smooth transition",
                "Explore performance optimization techniques for hybrid systems",
                "Assess the security of combining classical and post-quantum algorithms"
            ],
            "content": {
                "overview": "The transition to post-quantum cryptography will not happen overnight. For a long transitional period, systems will need to be secure against both classical and quantum adversaries. The solution is to use hybrid systems that combine the strengths of both classical and post-quantum algorithms. This lesson explores the design and security of these crucial transitional systems.",
                "sections": [
                    {
                        "title": "Classical-Quantum Hybrid Approach",
                        "content": "<p>A hybrid approach combines a well-established classical algorithm (like ECDH for key exchange) with a new post-quantum algorithm (like Kyber). The idea is to perform both cryptographic operations and combine their results.</p><p>For a key exchange, you would generate an ECDH shared secret AND a Kyber shared secret. The final shared secret used for encryption would be a cryptographic hash of both secrets combined. This is sometimes called 'belt and suspenders' cryptography.</p><div class='info-box tip'><div class='info-box-header'><i class='fas fa-lightbulb'></i><strong>The Security Guarantee</strong></div><p>The resulting hybrid key exchange is secure as long as *at least one* of the constituent algorithms is secure. An attacker would need to break both the classical (ECDH) and the post-quantum (Kyber) schemes to recover the final shared secret. This provides a very strong and conservative security posture during the transition period.</p></div>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Migration Strategy and Backward Compatibility",
                        "content": "<p>Implementing hybrid cryptography is a key part of a crypto-agile migration strategy. It allows for a gradual rollout of PQC without breaking existing systems.</p><h3>Example: TLS 1.3 Hybrid Key Exchange</h3><p>Protocols like TLS (which secures the web) can be updated to support a hybrid key exchange. A PQC-aware client could tell the server it wants to perform both an ECDH and a Kyber key exchange. An older, non-PQC-aware client would simply perform the standard ECDH exchange. This ensures backward compatibility, allowing the ecosystem to migrate at its own pace without a disruptive 'flag day'.</p>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Performance and Optimization",
                        "content": "<p>The main drawback of the hybrid approach is the performance overhead. It requires additional computation for the PQC algorithm and increases the size of the messages that need to be transmitted (e.g., sending both an ECDH public key and a Kyber public key).</p><p>While this overhead is a concern, for many protocols like TLS, it is considered an acceptable price to pay for the significant increase in security and the smooth migration path it provides. As PQC hardware acceleration becomes more common, this performance penalty will decrease.</p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the core security principle of a hybrid classical-quantum cryptographic system?",
                        "options": [
                            "It is twice as fast.",
                            "It is secure as long as at least one of the underlying algorithms (classical or quantum-resistant) remains secure.",
                            "It only uses quantum-resistant algorithms.",
                            "It is less secure but more compatible."
                        ],
                        "correct": 1,
                        "explanation": "The hybrid approach provides a conservative security guarantee. An attacker must break both the time-tested classical algorithm and the new PQC algorithm to compromise the system."
                    },
                    {
                        "id": 2,
                        "question": "Why is the hybrid approach crucial for a smooth migration to PQC?",
                        "options": [
                            "It forces everyone to upgrade at the same time.",
                            "It allows for backward compatibility, so new systems can interoperate with old systems during the transition period.",
                            "It is the cheapest option.",
                            "It is mandated by NIST."
                        ],
                        "correct": 1,
                        "explanation": "Backward compatibility is essential for any large-scale network migration. A hybrid approach allows for a gradual rollout without breaking connectivity for users who have not yet upgraded."
                    },
                    {
                        "id": 3,
                        "question": "What is the main drawback of using a hybrid cryptographic scheme?",
                        "options": [
                            "It is less secure.",
                            "It is not quantum-resistant.",
                            "It has not been studied by cryptographers.",
                            "It introduces performance overhead (larger keys, more computation)."
                        ],
                        "correct": 3,
                        "explanation": "Performing two cryptographic operations instead of one naturally incurs a performance penalty. This is the primary trade-off for the increased security and smoother migration path."
                    }
                ]
            }
        },
        {
            "id": "lesson-21",
            "title": "NIST PQC Standardization Process",
            "duration": "60 min",
            "objectives": [
                "Review the phases and timeline of the NIST PQC competition",
                "Understand the evaluation criteria used by NIST",
                "Perform a deep dive into the finally selected algorithms (Kyber, Dilithium, etc.)",
                "Analyze the official NIST implementation and testing guidelines",
                "Discuss the roadmap for future PQC standards"
            ],
            "content": {
                "overview": "The NIST Post-Quantum Cryptography Standardization Process has been a monumental, multi-year effort to prepare the world for the quantum threat. This lesson provides a detailed look at the process itself, the criteria used to judge the candidates, and the final algorithms selected to become the next generation of public-key standards.",
                "sections": [
                    {
                        "title": "Competition Phases and Timeline",
                        "content": "<p>The process was designed as an open, global competition to ensure the highest level of scrutiny and transparency.</p><ul><li><strong>Round 1 (2017):</strong> 69 candidate algorithms were accepted for consideration.</li><li><strong>Round 2 (2019):</strong> The field was narrowed down to 26 candidates based on initial analysis.</li><li><strong>Round 3 (2020):</strong> 15 candidates were selected as finalists and alternates, representing the most promising schemes for intensive analysis.</li><li><strong>Selection (2022):</strong> NIST announced the first set of algorithms to be standardized: CRYSTALS-Kyber for KEM and CRYSTALS-Dilithium, Falcon, and SPHINCS+ for signatures.</li><li><strong>Draft Standards (2023-2024):</strong> NIST is currently publishing draft standards for these algorithms, with final publication expected soon.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "NIST Evaluation Criteria",
                        "content": "<p>NIST evaluated the candidate algorithms based on three main categories:</p><ol><li><strong>Security:</strong> This was the most important criterion. It included the algorithm's confidence in resisting quantum and classical attacks, the strength of its security proofs, and the hardness of the underlying mathematical problem.</li><li><strong>Performance:</strong> This included factors like key size, signature/ciphertext size, and the speed of key generation, encryption, and decryption on various classical computing platforms.</li><li><strong>Implementation Characteristics:</strong> This covered aspects like the algorithm's flexibility, its simplicity to implement correctly, and its resistance to side-channel attacks.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Selected Algorithms",
                        "content": "<p>In July 2022, NIST announced its selection for the first PQC standards.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>The Winners</strong></div><p><ul><li><strong>Primary Standard for KEM (General Use):</strong> <strong>CRYSTALS-Kyber</strong>. Chosen for its small key sizes and high speed.</li><li><strong>Primary Standard for Signatures (General Use):</strong> <strong>CRYSTALS-Dilithium</strong>. Chosen for its strong security and good performance, particularly its relatively small signature size.</li><li><strong>Additional Standard for Signatures:</strong> <strong>Falcon</strong>. Chosen for cases where Dilithium's signature size is still too large, as Falcon offers even smaller signatures, but is more complex to implement correctly.</li><li><strong>Standard for Signatures (Conservative Backup):</strong> <strong>SPHINCS+</strong>. Chosen for its different security assumption (hash-based) as a hedge against a future break in lattice-based cryptography. It is slower and has larger signatures.</li></ul></p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Which algorithm did NIST select as the primary standard for Post-Quantum Key Encapsulation (KEM)?",
                        "options": [
                            "SIKE",
                            "CRYSTALS-Kyber",
                            "SPHINCS+",
                            "Classic McEliece"
                        ],
                        "correct": 1,
                        "explanation": "Kyber was chosen for general-purpose key exchange due to its strong security and excellent all-around performance characteristics (speed and key size)."
                    },
                    {
                        "id": 2,
                        "question": "Which algorithm did NIST select primarily for its different security foundation (hash-based) as a conservative backup?",
                        "options": [
                            "CRYSTALS-Dilithium",
                            "Falcon",
                            "SPHINCS+",
                            "Kyber"
                        ],
                        "correct": 2,
                        "explanation": "SPHINCS+ was standardized to provide security diversity. Its security relies only on hash functions, making it a good hedge in case an unexpected weakness is ever found in lattice-based cryptography."
                    },
                    {
                        "id": 3,
                        "question": "What was the most important evaluation criterion used by NIST in the PQC competition?",
                        "options": [
                            "Performance (speed)",
                            "Security",
                            "Smallest key size",
                            "Ease of implementation"
                        ],
                        "correct": 1,
                        "explanation": "While performance and implementation characteristics were very important, the fundamental security of the algorithm against both classical and quantum attacks was the primary and non-negotiable requirement."
                    }
                ]
            }
        },
        {
            "id": "lesson-22",
            "title": "PQC Implementation Security",
            "duration": "75 min",
            "objectives": [
                "Understand the threat of side-channel attacks on PQC implementations",
                "Learn about fault injection protection techniques",
                "Describe the importance of constant-time programming",
                "Analyze memory protection and data remanence issues",
                "Recognize hardware-specific security considerations"
            ],
            "content": {
                "overview": "A cryptographic algorithm can be theoretically secure, but a poorly written implementation can still leak secret keys. This lesson moves from theoretical security to practical implementation security, focusing on threats like side-channel attacks and the programming techniques required to mitigate them.",
                "sections": [
                    {
                        "title": "Side-Channel Attacks",
                        "content": "<p>A <strong>side-channel attack</strong> does not attack the mathematics of the algorithm itself, but rather the physical implementation. The attacker measures physical properties of the device performing the cryptographic operation to deduce secret information.</p><h3>Examples:</h3><ul><li><strong>Timing Attacks:</strong> If an operation takes a slightly different amount of time depending on a secret value, an attacker can measure this time difference over many operations to recover the secret.</li><li><strong>Power Analysis:</strong> The power consumption of a CPU can vary slightly depending on the data it is processing and the instruction it is executing. An attacker can monitor this power consumption to reveal secret keys.</li><li><strong>Cache Timing Attacks:</strong> A sophisticated attack where a malicious process on the same CPU can determine which memory locations a cryptographic process is accessing by measuring cache hit/miss timings.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Constant-Time Implementations",
                        "content": "<p>The primary defense against timing and cache-based side-channel attacks is to write <strong>constant-time code</strong>. This means the sequence of instructions and the memory access patterns of the implementation must not depend on any secret data.</p><p>For example, a conditional 'if' statement where the condition depends on a bit of the secret key is not constant-time. An attacker could potentially detect which branch of the 'if' was taken. A constant-time implementation would use clever bitwise operations to achieve the same result without a conditional branch. Writing secure, constant-time code is a highly specialized skill and is a critical requirement for any production-grade cryptographic library.</p>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Fault Injection Attacks",
                        "content": "<p>A <strong>fault injection</strong> or 'glitching' attack involves inducing a physical fault in a device during a cryptographic computation. This could be done by, for example, hitting the chip with a laser or temporarily dropping its voltage.</p><p>The goal is to cause a calculation error. By comparing the correct output with the faulty output, an attacker can often deduce the secret key. Defenses against these attacks involve a combination of hardware-level sensors and software-level checks, such as performing the same calculation twice and ensuring the results are identical.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Physical Access</strong></div><p>These attacks are particularly relevant for securing devices where an attacker might have physical possession, such as smart cards, hardware wallets, or IoT devices.</p></div>",
                        "image": "https://images.unsplash.com/photo-1587620962725-abab7fe55159?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a side-channel attack?",
                        "options": [
                            "An attack that exploits a mathematical flaw in an algorithm.",
                            "An attack that exploits physical leakage of information from a device (e.g., timing, power consumption) to deduce secrets.",
                            "A social engineering attack.",
                            "An attack on the network protocol."
                        ],
                        "correct": 1,
                        "explanation": "Side-channel attacks focus on the implementation rather than the algorithm's theory. They exploit unintentional information channels that leak data about the secret operations being performed."
                    },
                    {
                        "id": 2,
                        "question": "What is the primary goal of writing 'constant-time' cryptographic code?",
                        "options": [
                            "To make the code run as fast as possible.",
                            "To ensure the code's execution time and memory access patterns are independent of any secret values, thus preventing timing attacks.",
                            "To make the code easier to read.",
                            "To reduce the amount of memory the code uses."
                        ],
                        "correct": 1,
                        "explanation": "Constant-time programming is the main software countermeasure against a large class of side-channel attacks. It ensures that an observer cannot learn anything about the secret key by measuring how long operations take."
                    },
                    {
                        "id": 3,
                        "question": "A fault injection attack involves...",
                        "options": [
                            "Sending a malformed message to a server.",
                            "Intentionally introducing errors into a developer's code.",
                            "Physically inducing a calculation error in a chip to learn secret information.",
                            "Exploiting a bug in a compiler."
                        ],
                        "correct": 2,
                        "explanation": "These are active attacks where the adversary manipulates the physical environment of the cryptographic device to cause it to produce incorrect results, which can then be analyzed to reveal the secret key."
                    }
                ]
            }
        },
        {
            "id": "lesson-23",
            "title": "Lattice Cryptanalysis and Attacks",
            "duration": "90 min",
            "objectives": [
                "Understand advanced lattice reduction attack strategies",
                "Learn the principles of the BKZ algorithm",
                "Explore sieving and enumeration algorithms for solving SVP",
                "Analyze the potential of quantum lattice reduction algorithms",
                "Describe how these attacks inform security parameter selection"
            ],
            "content": {
                "overview": "The security of lattice-based cryptography depends on the difficulty of solving problems like the Shortest Vector Problem (SVP). This lesson delves into the attacker's toolkit, exploring the powerful algorithms used by cryptanalysts to attack lattices, from advanced basis reduction techniques like BKZ to quantum algorithms that may one day challenge these systems.",
                "sections": [
                    {
                        "title": "Advanced Lattice Reduction (BKZ)",
                        "content": "<p>The LLL algorithm provides a good approximation of the shortest vector, but for attacking modern cryptosystems, more powerful algorithms are needed. The <strong>Block Korkine-Zolotarev (BKZ)</strong> algorithm is a stronger, but more computationally expensive, lattice reduction algorithm.</p><p>BKZ works by repeatedly calling an SVP oracle (a solver for the Shortest Vector Problem) on smaller, projected sub-lattices (or 'blocks'). By finding the exact shortest vectors in these smaller blocks, it can iteratively improve the overall basis quality far beyond what LLL can achieve. The performance of BKZ is a key benchmark used to determine the security of lattice-based schemes.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "SVP Solvers: Sieving and Enumeration",
                        "content": "<p>The SVP oracles used inside BKZ are algorithms that solve SVP exactly, but their runtime is exponential in the dimension of the lattice. The two main families of SVP solvers are:</p><ul><li><strong>Enumeration:</strong> These algorithms perform a clever, pruned search through the lattice points within a certain radius of the origin to find the shortest one.</li><li><strong>Sieving:</strong> These algorithms start with a very large list of lattice vectors and iteratively combine them to produce shorter and shorter vectors until they converge on the shortest one. Sieving algorithms often have a better theoretical runtime but require a massive amount of memory.</li></ul><p>The concrete performance of these exponential-time algorithms against a given lattice dimension is what determines the real-world security of a set of PQC parameters.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum Algorithms for Lattices",
                        "content": "<p>While lattice problems are believed to be quantum-resistant, quantum computers are still expected to provide some speedup for attacking them. There is active research into quantum algorithms that can solve SVP faster than the best classical sieving or enumeration algorithms.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Impact on Security Parameters</strong></div><p>Current PQC security levels are defined against the best-known *classical* attacks. When selecting parameters for long-term security, cryptographers must also estimate the potential speedup from future quantum lattice solvers. This is why the security parameters for NIST Level 5 (the highest security category) are much larger than those for Level 1, to provide a significant margin of safety against both classical and future quantum cryptanalysis.</p></div>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a3d3?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": []
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Which lattice reduction algorithm is significantly more powerful, but also more computationally expensive, than LLL?",
                        "options": [
                            "Shor's Algorithm",
                            "BKZ (Block Korkine-Zolotarev)",
                            "Grover's Algorithm",
                            "The Euclidean Algorithm"
                        ],
                        "correct": 1,
                        "explanation": "BKZ is a stronger generalization of LLL that provides much better basis reduction by solving SVP on smaller blocks. Its performance is a critical benchmark for lattice security."
                    },
                    {
                        "id": 2,
                        "question": "Sieving and Enumeration are algorithms designed to do what?",
                        "options": [
                            "Encrypt data.",
                            "Sign messages.",
                            "Solve the Shortest Vector Problem (SVP) exactly, though in exponential time.",
                            "Reduce the size of a public key."
                        ],
                        "correct": 2,
                        "explanation": "These are the two primary methods for finding the exact shortest vector in a lattice. They are used as subroutines in attacks and their high (but not impossible) computational cost is what makes lattices secure."
                    },
                    {
                        "id": 3,
                        "question": "How do the results of lattice cryptanalysis research affect the design of PQC schemes?",
                        "options": [
                            "They have no effect.",
                            "They help designers choose security parameters (like the lattice dimension) that are large enough to resist the best-known attacks.",
                            "They are only of theoretical interest.",
                            "They help to make the algorithms run faster."
                        ],
                        "correct": 1,
                        "explanation": "The entire process is an arms race. As cryptanalysts develop better attack algorithms, cryptographers must respond by increasing the size and complexity of their cryptographic parameters to maintain the desired security level."
                    }
                ]
            }
        },
        {
            "id": "lesson-24",
            "title": "Code-Based Cryptanalysis",
            "duration": "75 min",
            "objectives": [
                "Understand the mechanism of Information Set Decoding (ISD)",
                "Learn about structural attacks on specific code families",
                "Explore the principles of algebraic attacks",
                "Analyze the impact of quantum algorithms on code-based crypto",
                "Describe how security parameters are estimated based on these attacks"
            ],
            "content": {
                "overview": "The security of code-based cryptosystems like McEliece relies on the difficulty of the Syndrome Decoding Problem. This lesson explores the attacker's perspective, focusing on the main family of algorithms used to attack this problem—Information Set Decoding—and how its performance dictates the security of these decades-old schemes.",
                "sections": [
                    {
                        "title": "Information Set Decoding (ISD)",
                        "content": "<p><strong>Information Set Decoding (ISD)</strong> is the class of best-known generic attacks against code-based cryptosystems. These are probabilistic algorithms that attempt to solve the Syndrome Decoding Problem.</p><p>The core idea is to guess which 'k' positions of the codeword correspond to the original message (the 'information set'). If the guess is correct and none of these positions contain an error, the attacker can use this information to quickly solve for the error vector. The algorithm repeatedly guesses different information sets until it finds one that is error-free. The complexity of the algorithm depends on the probability of finding such an error-free set.</p>",
                        "image": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Structural Attacks",
                        "content": "<p>While ISD is a generic attack, some code-based proposals have been broken by <strong>structural attacks</strong>. These attacks exploit the specific algebraic structure of the code family being used, rather than treating it as a generic random code. </p><p>For example, if a cryptosystem uses a family of codes that is not well-disguised by the public key scrambling, an attacker might be able to identify the underlying structure and use a specialized, non-generic decoding algorithm to break the system. This is why the choice of code family (like the Goppa codes in Classic McEliece) is so critical.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Estimating Security Parameters",
                        "content": "<p>The security of a code-based system is determined by the complexity of the best known ISD algorithm against its parameters. Cryptographers maintain complex formulas that model the performance of the latest ISD variants (like Prange's, Stern's, or May-Ozerov).</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>A Decades-Long Battle</strong></div><p>The study of ISD algorithms is a mature field. While the algorithms have been consistently improved over the past 50 years, the improvements have been gradual, and the complexity remains exponential. This long history of analysis without any catastrophic breakthrough gives us strong confidence in the security of well-parameterized systems like Classic McEliece. The parameters chosen for the NIST standard are designed to provide a large security margin against all known classical and quantum ISD variants.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the general class of algorithms that represents the best-known attack on cryptosystems like Classic McEliece?",
                        "options": [
                            "Shor's Algorithm",
                            "Lattice Reduction",
                            "Information Set Decoding (ISD)",
                            "Gröbner Basis Algorithms"
                        ],
                        "correct": 2,
                        "explanation": "ISD algorithms are the state-of-the-art for solving the general Syndrome Decoding Problem, and their performance is the primary benchmark for the security of code-based cryptography."
                    },
                    {
                        "id": 2,
                        "question": "What is the core idea of an Information Set Decoding (ISD) attack?",
                        "options": [
                            "To guess the entire secret key.",
                            "To guess which positions in a codeword are error-free, and then solve a simpler problem.",
                            "To build a quantum computer.",
                            "To analyze the power consumption of the decoder."
                        ],
                        "correct": 1,
                        "explanation": "The attack is probabilistic and relies on repeatedly guessing an 'information set' (a subset of the codeword positions) and hoping that the guess corresponds to a set of positions with no errors, which makes the problem easy to solve."
                    },
                    {
                        "id": 3,
                        "question": "Why is the long history of research into ISD algorithms a positive sign for the security of code-based crypto?",
                        "options": [
                            "It means no one is interested in attacking it.",
                            "It means the algorithms are simple.",
                            "It means the field is stagnant.",
                            "It shows that despite decades of intense study, the complexity remains exponential, giving us confidence that no simple solution exists."
                        ],
                        "correct": 3,
                        "explanation": "The slow, gradual improvement in ISD attacks, with no major breakthroughs that change the exponential complexity, provides strong evidence for the underlying hardness of the Syndrome Decoding Problem."
                    }
                ]
            }
        },
        {
            "id": "lesson-25",
            "title": "Multivariate Cryptanalysis",
            "duration": "75 min",
            "objectives": [
                "Understand how Gröbner basis algorithms are used to attack MQ systems",
                "Learn about the XL and F4/F5 algorithms",
                "Explore other attack vectors like differential cryptanalysis",
                "Analyze MinRank attacks on specific schemes",
                "Discuss the potential of quantum algorithms for solving the MQ problem"
            ],
            "content": {
                "overview": "The security of multivariate cryptography relies on the difficulty of the MQ problem. This lesson explores the primary tools of the cryptanalyst in this domain, focusing on powerful algebraic techniques like Gröbner basis algorithms that can be used to solve these systems of polynomial equations and break the underlying cryptosystems.",
                "sections": [
                    {
                        "title": "Gröbner Basis Attacks",
                        "content": "<p>A <strong>Gröbner basis</strong> is a special, 'better' representation of a system of polynomial equations. While the original system may be hard to solve, the Gröbner basis of that system has a triangular-like structure that makes it very easy to solve via simple substitution.</p><p>The primary attack against multivariate cryptosystems is to take the public key (the public system of equations) and compute its Gröbner basis. The complexity of computing a Gröbner basis is, in the worst case, doubly exponential, but for the structured systems used in cryptography, it can sometimes be much faster. The security of a multivariate scheme depends on its ability to resist these attacks.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "XL and F4/F5 Algorithms",
                        "content": "<p>Computing Gröbner bases is a major field of research in computer algebra. The classic method is Buchberger's algorithm. However, more efficient algorithms have been developed for solving systems over finite fields:</p><ul><li><strong>XL (eXtended Linearization):</strong> An algorithm that works by adding new polynomials (multiples of the original ones) to the system until it becomes a large but easy-to-solve system of *linear* equations.</li><li><strong>F4 and F5:</strong> These are highly efficient algorithms for computing Gröbner bases that are based on techniques from linear algebra. They are the state-of-the-art for this type of attack.</li></ul><p>When designing a multivariate scheme, cryptographers must carefully estimate the complexity of algorithms like F5 against their specific construction to choose secure parameters.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Structural Attacks",
                        "content": "<p>Just as with code-based crypto, some multivariate schemes can be vulnerable to <strong>structural attacks</strong> that exploit the specific trapdoor being used, rather than a generic MQ solver.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>The Rainbow Attack</strong></div><p>The attack that broke the Rainbow signature scheme was a structural attack. The researchers found a way to exploit the specific layered Oil and Vinegar structure to recover the secret linear maps (S and T), which allowed them to recover the entire private key without having to solve the underlying hard MQ problem directly. This highlights the danger of trapdoors that are not sufficiently hidden by the public key generation process.</p></div>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary algebraic method used to attack multivariate cryptosystems?",
                        "options": [
                            "Factoring the polynomials.",
                            "Finding the derivative of the polynomials.",
                            "Computing the Gröbner basis of the public system of equations.",
                            "Finding the roots of the polynomials."
                        ],
                        "correct": 2,
                        "explanation": "A Gröbner basis transforms a hard-to-solve system into an equivalent, easy-to-solve one. The attack's complexity lies in the difficulty of computing this basis, which is the main tool for analyzing multivariate schemes."
                    },
                    {
                        "id": 2,
                        "question": "The F4 and F5 algorithms are highly efficient methods for...?",
                        "options": [
                            "Reducing a lattice basis.",
                            "Solving the discrete logarithm problem.",
                            "Computing Gröbner bases.",
                            "Finding collisions in hash functions."
                        ],
                        "correct": 2,
                        "explanation": "F4 and F5 are the state-of-the-art algorithms for Gröbner basis computation and are the benchmark against which the security of multivariate cryptosystems is measured."
                    },
                    {
                        "id": 3,
                        "question": "The successful attack on the Rainbow signature scheme was what kind of attack?",
                        "options": [
                            "A generic attack that solved the underlying MQ problem.",
                            "A quantum attack.",
                            "A side-channel attack.",
                            "A structural attack that specifically exploited its layered Oil and Vinegar design."
                        ],
                        "correct": 3,
                        "explanation": "The attack didn't break the general MQ problem, but rather the specific trapdoor construction of Rainbow. This shows that hiding the secret structure is just as important as the hardness of the underlying problem."
                    }
                ]
            }
        },
        
        {
            "id": "lesson-26",
            "title": "PQC Performance Optimization",
            "duration": "75 min",
            "objectives": [
                "Understand algorithm-specific optimization techniques",
                "Explore the use of hardware acceleration (FPGA, ASIC) for PQC",
                "Learn about memory optimization methods for constrained environments",
                "Analyze the role of parallel processing and vector instructions (AVX2)",
                "Discuss implementation strategies for embedded systems"
            ],
            "content": {
                "overview": "For post-quantum algorithms to be practical, they must be fast. The transition from theory to real-world deployment depends heavily on performance optimization. This lesson covers the software and hardware techniques used to make PQC algorithms run efficiently on everything from large servers to tiny embedded devices.",
                "sections": [
                    {
                        "title": "Algorithm and Software Optimization",
                        "content": "<p>Many PQC algorithms, especially those based on lattices, involve a large number of polynomial multiplications. The speed of this core operation is critical.</p><h3>Key Techniques:</h3><ul><li><strong>Number Theoretic Transform (NTT):</strong> This is an algorithm, similar to the Fast Fourier Transform (FFT), that can multiply large polynomials much faster than the standard 'schoolbook' method. The use of NTT is a key reason for the high performance of schemes like Kyber and Dilithium.</li><li><strong>Vector Instructions (AVX2):</strong> Modern CPUs have special instructions (like AVX2 on x86) that can perform the same operation on multiple pieces of data simultaneously (SIMD). PQC implementations are heavily optimized to use these instructions to parallelize and speed up computations.</li><li><strong>Memory Optimization:</strong> Careful management of memory layout and access patterns is crucial to avoid performance bottlenecks, especially CPU cache misses.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Hardware Acceleration",
                        "content": "<p>For high-throughput applications, specialized hardware can provide a massive performance boost.</p><ul><li><strong>FPGAs (Field-Programmable Gate Arrays):</strong> These are chips that can be reconfigured by a developer. They are excellent for prototyping and deploying hardware-accelerated PQC in specific applications, as they can be tailored to the exact mathematical operations required.</li><li><strong>ASICs (Application-Specific Integrated Circuits):</strong> These are custom-designed chips built for one specific purpose. An ASIC for Kyber, for example, would be vastly more efficient and faster than a general-purpose CPU. In the long term, PQC acceleration will likely be built directly into the main CPUs of servers and devices.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee9 eighty-one c?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Implementations for Constrained Environments",
                        "content": "<p>Implementing PQC on small, resource-constrained devices like IoT sensors or smart cards is a major challenge. These devices have very little RAM and limited processing power.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>The Trade-off Space</strong></div><p>Researchers design special 'lightweight' implementations that prioritize low memory usage over raw speed. This might involve avoiding certain performance-enhancing structures that require large pre-computed tables or processing data in smaller chunks. The goal is to create a secure implementation that can practicably run within the tight constraints of an embedded system.</p></div>",
                        "image": "https://images.unsplash.com/photo-1587620962725-abab7fe55159?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the Number Theoretic Transform (NTT) used for in lattice-based PQC?",
                        "options": [
                            "To sign messages.",
                            "To perform fast polynomial multiplication.",
                            "To generate random numbers.",
                            "To reduce a lattice basis."
                        ],
                        "correct": 1,
                        "explanation": "NTT is a highly efficient algorithm for multiplying polynomials, which is the most computationally intensive part of many lattice-based schemes like Kyber. Its use is a major reason for their high performance."
                    },
                    {
                        "id": 2,
                        "question": "What is the purpose of using vector instructions like AVX2?",
                        "options": [
                            "To decrease the security of the algorithm.",
                            "To perform the same operation on multiple pieces of data at once (SIMD), leading to significant parallelization and speedup.",
                            "To reduce the size of the public key.",
                            "To protect against side-channel attacks."
                        ],
                        "correct": 1,
                        "explanation": "Vector instructions allow for a form of on-CPU parallelism, and PQC implementations are heavily optimized to take advantage of them for performance gains."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary challenge when implementing PQC on a resource-constrained IoT device?",
                        "options": [
                            "The algorithms are not secure enough.",
                            "The devices have too much processing power.",
                            "The limited RAM and computational power of the device, which requires specialized, memory-optimized implementations.",
                            "The lack of network connectivity."
                        ],
                        "correct": 2,
                        "explanation": "Standard PQC implementations may require too much memory to run on a small embedded device. Lightweight cryptography focuses on creating versions that can operate within these tight constraints, often by trading some speed for a smaller memory footprint."
                    }
                ]
            }
        },
        {
            "id": "lesson-27",
            "title": "PQC Protocol Integration",
            "duration": "75 min",
            "objectives": [
                "Understand the process of integrating PQC into the TLS protocol",
                "Learn about quantum-resistant upgrades for SSH and VPNs",
                "Analyze the challenges of migrating email encryption (PGP/S/MIME)",
                "Explore the adoption of PQC in secure messaging apps",
                "Discuss the role of hybrid systems in protocol migration"
            ],
            "content": {
                "overview": "Standardizing new algorithms is only the first step. The real challenge is integrating them into the complex, globally-deployed protocols that run the internet. This lesson covers the practical aspects of upgrading major protocols like TLS, SSH, and VPNs to be quantum-resistant.",
                "sections": [
                    {
                        "title": "Post-Quantum TLS Integration",
                        "content": "<p>Transport Layer Security (TLS) is the protocol that secures the vast majority of internet traffic (the 'S' in HTTPS). Upgrading TLS is a top priority for PQC migration.</p><h3>The Hybrid Approach in TLS 1.3:</h3><p>The IETF is standardizing a hybrid key exchange mechanism for TLS 1.3. When a PQC-aware browser connects to a PQC-aware web server, they will perform *two* key exchanges in parallel:<ul><li>A classical one (e.g., ECDH using curve P-256).</li><li>A post-quantum one (e.g., Kyber-512).</li></ul>The final session key is derived from both secrets. This ensures the connection is secure if either algorithm is secure and maintains backward compatibility with older clients.</p>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Upgrading SSH and VPNs",
                        "content": "<p>Protocols for secure remote access and networking are also being upgraded.</p><ul><li><strong>SSH (Secure Shell):</strong> Used by system administrators to securely log into servers. The standards bodies for SSH are working on defining new key exchange methods that incorporate PQC algorithms.</li><li><strong>VPNs (Virtual Private Networks):</strong> Protocols like IKEv2 and OpenVPN are being extended to support hybrid PQC key exchange to secure the VPN tunnel against future quantum attacks. This is crucial for protecting long-term government and corporate data.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Challenges with Asynchronous Communication",
                        "content": "<p>While real-time protocols like TLS and SSH can be upgraded with hybrid key exchange, asynchronous systems like encrypted email (PGP, S/MIME) face a different challenge.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The 'Harvest Now, Decrypt Later' Threat</strong></div><p>An adversary can record encrypted emails today and store them. Once they have a powerful quantum computer, they can go back and decrypt these stored messages. For data that needs to remain secret for decades, this is a major threat. Migrating these systems is complex, as it involves updating key management, public key servers, and the standards themselves. PQC signatures are also critical for ensuring the long-term authenticity of signed documents and software updates.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary strategy being used to integrate PQC into the TLS 1.3 protocol?",
                        "options": [
                            "Replacing the entire protocol.",
                            "Using a hybrid key exchange that combines a classical and a post-quantum algorithm.",
                            "Waiting until a quantum computer is built.",
                            "Using symmetric encryption for everything."
                        ],
                        "correct": 1,
                        "explanation": "The hybrid approach is the consensus method for TLS migration, as it provides strong security guarantees and allows for backward compatibility, which is essential for the internet."
                    },
                    {
                        "id": 2,
                        "question": "The 'Harvest Now, Decrypt Later' attack is a particularly serious threat for which type of application?",
                        "options": [
                            "Live video streaming.",
                            "Online gaming.",
                            "Asynchronous systems like encrypted email, where data needs to remain secret for many years.",
                            "Cryptocurrency transactions."
                        ],
                        "correct": 2,
                        "explanation": "For data with long-term confidentiality requirements, an attacker can simply store the encrypted data today and wait for a quantum computer to break the classical encryption in the future. This necessitates a more urgent transition to PQC for such data."
                    },
                    {
                        "id": 3,
                        "question": "What is the main benefit of the hybrid approach to protocol migration?",
                        "options": [
                            "It has the best performance.",
                            "It simplifies the protocol.",
                            "It provides security against both classical and quantum attacks and maintains backward compatibility.",
                            "It requires no changes to client software."
                        ],
                        "correct": 2,
                        "explanation": "The hybrid model is a conservative and practical migration strategy that ensures security and prevents the ecosystem from fragmenting during the long transition period."
                    }
                ]
            }
        },
        {
            "id": "lesson-28",
            "title": "Key Management in PQC Era",
            "duration": "75 min",
            "objectives": [
                "Analyze the challenges of managing the lifecycle of post-quantum keys",
                "Understand the process for transitioning Certificate Authorities (CAs) to PQC",
                "Explore the impact of PQC on key escrow and recovery systems",
                "Learn about post-quantum Distributed Key Generation (DKG)",
                "Discuss strategies for long-term key security and rotation"
            ],
            "content": {
                "overview": "The transition to PQC is not just about swapping algorithms; it's about upgrading the entire infrastructure that manages cryptographic keys. From Certificate Authorities to enterprise key vaults, the larger key sizes and new properties of PQC algorithms present significant new challenges for key management.",
                "sections": [
                    {
                        "title": "Post-Quantum Key Lifecycle Management",
                        "content": "<p>The key lifecycle includes generation, distribution, storage, usage, and destruction. PQC impacts every stage.</p><ul><li><strong>Key Generation:</strong> Requires a secure source of randomness, as always.</li><li><strong>Distribution:</strong> The much larger public keys of PQC algorithms can be a challenge for bandwidth- or storage-constrained systems.</li><li><strong>Storage:</strong> Larger keys require more secure storage space. Hardware Security Modules (HSMs) and other key management systems need to be updated to support the new algorithms and key sizes.</li><li><strong>Rotation:</strong> Policies for key rotation may need to be adjusted based on the properties of the new algorithms.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1630569772629-19593122c6a6?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Certificate Authority (CA) Transitions",
                        "content": "<p>The entire Public Key Infrastructure (PKI) that secures the web relies on Certificate Authorities issuing digital certificates. These certificates bind a public key to an identity. The whole chain of trust needs to be upgraded to PQC.</p><h3>The Hybrid Certificate Approach:</h3><p>The likely migration path is for CAs to issue <strong>hybrid certificates</strong>. Such a certificate would contain two public keys and be signed twice:<ul><li>One classical key (e.g., ECDSA) and signature.</li><li>One post-quantum key (e.g., Dilithium) and signature.</li></ul>This would allow new PQC-aware systems to validate the post-quantum signature, while old systems could still validate the classical one, ensuring a gradual transition.</p>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Distributed Key Generation (DKG)",
                        "content": "<p>For systems that require high security and no single point of failure (like cryptocurrency wallets or threshold signature schemes), it's desirable to generate a key in a distributed way, where no single party ever knows the full secret key. This is done with a <strong>Distributed Key Generation (DKG)</strong> protocol.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Post-Quantum DKG</strong></div><p>Developing DKG protocols for post-quantum algorithms is an active area of research. While classical DKG is well-understood, the unique algebraic structure of PQC schemes (especially lattice-based ones) requires the design of new, secure DKG protocols that are compatible with their mathematical properties.</p></div>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a major practical challenge introduced by PQC algorithms for key management systems?",
                        "options": [
                            "The algorithms are too simple.",
                            "The significantly larger public key and signature sizes compared to classical algorithms.",
                            "The keys can only be stored for a short time.",
                            "The algorithms are too fast."
                        ],
                        "correct": 1,
                        "explanation": "The larger key sizes can strain the storage and bandwidth capabilities of existing infrastructure, from smart cards to TLS handshakes, requiring system upgrades."
                    },
                    {
                        "id": 2,
                        "question": "What is a 'hybrid certificate' in the context of a PQC transition?",
                        "options": [
                            "A certificate that is partially encrypted.",
                            "A certificate that contains both a classical key/signature and a post-quantum key/signature.",
                            "A certificate that is valid for both websites and email.",
                            "A certificate that does not expire."
                        ],
                        "correct": 1,
                        "explanation": "Hybrid certificates are a key part of the migration strategy. They allow a new PQC-capable PKI to remain backward-compatible with legacy systems that only understand classical algorithms."
                    },
                    {
                        "id": 3,
                        "question": "What is the goal of a Distributed Key Generation (DKG) protocol?",
                        "options": [
                            "To generate a very long key.",
                            "To allow multiple parties to jointly create a public/private key pair such that no single party ever learns the full private key.",
                            "To generate keys faster.",
                            "To distribute public keys to all users."
                        ],
                        "correct": 1,
                        "explanation": "DKG is a crucial technique for high-security applications, as it removes the single point of failure of having one person or machine generate and hold the master secret key."
                    }
                ]
            }
        },
        {
            "id": "lesson-29",
            "title": "PQC Digital Identity Systems",
            "duration": "60 min",
            "objectives": [
                "Understand the architecture of a Post-Quantum Public Key Infrastructure (PKI)",
                "Analyze the lifecycle of PQC digital signatures",
                "Explore quantum-safe identity verification protocols",
                "Learn about post-quantum revocation mechanisms",
                "Discuss cross-certification between classical and PQC systems"
            ],
            "content": {
                "overview": "Our digital identity is fundamentally tied to public-key cryptography. From the certificates that secure websites to the digital signatures on legal documents, everything needs to be upgraded to be quantum-resistant. This lesson explores the architecture of a post-quantum PKI and the future of quantum-safe digital identity.",
                "sections": [
                    {
                        "title": "Post-Quantum PKI Architectures",
                        "content": "<p>A Public Key Infrastructure (PKI) is the set of hardware, software, policies, and procedures needed to manage digital certificates. A post-quantum PKI will need to be 'crypto-agile', meaning it can support multiple cryptographic algorithms at once.</p><h3>Key Features:</h3><ul><li><strong>Hybrid Certificate Chains:</strong> A root CA might have a classical root certificate and a separate PQC root certificate. It could issue intermediate certificates that are cross-signed by both, allowing for gradual migration.</li><li><strong>PQC-aware Certificate Revocation:</strong> Mechanisms like Certificate Revocation Lists (CRLs) and OCSP must be able to handle certificates with PQC identifiers and signatures.</li><li><strong>New Certificate Formats:</strong> Standards like X.509 will need to be updated to accommodate the larger key and signature sizes of PQC algorithms.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Digital Signature Lifecycles",
                        "content": "<p>Digital signatures are used to provide authenticity, integrity, and non-repudiation. The 'Harvest Now, Decrypt Later' threat doesn't apply to signatures, but a different long-term threat does: an attacker could forge a signature on a document *in the future* once they have a quantum computer.</p><p>For documents that need to remain legally valid for decades (e.g., contracts, wills, government records), it is critical to start using PQC signatures as soon as they are standardized. The migration strategy may involve <strong>dual signing</strong>, where a document is signed with both a classical (ECDSA) and a post-quantum (Dilithium) signature.</p>",
                        "image": "https://images.unsplash.com/photo-1555949963-ff9808202532?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Cross-Certification",
                        "content": "<p>During the long transition period, there will be two parallel PKI hierarchies: the legacy classical one and the new PQC one. To bridge this gap, a mechanism called <strong>cross-certification</strong> can be used.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Building a Bridge of Trust</strong></div><p>A classical CA can issue a certificate for a PQC CA, and the PQC CA can issue a certificate for the classical CA. This creates a bridge of trust between the two worlds, allowing a user who only trusts the classical root to validate a certificate issued in the PQC hierarchy, and vice-versa. This is a critical tool for ensuring a smooth, interoperable migration.</p></div>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What does it mean for a PKI to be 'crypto-agile'?",
                        "options": [
                            "It is very fast.",
                            "It is a new concept that has not been tested.",
                            "It is designed to support multiple cryptographic algorithms simultaneously and can gracefully transition to new ones.",
                            "It only uses one algorithm."
                        ],
                        "correct": 2,
                        "explanation": "Crypto-agility is a core principle for modern PKI. It acknowledges that algorithms will need to be replaced over time and builds the infrastructure to handle that transition smoothly, such as by supporting hybrid certificates."
                    },
                    {
                        "id": 2,
                        "question": "Why is it important to start using PQC signatures for long-term legal documents now?",
                        "options": [
                            "It is not important.",
                            "To protect against future attackers with quantum computers who could forge signatures on those documents.",
                            "PQC signatures are smaller and faster.",
                            "Classical signatures are already broken."
                        ],
                        "correct": 1,
                        "explanation": "The threat to signatures is forward-facing: forgery. If a document's validity needs to be assured for 30 years, it must be signed with an algorithm that is expected to remain secure for 30 years. Using PQC signatures today protects against future forgery."
                    },
                    {
                        "id": 3,
                        "question": "What is the purpose of 'cross-certification' between a classical CA and a PQC CA?",
                        "options": [
                            "To create competition between them.",
                            "To create a bridge of trust, allowing validation of certificates across the two different PKI hierarchies.",
                            "To slowly phase out the classical CA.",
                            "To make certificates more expensive."
                        ],
                        "correct": 1,
                        "explanation": "Cross-certification is a standard technique for establishing trust between different PKI domains. It will be a vital tool for ensuring that users can trust both classical and PQC certificates during the long migration period."
                    }
                ]
            }
        },
        {
            "id": "lesson-30",
            "title": "Quantum Random Number Generation",
            "duration": "60 min",
            "objectives": [
                "Understand the difference between true randomness and pseudorandomness",
                "Learn how quantum phenomena can be used as entropy sources",
                "Describe the process of random number extraction",
                "Explore techniques for verifying quantum randomness",
                "Discuss the role of high-quality randomness in cryptography"
            ],
            "content": {
                "overview": "The security of all cryptography, classical or post-quantum, depends on a source of high-quality, unpredictable random numbers. This lesson explores the ultimate source of true randomness—quantum mechanics—and how Quantum Random Number Generators (QRNGs) can provide a level of unpredictability that classical systems cannot match.",
                "sections": [
                    {
                        "title": "True Randomness vs. Pseudorandomness",
                        "content": "<p>The 'random' numbers generated by a classical computer are not truly random. They are <strong>pseudorandom</strong>. A Pseudorandom Number Generator (PRNG) is a deterministic algorithm that takes a short, secret seed and stretches it into a long sequence of numbers that *appears* random but is entirely predictable if you know the seed.</p><p>A <strong>True Random Number Generator (TRNG)</strong>, on the other hand, gets its unpredictability from a physical source of entropy. This could be thermal noise, atmospheric noise, or, for the highest quality, a quantum process.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum Entropy Sources",
                        "content": "<p>Quantum mechanics is inherently probabilistic and unpredictable. This makes it a perfect source of entropy for a TRNG. A <strong>Quantum Random Number Generator (QRNG)</strong> leverages this property.</p><h3>Common Approaches:</h3><ul><li><strong>Photon Path:</strong> Fire a single photon at a semi-transparent mirror. Quantum mechanics dictates that it is fundamentally unpredictable whether the photon will pass through or be reflected. Detecting which path it took generates a truly random 0 or 1.</li><li><strong>Quantum Tunneling:</strong> Measuring the unpredictable behavior of electrons in a quantum tunneling diode.</li><li><strong>Radioactive Decay:</strong> Measuring the timing of particle emission from a radioactive source (though less common in commercial devices).</li></ul>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a3d3?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Extraction and Verification",
                        "content": "<p>The raw output from a physical entropy source is never perfectly random. It may have slight biases. Therefore, the raw data is passed through a <strong>randomness extractor</strong>, which is a cryptographic algorithm (often based on hash functions) that takes a slightly biased, non-uniform input and produces a shorter, but statistically uniform and unbiased, output string of random bits.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>The Role in Cryptography</strong></div><p>While PQC algorithms themselves don't require a QRNG to run, the security of their key generation absolutely depends on unpredictable randomness. Using a QRNG to generate the secret keys for Kyber or the random values for a Dilithium signature provides the highest possible level of security, as it ensures the seed material is theoretically unpredictable, even to an adversary with complete knowledge of the system's software.</p></div>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the key difference between a PRNG and a TRNG?",
                        "options": [
                            "There is no difference.",
                            "A PRNG is an algorithm that produces a predictable sequence from a seed, while a TRNG gets its unpredictability from a physical process.",
                            "A TRNG is faster than a PRNG.",
                            "A PRNG is more secure than a TRNG."
                        ],
                        "correct": 1,
                        "explanation": "A pseudorandom generator is deterministic and predictable if the seed is known. A true random generator is, by definition, based on an unpredictable physical source of entropy."
                    },
                    {
                        "id": 2,
                        "question": "Why is quantum mechanics a good source of entropy for a TRNG?",
                        "options": [
                            "Because it is very fast.",
                            "Because quantum processes are deterministic.",
                            "Because quantum phenomena like superposition and measurement are fundamentally probabilistic and unpredictable.",
                            "Because it consumes very little energy."
                        ],
                        "correct": 2,
                        "explanation": "The inherent randomness of quantum mechanics at a fundamental level provides a source of unpredictability that is not available in the classical world, making it the gold standard for entropy sources."
                    },
                    {
                        "id": 3,
                        "question": "What is the purpose of a 'randomness extractor'?",
                        "options": [
                            "To generate the initial randomness.",
                            "To make the random numbers predictable.",
                            "To take a slightly biased input from a physical source and produce a shorter, but statistically uniform and unbiased, output.",
                            "To store random numbers."
                        ],
                        "correct": 2,
                        "explanation": "Real-world physical processes are never perfect. A randomness extractor is a crucial post-processing step that 'cleans up' the raw entropy to produce a high-quality, truly random output suitable for cryptographic use."
                    }
                ]
            }
        },
        {
            "id": "lesson-31",
            "title": "Post-Quantum Zero-Knowledge Proofs",
            "duration": "75 min",
            "objectives": [
                "Understand the need for quantum-resistant Zero-Knowledge Proofs",
                "Explore the construction of ZKPs from lattice-based assumptions",
                "Learn about ZK constructions from code-based and multivariate problems",
                "Analyze the challenges in adapting SNARKs and STARKs to be post-quantum",
                "Discuss the applications of PQC ZKPs"
            ],
            "content": {
                "overview": "Zero-Knowledge Proofs (ZKPs) are a cornerstone of modern privacy and scalability solutions, especially in blockchain. However, many existing ZKP systems are not secure against quantum computers. This lesson explores the cutting-edge research into building new ZKP systems based on post-quantum hard problems like lattices.",
                "sections": [
                    {
                        "title": "The Need for Quantum-Resistant ZKPs",
                        "content": "<p>Many of the most efficient and popular ZKP systems, especially zk-SNARKs, get their security from assumptions related to elliptic curves (like the pairing-based assumption). These assumptions are vulnerable to Shor's algorithm, just like ECDSA.</p><p>This means that a quantum computer could not only break the signatures on a blockchain but could also potentially forge the privacy-preserving proofs used in systems like Zcash or ZK-rollups. Therefore, for long-term security, the entire ZKP stack must also be migrated to post-quantum foundations.</p>",
                        "image": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "ZKPs from Lattices",
                        "content": "<p>Building ZKPs from lattice-based assumptions is the most active and promising area of PQC ZKP research. The core idea is to use the Short Integer Solution (SIS) and Learning With Errors (LWE) problems as the foundation for the proof system.</p><p>A prover can demonstrate that they know a 'small' solution (a secret) to a public lattice problem without revealing the solution itself. Early lattice-based proof systems were inefficient, but recent breakthroughs have led to practical and efficient constructions. This is a highly advanced field, but it is critical for the future of private, scalable blockchains.</p>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum-Resistant STARKs",
                        "content": "<p>zk-STARKs are a type of ZKP that are already quantum-resistant by design. Their security is not based on elliptic curves or factoring, but on the collision resistance of the hash function used in their construction.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Collision Resistance is Key</strong></div><p>As we learned in a previous lesson, the security of standard hash functions is only moderately affected by quantum computers (Grover's algorithm). It does not face the catastrophic break that public-key systems do from Shor's algorithm. Because STARKs are based on this more conservative hash-based assumption, they are considered an excellent candidate for post-quantum ZKP systems. Their main drawback is that their proof sizes are significantly larger than SNARKs.</p></div>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Why are many popular zk-SNARK systems not resistant to quantum computers?",
                        "options": [
                            "They are too slow.",
                            "Their security relies on elliptic curve-based assumptions that are broken by Shor's algorithm.",
                            "They have not been audited.",
                            "They are already post-quantum."
                        ],
                        "correct": 1,
                        "explanation": "Many efficient SNARKs rely on problems like bilinear pairings on elliptic curves, which are a form of the discrete logarithm problem and are therefore vulnerable to quantum attacks."
                    },
                    {
                        "id": 2,
                        "question": "Which family of PQC problems is the most promising foundation for building new, quantum-resistant ZKPs?",
                        "options": [
                            "Code-based problems",
                            "Isogeny-based problems",
                            "Lattice-based problems (like LWE and SIS)",
                            "Hash-based problems"
                        ],
                        "correct": 2,
                        "explanation": "Lattice-based cryptography has the right mathematical structure to build efficient and powerful ZKPs, and it is the most active area of research in this field."
                    },
                    {
                        "id": 3,
                        "question": "Why are zk-STARKs considered to be inherently quantum-resistant?",
                        "options": [
                            "They use quantum gates.",
                            "They are a new invention.",
                            "Their security is based on the collision-resistance of hash functions, which are not broken by Shor's algorithm.",
                            "They have a trusted setup."
                        ],
                        "correct": 2,
                        "explanation": "STARKs rely on a more conservative security assumption than SNARKs. Since standard hash functions are only moderately affected by quantum computers, STARKs are a strong candidate for post-quantum privacy and scaling."
                    }
                ]
            }
        },
        {
            "id": "lesson-32",
            "title": "PQC in Blockchain Systems",
            "duration": "75 min",
            "objectives": [
                "Understand the meaning of 'blockchain quantum resistance'",
                "Explore the design of post-quantum consensus mechanisms",
                "Analyze the challenges of creating quantum-safe smart contracts",
                "Learn about cryptocurrency migration strategies",
                "Discuss the specific adaptations needed for DeFi protocols"
            ],
            "content": {
                "overview": "Blockchain technology is a prime target for quantum adversaries due to the high value of the assets secured. This lesson explores the multifaceted challenge of making a blockchain quantum-resistant, from the signature schemes that protect user funds to the consensus mechanisms that secure the network itself.",
                "sections": [
                    {
                        "title": "What Makes a Blockchain Quantum-Resistant?",
                        "content": "<p>A blockchain is considered quantum-resistant if all of its core cryptographic components are secure against quantum attacks.</p><h3>This requires:</h3><ul><li><strong>Quantum-Resistant Signatures:</strong> The digital signature scheme used to authorize transactions must be post-quantum (e.g., Dilithium instead of ECDSA). This is the most critical component for protecting user funds.</li><li><strong>Quantum-Resistant Consensus:</strong> Any cryptography used in the consensus mechanism (e.g., for validator selection or randomness generation) must also be quantum-safe.</li><li><strong>Quantum-Resistant Privacy:</strong> If the blockchain uses privacy features like ZKPs, those must be upgraded to post-quantum versions.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1639322537228-f710d846310a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Cryptocurrency Migration Strategies",
                        "content": "<p>Migrating a live, decentralized cryptocurrency like Bitcoin or Ethereum to PQC is an immense challenge. Users cannot be forced to upgrade. The process must be gradual and voluntary.</p><h3>A Likely Migration Path:</h3><ol><li>Introduce support for PQC addresses and signature schemes in a hard fork.</li><li>Users can voluntarily move their funds from their old, ECDSA-based addresses to new, PQC-based addresses.</li><li>This would likely be a one-way 'claim' process. For a period, both types of addresses would co-exist.</li><li>Eventually, the community might decide to phase out support for the old addresses, but this would be a long and complex social and technical process.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1641829032486-0733a01728a5?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum-Safe Smart Contracts",
                        "content": "<p>Smart contracts themselves do not typically perform cryptographic operations like signing. However, they very frequently *verify* signatures. For example, a multi-signature wallet is a smart contract that checks if a transaction has been signed by a valid set of owners.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Gas Costs and Complexity</strong></div><p>PQC signature schemes have larger signatures and more complex verification logic than ECDSA. This means that verifying a PQC signature on-chain inside a smart contract will consume significantly more gas. This increased cost is a major challenge for the design of quantum-safe smart contracts and DeFi protocols, as it could make complex multi-signature or DAO governance schemes prohibitively expensive.</p></div>",
                        "image": "https://images.unsplash.com/photo-1640951613773-54706e06851d?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the most critical cryptographic component to upgrade for protecting user funds on a blockchain?",
                        "options": [
                            "The hash function used for blocks.",
                            "The digital signature scheme (e.g., ECDSA).",
                            "The network communication protocol.",
                            "The programming language of the smart contracts."
                        ],
                        "correct": 1,
                        "explanation": "The digital signature scheme is what prevents unauthorized spending. A quantum computer breaking ECDSA would allow an attacker to forge signatures and steal funds from any address whose public key is known."
                    },
                    {
                        "id": 2,
                        "question": "What is a major challenge for verifying PQC signatures inside a smart contract?",
                        "options": [
                            "The signatures are too secure.",
                            "The verification logic is often more complex and the signatures are larger, leading to much higher gas costs.",
                            "It is impossible to do.",
                            "Smart contracts cannot perform cryptographic operations."
                        ],
                        "correct": 1,
                        "explanation": "The increased computational cost (gas) of PQC signature verification is a significant hurdle for on-chain applications like multi-sig wallets and DAOs, which rely heavily on signature checks."
                    },
                    {
                        "id": 3,
                        "question": "How is a major cryptocurrency like Ethereum likely to migrate to PQC?",
                        "options": [
                            "It will happen automatically overnight.",
                            "It will require a hard fork to introduce support for new PQC address types, followed by a long, voluntary migration period for users.",
                            "It is not possible to migrate it.",
                            "All users will be sent new keys in the mail."
                        ],
                        "correct": 1,
                        "explanation": "Due to the decentralized nature of these networks, any upgrade must be opt-in. The most plausible strategy is a planned hard fork that adds PQC capabilities, allowing users to move their assets to new, secure addresses over time."
                    }
                ]
            }
        },
        {
            "id": "lesson-33",
            "title": "Quantum-Safe IoT Security",
            "duration": "60 min",
            "objectives": [
                "Understand the need for lightweight PQC in IoT",
                "Analyze the challenges of implementing PQC on resource-constrained devices",
                "Explore quantum-safe authentication protocols for IoT",
                "Learn about secure firmware update mechanisms in the PQC era",
                "Discuss the importance of energy-efficient cryptography"
            ],
            "content": {
                "overview": "The Internet of Things (IoT) consists of billions of small, resource-constrained devices that require secure communication for decades. These devices are particularly vulnerable to the 'Harvest Now, Decrypt Later' threat and need lightweight, efficient post-quantum cryptography to remain secure in the future.",
                "sections": [
                    {
                        "title": "Lightweight PQC for IoT",
                        "content": "<p>IoT devices, such as sensors, smart meters, and medical implants, have very limited resources:</p><ul><li><strong>Low CPU Power:</strong> They use small microcontrollers, not powerful processors.</li><li><strong>Minimal RAM:</strong> Memory is often measured in kilobytes, not gigabytes.</li><li><strong>Low Power Consumption:</strong> Many devices run on batteries and must operate for years.</li></ul><p>This means that large, computationally intensive PQC algorithms may not be suitable. <strong>Lightweight cryptography</strong> is a subfield focused on designing algorithms that are secure but can also meet these strict performance and energy constraints.</p>",
                        "image": "https://images.unsplash.com/photo-1587560699334-cc426240a24a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Challenges and Algorithm Selection",
                        "content": "<p>While NIST's selected PQC standards are highly performant on servers and laptops, they can still be challenging for the smallest IoT devices.</p><h3>The Trade-offs:</h3><ul><li><strong>Lattice-based (Kyber/Dilithium):</strong> Generally offer the best all-around performance. However, the need for large polynomial multiplications and secure randomness generation can be demanding for a tiny microcontroller.</li><li><strong>Hash-based (SPHINCS+):</strong> Signatures are very large, which is a problem for bandwidth-constrained IoT networks. Verification is fast, but signing is very slow, which may not be suitable for all applications.</li><li><strong>Code-based / Multivariate:</strong> Often have very large public keys, which can be difficult to store on a device with limited memory.</li></ul><p>Choosing the right algorithm for an IoT use case requires a careful analysis of these trade-offs.</p>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Secure Firmware Updates",
                        "content": "<p>One of the most critical security functions for an IoT device is the <strong>secure firmware update</strong>. The manufacturer needs to be able to patch security vulnerabilities over the air. This process relies on a digital signature to ensure that the device only accepts authentic, untampered updates.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Long-Term Authenticity</strong></div><p>If the signature scheme used for firmware updates (e.g., ECDSA) is broken by a quantum computer in the future, an attacker could forge a valid signature and push a malicious update to the entire fleet of devices, creating a massive botnet. Therefore, firmware signing must be migrated to a quantum-resistant signature scheme like Dilithium or SPHINCS+ to ensure the long-term integrity of these devices.</p></div>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee9 eighty-one c?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary focus of 'lightweight cryptography'?",
                        "options": [
                            "Creating algorithms with weaker security.",
                            "Designing secure algorithms that can run efficiently on resource-constrained devices like IoT sensors.",
                            "Algorithms that are easy to understand.",
                            "Algorithms that have small source code."
                        ],
                        "correct": 1,
                        "explanation": "Lightweight crypto is about finding the right balance of security, performance, and resource usage (RAM, CPU cycles, energy) to make cryptography practical for the billions of small devices that make up the IoT."
                    },
                    {
                        "id": 2,
                        "question": "Why is PQC particularly important for IoT devices with a long operational life?",
                        "options": [
                            "Because IoT devices are faster than servers.",
                            "They are especially vulnerable to the 'Harvest Now, Decrypt Later' attack, as their communications may need to be kept secret for 10-20 years.",
                            "PQC algorithms consume less power.",
                            "It is not important for IoT devices."
                        ],
                        "correct": 1,
                        "explanation": "A smart meter or medical device installed today might be in the field for decades. The data it transmits must be protected by cryptography that can withstand a future quantum computer."
                    },
                    {
                        "id": 3,
                        "question": "Why is a quantum-resistant signature scheme critical for secure firmware updates?",
                        "options": [
                            "It makes the updates download faster.",
                            "It protects against future attackers forging a valid signature and pushing malicious firmware to devices.",
                            "It reduces the size of the firmware.",
                            "It allows users to write their own firmware."
                        ],
                        "correct": 1,
                        "explanation": "The authenticity of firmware updates is paramount. A break in the signature scheme would give an attacker complete control over the device, so the signature must remain secure for the device's entire lifetime."
                    }
                ]
            }
        },
        {
            "id": "lesson-34",
            "title": "Cloud Security in Quantum Era",
            "duration": "75 min",
            "objectives": [
                "Understand the design of quantum-safe cloud architectures",
                "Explore the evolution of Fully Homomorphic Encryption (FHE)",
                "Learn about quantum-safe Secure Multi-Party Computation (MPC)",
                "Analyze the challenges for cloud-based Key Management Systems (KMS)",
                "Discuss the future of privacy-preserving computation"
            ],
            "content": {
                "overview": "Cloud computing platforms are the backbone of the modern internet. Securing the massive amounts of data and communication within the cloud against quantum threats is a top priority. This lesson explores the migration of cloud services to PQC and the advanced cryptographic techniques, like homomorphic encryption and MPC, that will enable secure cloud computing in the future.",
                "sections": [
                    {
                        "title": "Quantum-Safe Cloud Architectures",
                        "content": "<p>Major cloud providers (Amazon, Google, Microsoft) are actively integrating PQC into their services. This involves a multi-layered approach:</p><ul><li><strong>Secure Communication:</strong> Upgrading all internal and external network connections (TLS, VPNs) to use hybrid PQC key exchange.</li><li><strong>Key Management Services (KMS):</strong> Upgrading their cloud-based HSMs and KMS offerings to support the generation, storage, and use of PQC keys.</li><li><strong>Secure Storage:</strong> Ensuring that data encrypted at rest is protected with quantum-resistant algorithms.</li><li><strong>Crypto-Agility:</strong> Building their services to be crypto-agile, so they can easily swap in new algorithms as standards evolve.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Post-Quantum Homomorphic Encryption (FHE)",
                        "content": "<p><strong>Fully Homomorphic Encryption (FHE)</strong> is a powerful form of encryption that allows you to perform computations directly on encrypted data, without decrypting it first. A cloud server could, for example, analyze an encrypted medical database without ever having access to the sensitive patient data.</p><p>Interestingly, most modern FHE schemes are themselves based on the hardness of lattice problems (specifically, LWE). This means they are already believed to be quantum-resistant. As FHE becomes more efficient, it will be a critical tool for secure cloud computing in the PQC era.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum-Safe Secure Multi-Party Computation (MPC)",
                        "content": "<p>MPC allows multiple parties to jointly compute a function while keeping their individual inputs private. It is heavily used in the cloud for applications like private machine learning and secure data analytics.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Migrating MPC</strong></div><p>Many MPC protocols rely on public-key cryptography (like discrete-log-based assumptions) for their security. To be secure in the long term, these protocols must be rebuilt using post-quantum primitives. This involves designing new, quantum-safe protocols for core MPC components like Oblivious Transfer and threshold cryptography, often based on lattice assumptions.</p></div>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is Fully Homomorphic Encryption (FHE)?",
                        "options": [
                            "A type of signature.",
                            "An encryption method that allows computation on encrypted data without decrypting it first.",
                            "An encryption method that is vulnerable to quantum attacks.",
                            "A way to speed up cloud servers."
                        ],
                        "correct": 1,
                        "explanation": "FHE is a revolutionary form of encryption that enables privacy-preserving computation, allowing a third party (like a cloud provider) to process data while it remains encrypted."
                    },
                    {
                        "id": 2,
                        "question": "Why are most modern Fully Homomorphic Encryption (FHE) schemes already considered quantum-resistant?",
                        "options": [
                            "They are very new.",
                            "They are based on the hardness of lattice problems like LWE, which are believed to be quantum-resistant.",
                            "They do not use public-key cryptography.",
                            "They run on quantum computers."
                        ],
                        "correct": 1,
                        "explanation": "The same mathematical foundations (lattices) that are being used to build PQC standards like Kyber are also the foundation for the most efficient FHE schemes, giving them a built-in resistance to quantum attacks."
                    },
                    {
                        "id": 3,
                        "question": "What is a key priority for cloud providers in their transition to PQC?",
                        "options": [
                            "Deleting all customer data.",
                            "Increasing prices.",
                            "Upgrading their internal network protocols (TLS) and customer-facing services (KMS) to support hybrid PQC algorithms.",
                            "Switching to their own private blockchain."
                        ],
                        "correct": 2,
                        "explanation": "Cloud providers must secure their own vast infrastructure and also provide their customers with the tools (like quantum-safe KMS) needed to secure their own applications, making it a two-pronged migration effort."
                    }
                ]
            }
        },
        {
            "id": "lesson-35",
            "title": "PQC Formal Verification",
            "duration": "75 min",
            "objectives": [
                "Understand the role of formal methods in high-assurance cryptography",
                "Learn how security proofs are machine-verified",
                "Explore techniques for proving implementation correctness against a specification",
                "Analyze the capabilities of automated verification tools",
                "Discuss the importance of formal specification languages"
            ],
            "content": {
                "overview": "A security proof written on a whiteboard is one thing; a software implementation with millions of lines of code is another. Formal verification is the use of mathematical methods to prove the correctness of hardware and software. In high-stakes cryptography, it provides the highest possible level of assurance that a system is secure and implemented correctly.",
                "sections": [
                    {
                        "title": "Formal Methods for PQC",
                        "content": "<p>Formal verification can be applied at multiple levels of the cryptographic stack:</p><ul><li><strong>Proof Verification:</strong> The complex mathematical security proofs for PQC algorithms can be written in a formal language (like EasyCrypt or Coq) and checked by a computer. This eliminates the possibility of human error in a complex, multi-page mathematical proof.</li><li><strong>Implementation Correctness:</strong> A formal proof can be constructed to show that a specific piece of software (e.g., a C implementation of Kyber) is a correct implementation of the abstract mathematical specification. This proves the code does exactly what the specification says it should.</li><li><strong>Security Properties:</strong> It's possible to prove that an implementation is free from certain classes of bugs, such as side-channel vulnerabilities (by proving constant-time execution) or buffer overflows.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Automated Verification Tools",
                        "content": "<p>While full formal verification is extremely labor-intensive, there is a spectrum of tools that provide different levels of assurance.</p><ul><li><strong>Model Checkers:</strong> These tools can explore all possible states of a system to check if a certain property (e.g., 'the secret key never leaks') holds true.</li><li><strong>Symbolic Execution:</strong> Tools like Manticore analyze a program by exploring paths through it with symbolic variables instead of concrete data, which can be used to find vulnerabilities.</li><li><strong>Automated Theorem Provers:</strong> These systems can automatically prove mathematical theorems that are part of a security proof.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Future of High-Assurance Crypto",
                        "content": "<p>The complexity of PQC algorithms makes formal verification more important than ever. The subtle algebraic properties can be a source of implementation bugs that traditional testing might miss.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Verified Implementations</strong></div><p>As part of the NIST PQC process, researchers have produced formally verified implementations of some of the standard algorithms. These high-assurance libraries, while perhaps not the absolute fastest, provide a rock-solid foundation and a reference against which other implementations can be tested. For mission-critical applications, using a formally verified cryptographic library will become the gold standard.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary goal of formal verification in cryptography?",
                        "options": [
                            "To make the code run faster.",
                            "To use mathematical methods to prove the correctness and security of a cryptographic proof or implementation.",
                            "To replace the need for testing.",
                            "To automatically generate cryptographic code."
                        ],
                        "correct": 1,
                        "explanation": "Formal verification provides a level of assurance that goes beyond testing. It can prove that a system is correct for all possible inputs, not just the ones that were tested."
                    },
                    {
                        "id": 2,
                        "question": "What does it mean to prove 'implementation correctness'?",
                        "options": [
                            "The code has no bugs.",
                            "The code is well-commented.",
                            "To prove that a specific piece of software is a faithful implementation of its mathematical specification.",
                            "The code was written by a famous cryptographer."
                        ],
                        "correct": 2,
                        "explanation": "This is a critical step. It bridges the gap between the abstract math of a security proof and the concrete reality of the code running on a machine, ensuring that no bugs were introduced during implementation."
                    },
                    {
                        "id": 3,
                        "question": "Why is formal verification particularly valuable for PQC algorithms?",
                        "options": [
                            "Because they are simpler than classical algorithms.",
                            "Because it is a requirement from NIST.",
                            "Because their high mathematical complexity can lead to subtle implementation bugs that are hard to find with traditional testing.",
                            "Because it makes them quantum-resistant."
                        ],
                        "correct": 2,
                        "explanation": "The complex algebra of lattices or codes can be a source of tricky bugs. Formal verification provides the highest level of confidence that these complex algorithms have been implemented correctly and securely."
                    }
                ]
            }
        },
        {
            "id": "lesson-36",
            "title": "Quantum Cryptanalysis Simulation",
            "duration": "75 min",
            "objectives": [
                "Understand the goals of quantum algorithm simulation",
                "Learn about resource estimation methodologies for quantum attacks",
                "Analyze how attack timelines are predicted",
                "Explore quantum advantage modeling",
                "Discuss the role of cryptanalytic tool development"
            ],
            "content": {
                "overview": "While a large-scale, fault-tolerant quantum computer doesn't exist yet, we don't need to wait for one to be built to understand its power. Researchers use sophisticated simulation and resource estimation techniques to predict exactly what it will take to break current cryptography. This lesson explores the field of quantum cryptanalysis and how we model future threats.",
                "sections": [
                    {
                        "title": "Resource Estimation Methodologies",
                        "content": "<p>The key question for cryptanalysts is: 'How many qubits and how much time would it take for a given quantum computer to break a specific algorithm?' <strong>Resource estimation</strong> is the process of answering this question.</p><p>Researchers take a quantum algorithm like Shor's and compile it down to a sequence of basic quantum gates. They then analyze this circuit, taking into account the expected error rates of future quantum hardware, to produce a detailed estimate of the required resources, including:<ul><li>Total number of physical qubits.</li><li>Number of logical (error-corrected) qubits.</li><li>Circuit depth (runtime).</li><li>Number of specific, resource-intensive gates (like T-gates).</li></ul></p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Attack Timeline Prediction",
                        "content": "<p>Predicting the future of technology is notoriously difficult, but by combining resource estimates with projections of quantum computing hardware progress, experts can make educated guesses about when the threat will become practical.</p><p>This involves tracking the progress of major quantum hardware companies (like Google, IBM, Quantinuum) on metrics like qubit count and error rates. While timelines vary, many experts believe a cryptographically relevant quantum computer could be built within the next 10-20 years. This is the timeline that organizations must plan for.</p>",
                        "image": "https://images.unsplash.com/photo-1502920514358-197e44948a35?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Global Risk Assessment",
                        "content": "<p>Organizations like the Global Risk Institute and various government agencies use these resource estimates to create risk assessments. For example, the statement 'AES-256 provides 128 bits of post-quantum security' is a conclusion derived from this type of detailed analysis.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Informing the Migration</strong></div><p>These detailed estimates are what inform the entire PQC migration effort. They provide the concrete data that allows organizations like NIST to set security strength categories and helps system owners to understand the urgency of the threat and prioritize their migration efforts. It turns the abstract threat of a 'quantum computer' into a quantifiable risk that can be managed.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary goal of 'resource estimation' in quantum cryptanalysis?",
                        "options": [
                            "To build a quantum computer.",
                            "To estimate the number of qubits and time required for a quantum computer to break a given cryptosystem.",
                            "To design new PQC algorithms.",
                            "To find bugs in classical software."
                        ],
                        "correct": 1,
                        "explanation": "Resource estimation is a critical field that translates the theoretical threat of quantum algorithms into concrete numbers, providing the data needed for risk assessment and migration planning."
                    },
                    {
                        "id": 2,
                        "question": "How are attack timelines for quantum computers predicted?",
                        "options": [
                            "By guessing randomly.",
                            "By combining the results of resource estimation with projections of progress in quantum hardware development.",
                            "They cannot be predicted.",
                            "By waiting for a company to announce a breakthrough."
                        ],
                        "correct": 1,
                        "explanation": "Experts use a data-driven approach, modeling the requirements of the attack algorithms and the likely trajectory of hardware improvements to create a probable window for when the threat will become practical."
                    },
                    {
                        "id": 3,
                        "question": "Why is this type of simulation and estimation important, even though a large-scale quantum computer doesn't exist yet?",
                        "options": [
                            "It is not important.",
                            "It is purely an academic exercise.",
                            "It allows us to quantify the risk and provides the concrete justification for the global migration to PQC.",
                            "It helps to design better classical computers."
                        ],
                        "correct": 2,
                        "explanation": "Due to the 'Harvest Now, Decrypt Later' threat, we cannot wait until a quantum computer is built to start migrating. These estimates provide the necessary data to understand the urgency and to design new systems that will be secure against the future threat."
                    }
                ]
            }
        },
        {
            "id": "lesson-37",
            "title": "PQC Standardization Beyond NIST",
            "duration": "60 min",
            "objectives": [
                "Learn about other international standardization efforts (ETSI, ISO/IEC)",
                "Explore regional PQC initiatives (e.g., in Europe and China)",
                "Understand the role of industry-specific standards bodies",
                "Discuss the importance of open-source and academic standardization",
                "Analyze the goal of global cryptographic harmonization"
            ],
            "content": {
                "overview": "While the NIST PQC process is the most prominent, it is not the only standardization effort in the world. This lesson provides a global perspective, exploring the work of other international, regional, and industry-specific bodies that are also defining the future of quantum-safe cryptography.",
                "sections": [
                    {
                        "title": "International Standardization Efforts",
                        "content": "<p>Other major international standards bodies are also working on PQC.</p><ul><li><strong>ISO/IEC (International Organization for Standardization / International Electrotechnical Commission):</strong> This body is working on its own multi-part standard for PQC (ISO/IEC 18033-5), which includes algorithms like FrodoKEM and SIKE (prior to its break).</li><li><strong>IETF (Internet Engineering Task Force):</strong> The IETF is the body that standardizes internet protocols like TLS and SSH. They are working closely with NIST to create the standards for integrating the NIST-selected algorithms into these core internet protocols.</li><li><strong>ETSI (European Telecommunications Standards Institute):</strong> A key player in Europe, ETSI is also running workshops and producing reports to guide the European transition to PQC.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Regional and National Initiatives",
                        "content": "<p>Several countries and regions have their own PQC initiatives, often with a focus on their specific national security and economic needs.</p><ul><li><strong>China:</strong> The Chinese Association for Cryptologic Research (CACR) has been running its own PQC competition, with a different set of candidate algorithms and design philosophies.</li><li><strong>Europe:</strong> Various EU-funded projects (like PROMETHEUS and PQ-CRYPTO) have been crucial for driving PQC research and preparing European industry for the transition.</li><li><strong>Germany (BSI):</strong> The German Federal Office for Information Security (BSI) has also published its own recommendations for PQC algorithms, which closely track the NIST process.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Goal of Harmonization",
                        "content": "<p>While multiple standardization efforts exist, the ultimate goal for the global cryptographic community is <strong>harmonization</strong>. The internet is a global network, and it works because everyone uses the same standard protocols.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Interoperability is Key</strong></div><p>It is crucial that these different standards bodies ultimately converge on a compatible set of algorithms and protocols. A fragmented world where different regions use different, incompatible PQC standards would be a major blow to internet security and interoperability. For this reason, there is a high degree of collaboration and communication between these different organizations, with the NIST process acting as the primary focal point.</p></div>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Which organization is responsible for standardizing the integration of PQC into internet protocols like TLS?",
                        "options": [
                            "NIST",
                            "ISO/IEC",
                            "IETF (Internet Engineering Task Force)",
                            "ETSI"
                        ],
                        "correct": 2,
                        "explanation": "While NIST standardizes the core algorithms, the IETF is responsible for the practical work of defining how those algorithms are used within the specific protocols that run the internet."
                    },
                    {
                        "id": 2,
                        "question": "Is the NIST PQC competition the only such effort in the world?",
                        "options": [
                            "Yes, it is the only one.",
                            "No, other regions like China and Europe have their own parallel initiatives and competitions.",
                            "Yes, but it is not public.",
                            "No, but the others are not important."
                        ],
                        "correct": 1,
                        "explanation": "Several countries and standards bodies have their own PQC programs, reflecting the global importance of this transition. The NIST process is simply the largest and most influential."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary goal of 'harmonization' between different PQC standardization bodies?",
                        "options": [
                            "To create more competition.",
                            "To ensure that different countries and regions adopt compatible, interoperable standards to avoid fragmenting the internet.",
                            "To slow down the adoption of PQC.",
                            "To choose a single algorithm for all purposes."
                        ],
                        "correct": 1,
                        "explanation": "For global systems like the internet to function, they must be based on common standards. Harmonization is the process of ensuring that the PQC standards adopted around the world can work together seamlessly."
                    }
                ]
            }
        },
        {
            "id": "lesson-38",
            "title": "Economic Impact of Quantum Computing",
            "duration": "60 min",
            "objectives": [
                "Perform a cost-benefit analysis of the quantum transition",
                "Assess the impact of quantum computing on various industries",
                "Analyze the economics of the migration timeline",
                "Explore risk management strategies for organizations",
                "Learn about investment planning for a quantum-safe future"
            ],
            "content": {
                "overview": "The transition to post-quantum cryptography is not just a technical challenge; it is a massive economic undertaking that will affect every industry. This lesson explores the economic impact of the quantum threat, the costs of migration, and the strategic importance of investing in a quantum-safe future.",
                "sections": [
                    {
                        "title": "The Cost of Inaction",
                        "content": "<p>The economic impact of a 'Q-Day'—the day a cryptographically relevant quantum computer is revealed—would be catastrophic. It would instantly render much of our digital infrastructure insecure, leading to:</p><ul><li>A collapse of trust in e-commerce and digital finance.</li><li>The potential for massive theft of digital assets (cryptocurrencies).</li><li>The compromise of all stored encrypted data and digital signatures.</li><li>A breakdown of secure communication for governments and critical infrastructure.</li></ul><p>The cost of migrating to PQC, while significant, is dwarfed by the potential economic damage of failing to do so.</p>",
                        "image": "https://images.unsplash.com/photo-1516245834210-c4c1427873ab?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Migration Costs and Timelines",
                        "content": "<p>For a large organization, the transition to PQC is a multi-year, multi-million dollar project. The costs include:</p><ul><li><strong>Inventory and Discovery:</strong> The first and often hardest step is finding all the systems in the organization that use public-key cryptography (a 'crypto-inventory').</li><li><strong>R&D and Testing:</strong> Evaluating and testing PQC libraries and their performance impact.</li><li><strong>Implementation and Deployment:</strong> Upgrading software, hardware, and protocols across the entire organization.</li><li><strong>Training and Education:</strong> Training developers and IT staff on the new cryptographic standards and practices.</li></ul><p>Because of this complexity, organizations cannot wait until the threat is imminent. The migration needs to start now.</p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Risk Management and Strategic Investment",
                        "content": "<p>Viewing the quantum threat through a risk management lens is essential. Organizations must assess their own specific risks based on the lifespan of their data and systems.</p><div class='info-box tip'><div class='info-box-header'><i class='fas fa-lightbulb'></i><strong>Mosca's Theorem</strong></div><p>A simple way to think about this is Michele Mosca's theorem: 'If x + y > z, then you're in trouble'. Where:<ul><li><strong>x</strong> = How long you need your data to be secure.</li><li><strong>y</strong> = How long it will take you to migrate to a quantum-safe solution.</li><li><strong>z</strong> = How long it will take for a large-scale quantum computer to be built.</li></ul>If the time you need to be secure plus your migration time is greater than the time until the threat arrives, you need to start migrating now.</p></div>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "The 'Harvest Now, Decrypt Later' attack means that the urgency of migrating to PQC depends on what?",
                        "options": [
                            "The performance of the new algorithms.",
                            "The desired lifespan of the confidential data.",
                            "The cost of the migration.",
                            "The current price of Bitcoin."
                        ],
                        "correct": 1,
                        "explanation": "If data needs to be kept secret for 30 years, it must be encrypted with an algorithm that can resist an attack that might happen 10 or 20 years from now. This makes the migration urgent for long-term secrets."
                    },
                    {
                        "id": 2,
                        "question": "What is often the most difficult first step for a large organization beginning its PQC migration?",
                        "options": [
                            "Choosing an algorithm.",
                            "Writing the code.",
                            "Creating a complete inventory of all systems and applications that use public-key cryptography.",
                            "Training employees."
                        ],
                        "correct": 2,
                        "explanation": "In a large, complex organization, cryptography is often deeply embedded in thousands of systems, many of which may be old or poorly documented. Simply discovering all the places that need to be upgraded is a massive challenge."
                    },
                    {
                        "id": 3,
                        "question": "According to Mosca's Theorem ('x + y > z'), when should an organization start its PQC migration?",
                        "options": [
                            "After a quantum computer is built (z=0).",
                            "When the time needed to migrate (y) is greater than the expected life of the quantum computer.",
                            "When the confidentiality lifespan of their data (x) plus their migration time (y) is greater than the estimated time until the threat arrives (z).",
                            "They should not start the migration."
                        ],
                        "correct": 2,
                        "explanation": "This simple formula is a powerful tool for risk assessment. It makes it clear that for assets with a long security life (large 'x') or for organizations with a long migration timeline (large 'y'), the time to start preparing is now."
                    }
                ]
            }
        },
        {
            "id": "lesson-39",
            "title": "Legal and Regulatory Considerations",
            "duration": "60 min",
            "objectives": [
                "Understand how PQC migration impacts regulatory compliance",
                "Learn about export control implications for cryptographic software",
                "Analyze how privacy laws (GDPR, etc.) affect PQC implementation",
                "Explore liability and insurance issues related to the quantum threat",
                "Discuss the role of international cooperation and legal frameworks"
            ],
            "content": {
                "overview": "The transition to PQC is not just a technical and economic issue; it also has significant legal and regulatory dimensions. This lesson explores the complex web of compliance, liability, and international law that organizations must navigate as they move to a quantum-safe world.",
                "sections": [
                    {
                        "title": "Regulatory Compliance",
                        "content": "<p>Many industries have strict regulatory requirements for data protection (e.g., HIPAA for healthcare, PCI DSS for payments). These regulations often mandate the use of 'industry-standard' or 'NIST-approved' cryptography.</p><p>Once NIST finalizes the PQC standards, these regulatory bodies will begin to update their own requirements. Organizations in these fields will have a legal and compliance obligation to migrate to the new standards. Failing to do so could result in fines and legal penalties, in addition to the security risk.</p>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Export Controls",
                        "content": "<p>Strong cryptography has historically been classified as a 'munition' and subject to strict export control regulations. While these rules have been relaxed, the development of new, powerful PQC algorithms could lead to new restrictions.</p><p>Companies that develop or export software containing PQC implementations must be aware of the international legal landscape (e.g., Wassenaar Arrangement) and the specific regulations of their home country to ensure they are not in violation of the law.</p>",
                        "image": "https://images.unsplash.com/photo-1544383835-bda2bc66a22d?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Liability and Standard of Care",
                        "content": "<p>As PQC standards become widely available and the quantum threat becomes more widely understood, a legal 'standard of care' will emerge. This means that organizations will be expected to take reasonable steps to protect their systems and customer data from the quantum threat.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Risk of Negligence</strong></div><p>An organization that suffers a data breach in the future due to a quantum attack could be found legally negligent if they failed to migrate to available PQC standards in a timely manner. This creates a strong legal incentive, in addition to the security one, for organizations to develop and execute a PQC transition plan.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "How will the finalization of NIST PQC standards likely affect regulatory compliance?",
                        "options": [
                            "It will have no effect.",
                            "Regulatory bodies (for healthcare, finance, etc.) will begin to update their rules to mandate the use of the new PQC standards.",
                            "It will make all current regulations obsolete.",
                            "It will only affect government agencies."
                        ],
                        "correct": 1,
                        "explanation": "Regulations often refer to 'NIST standards' as the benchmark for required security. As NIST updates its standards to include PQC, this will create a ripple effect, making PQC adoption a compliance requirement for many industries."
                    },
                    {
                        "id": 2,
                        "question": "What is a potential legal risk for an organization that fails to migrate to PQC in a timely manner?",
                        "options": [
                            "There are no legal risks.",
                            "They could be sued for negligence if a future quantum-based data breach occurs.",
                            "They will be forced to shut down by NIST.",
                            "They will lose their internet connection."
                        ],
                        "correct": 1,
                        "explanation": "Once a threat is known and a solution (PQC standards) is available, a 'standard of care' is established. An organization that ignores this could be held liable for damages that could have been prevented by migrating."
                    },
                    {
                        "id": 3,
                        "question": "Why are export controls a concern for developers of PQC software?",
                        "options": [
                            "They make the software more expensive.",
                            "Strong cryptographic software can be subject to international regulations that restrict its distribution to certain countries.",
                            "They slow down the software's performance.",
                            "They are not a concern."
                        ],
                        "correct": 1,
                        "explanation": "Historically, governments have regulated the export of strong cryptography. Developers and companies must be aware of these complex legal frameworks when distributing PQC software globally."
                    }
                ]
            }
        },
        {
            "id": "lesson-40",
            "title": "PQC Education and Training",
            "duration": "60 min",
            "objectives": [
                "Understand the need for PQC-focused educational curriculum",
                "Explore the design of training programs for developers and engineers",
                "Analyze skill assessment and professional certification for PQC",
                "Learn about strategies for raising PQC awareness among decision-makers",
                "Discuss the role of community and open-source learning"
            ],
            "content": {
                "overview": "The global transition to PQC requires a massive upskilling of the technical workforce. Developers, engineers, and IT professionals need to be trained on the new algorithms, libraries, and protocols. This lesson covers the critical role of education and training in ensuring a successful and secure migration.",
                "sections": [
                    {
                        "title": "Curriculum Development",
                        "content": "<p>A skills gap is one of the biggest obstacles to PQC migration. Universities and professional training programs need to develop new curricula that cover:</p><ul><li>The mathematical foundations of PQC (lattices, codes, etc.).</li><li>The details of the new NIST standard algorithms.</li><li>Practical skills in using PQC libraries (like liboqs).</li><li>Secure implementation practices (e.g., constant-time programming).</li><li>Protocol integration for TLS, SSH, etc.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Training for Developers and Decision-Makers",
                        "content": "<p>Training needs to be tailored to different audiences within an organization.</p><ul><li><strong>For Developers:</strong> Hands-on workshops on how to replace classical crypto libraries with their PQC counterparts, how to test for performance, and how to avoid common implementation pitfalls.</li><li><strong>For Architects and IT Professionals:</strong> Training on migration strategies, crypto-inventory tools, and how to integrate PQC into existing infrastructure without causing disruptions.</li><li><strong>For Executives and Managers:</strong> High-level briefings on the quantum threat, the business risks, the expected costs and timelines, and the importance of prioritizing the migration.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Professional Certification and Awareness",
                        "content": "<p>As PQC becomes the new standard, professional certifications will emerge to validate an individual's skills and knowledge. This will be important for hiring and for ensuring that teams have the requisite expertise.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Raising Awareness</strong></div><p>General awareness campaigns are also crucial. Many business leaders are still unaware of the quantum threat or believe it is a distant, academic problem. Clear, concise communication about the 'Harvest Now, Decrypt Later' risk and the long migration timelines is essential for creating the organizational will to invest in a PQC transition plan.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a major obstacle to the global PQC migration?",
                        "options": [
                            "A lack of available algorithms.",
                            "A skills gap and a shortage of developers and engineers trained in PQC.",
                            "The algorithms are too simple.",
                            "NIST has not provided enough information."
                        ],
                        "correct": 1,
                        "explanation": "There is a significant shortage of people with the specialized knowledge required to implement, test, and deploy PQC correctly, which is why education and training are so critical."
                    },
                    {
                        "id": 2,
                        "question": "Why is it important to provide different types of training for developers versus executives?",
                        "options": [
                            "It is not important.",
                            "Developers need deep, hands-on technical training, while executives need high-level briefings focused on business risk and strategy.",
                            "Executives need more technical training than developers.",
                            "Only developers need to be trained."
                        ],
                        "correct": 1,
                        "explanation": "Effective training must be tailored to the audience's role. A developer needs to know how to write the code, while a CIO needs to know how to budget for the project and explain the risk to the board."
                    },
                    {
                        "id": 3,
                        "question": "What is the purpose of a PQC awareness campaign for business leaders?",
                        "options": [
                            "To teach them how to code.",
                            "To convince them that the quantum threat is a real and urgent business risk that requires strategic planning and investment now.",
                            "To sell them new hardware.",
                            "To downplay the risk."
                        ],
                        "correct": 1,
                        "explanation": "Without buy-in from leadership, a large-scale PQC migration project will not receive the necessary resources and priority. Awareness is the first step to action."
                    }
                ]
            }
        },
        {
            "id": "lesson-41",
            "title": "Advanced Lattice Techniques",
            "duration": "90 min",
            "objectives": [
                "Understand the construction of lattice-based trapdoor functions",
                "Learn about Gaussian sampling techniques over lattices",
                "Explore the principles of lattice-based Functional Encryption (FE)",
                "Get an overview of Attribute-Based Encryption (ABE) from lattices",
                "Discuss the path towards Fully Homomorphic Encryption (FHE)"
            ],
            "content": {
                "overview": "Beyond the standard PQC schemes, lattices provide a rich mathematical foundation for building a wide range of advanced cryptographic primitives. This lesson offers a glimpse into the cutting-edge of lattice cryptography research, exploring powerful concepts like trapdoor functions and Fully Homomorphic Encryption.",
                "sections": [
                    {
                        "title": "Lattice Trapdoor Functions",
                        "content": "<p>A <strong>trapdoor function</strong> is a function that is easy to compute in one direction, but hard to invert unless you have a secret piece of information, the 'trapdoor'. This is the basis of most public-key cryptography.</p><p>In lattices, a trapdoor can be constructed by generating a 'bad' public basis for a lattice that has a secret, 'good' basis (the trapdoor). It is easy to generate the bad basis from the good one, but hard to go the other way. This allows for the creation of advanced primitives like identity-based encryption and attribute-based encryption, where an encryption key can be a string like 'doctor@hospital' instead of a random public key.</p>",
                        "image": "https://images.unsplash.com/photo-1621609764095-b32635d9a745?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Gaussian Sampling",
                        "content": "<p><strong>Gaussian sampling</strong> is a crucial algorithmic tool in advanced lattice cryptography. It is a procedure that, given a bad basis for a lattice, can produce samples of lattice points that are drawn from a discrete Gaussian distribution centered at a given point.</p><p>This is a key ingredient in constructing lattice-based trapdoors and is also used in some signature schemes. The ability to perform this sampling without revealing the secret good basis is a non-trivial and powerful technique.</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Fully Homomorphic Encryption (FHE)",
                        "content": "<p>FHE is the 'holy grail' of cryptography, allowing for arbitrary computation on encrypted data. The breakthrough that made FHE possible came from lattice-based cryptography.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>FHE from LWE</strong></div><p>Most FHE schemes are based on the Learning With Errors (LWE) problem. The 'errors' in the LWE problem provide a small amount of room to perform computations. When you add or multiply two LWE-encrypted ciphertexts, the underlying messages add or multiply as desired, but the 'noise' or 'error' in the ciphertexts also grows. The main challenge of FHE is managing this noise growth through a process called 'bootstrapping' to allow for an unlimited number of computations. Because they are based on LWE, these FHE schemes are naturally quantum-resistant.</p></div>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "In lattice-based cryptography, what typically constitutes the 'trapdoor'?",
                        "options": [
                            "A secret error vector.",
                            "A secret, 'good' basis (short, orthogonal vectors) for a lattice whose public basis is 'bad' (long, skewed vectors).",
                            "A secret polynomial.",
                            "A quantum computer."
                        ],
                        "correct": 1,
                        "explanation": "The asymmetry between how easy it is to generate a bad basis from a good one, and how hard it is to do the reverse, is the fundamental concept behind lattice trapdoors."
                    },
                    {
                        "id": 2,
                        "question": "The security of most modern Fully Homomorphic Encryption (FHE) schemes is based on what?",
                        "options": [
                            "The difficulty of factoring.",
                            "The security of hash functions.",
                            "The hardness of lattice problems like LWE.",
                            "The security of elliptic curves."
                        ],
                        "correct": 2,
                        "explanation": "The first plausible FHE construction was based on lattices, and this remains the most fruitful approach. This has the convenient side effect of making these schemes quantum-resistant by design."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary challenge that must be managed in an FHE scheme?",
                        "options": [
                            "Key size.",
                            "The growth of 'noise' or 'error' in the ciphertexts as computations are performed.",
                            "Signing speed.",
                            "The need for a trusted setup."
                        ],
                        "correct": 1,
                        "explanation": "Each homomorphic operation increases the noise in the ciphertext. The key to FHE is a 'bootstrapping' procedure that can 'refresh' the ciphertext and reduce its noise, allowing for an unlimited number of operations."
                    }
                ]
            }
        },
        {
            "id": "lesson-42",
            "title": "Novel PQC Constructions",
            "duration": "60 min",
            "objectives": [
                "Explore the principles of group action-based cryptography",
                "Get an overview of tensor-based and combinatorial cryptography",
                "Learn about graph-based cryptographic schemes",
                "Understand how algebraic geometry can be applied",
                "Appreciate the role of research into novel PQC families"
            ],
            "content": {
                "overview": "While the mainstream of PQC research is focused on the 'big five' families, researchers are continuously exploring new mathematical structures in search of the next generation of post-quantum algorithms. This lesson provides a brief tour of more experimental and novel PQC constructions that could one day become standards.",
                "sections": [
                    {
                        "title": "Group Action-Based Cryptography",
                        "content": "<p>This is the family of cryptography that includes isogeny-based schemes. The general idea is to have a group that acts on a set of objects. The public key is the starting object and the result of the group action, and the private key is the secret group element that defines the action.</p><p>While the supersingular isogeny approach (SIDH/SIKE) was broken, research continues into other types of group actions that might provide post-quantum security. This is a very abstract but powerful area of mathematics.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Graph-Based Cryptography",
                        "content": "<p>The security of these schemes is based on the difficulty of problems in graph theory. For example, the <strong>Graph Isomorphism Problem</strong> is the problem of determining whether two finite graphs are isomorphic (i.e., whether they have the same structure).</p><p>While this problem is not believed to be NP-complete, there is no known efficient classical or quantum algorithm for it. This has led to research into using it as a basis for zero-knowledge proofs and other cryptographic primitives, though practical schemes have yet to emerge.</p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Importance of Research Diversity",
                        "content": "<p>The stories of Rainbow and SIKE, both of which were leading PQC candidates before being broken, provide a powerful lesson: we should not put all of our eggs in one mathematical basket. While lattice-based cryptography currently appears to be the strongest and most versatile foundation for PQC, it is crucial that the research community continues to explore these novel and diverse approaches.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>A Hedge Against the Unknown</strong></div><p>A future mathematical breakthrough could reveal a weakness in our understanding of lattices. By continuing to study and develop these other families, we ensure that we will have alternative, well-vetted cryptographic foundations to fall back on, providing long-term resilience for our digital security.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Isogeny-based cryptography, like the broken SIDH, is part of which broader family of cryptographic constructions?",
                        "options": [
                            "Lattice-based cryptography",
                            "Group action-based cryptography",
                            "Code-based cryptography",
                            "Hash-based cryptography"
                        ],
                        "correct": 1,
                        "explanation": "Isogeny schemes are a specific instance of a more general paradigm where a group (the isogenies) acts on a set (the elliptic curves). Research is ongoing into other group actions that might be secure."
                    },
                    {
                        "id": 2,
                        "question": "The security of graph-based cryptography is sometimes based on which hard problem?",
                        "options": [
                            "The Graph Coloring Problem",
                            "The Shortest Path Problem",
                            "The Graph Isomorphism Problem",
                            "The Maximum Flow Problem"
                        ],
                        "correct": 2,
                        "explanation": "The problem of determining if two graphs have the same structure is a famous computational problem for which no efficient quantum or classical algorithm is known, making it a candidate for cryptographic constructions."
                    },
                    {
                        "id": 3,
                        "question": "Why is it important for the research community to continue studying novel PQC families, even after NIST has selected standards?",
                        "options": [
                            "It is not important.",
                            "To provide a diverse set of backup solutions in case a fundamental weakness is ever discovered in the primary standards (like lattices).",
                            "To create more competition for NIST.",
                            "To find an algorithm that is faster than AES."
                        ],
                        "correct": 1,
                        "explanation": "The breaks of leading candidates like SIKE and Rainbow demonstrate that our understanding of these mathematical problems is still evolving. Maintaining a diverse research portfolio is a crucial long-term strategy for ensuring cryptographic resilience."
                    }
                ]
            }
        },
        {
            "id": "lesson-43",
            "title": "Quantum-Classical Hybrid Attacks",
            "duration": "60 min",
            "objectives": [
                "Understand combined attack methodologies",
                "Learn how classical preprocessing can speed up quantum attacks",
                "Explore the concept of quantum-assisted classical algorithms",
                "Analyze the complexity of hybrid attacks",
                "Discuss defensive strategies against these combined threats"
            ],
            "content": {
                "overview": "The future of cryptanalysis may not be purely quantum, but a hybrid of the best classical and quantum techniques. This lesson explores the frontier of attack research, where classical supercomputers could be used for massive preprocessing steps to aid a smaller quantum computer in breaking a cryptosystem.",
                "sections": [
                    {
                        "title": "Combined Attack Methodologies",
                        "content": "<p>A hybrid attack is one that uses both classical and quantum computational resources to solve a problem faster than either could alone. The general idea is to use the strengths of each type of computer where it is most effective.</p><h3>Example: Preprocessing for Lattice Attacks</h3><p>Some advanced lattice reduction algorithms (like sieving) require a massive amount of memory. An attacker might use a classical supercomputer to perform a huge preprocessing step, generating a vast database of short vectors in related lattices. The quantum computer could then be used to perform a clever search on this pre-computed data, significantly reducing the amount of quantum computation time (the 'quantum depth') required for the attack.</p>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum-Assisted Algorithms",
                        "content": "<p>In some cases, a mostly classical algorithm can be sped up by using a quantum computer as a subroutine to solve a specific, hard sub-problem. This is different from a pure quantum algorithm like Shor's.</p><p>For example, Grover's algorithm could be used to speed up a search step that is a bottleneck within a larger classical cryptanalysis algorithm. Understanding the potential for these smaller, targeted quantum speedups is a key part of estimating the true post-quantum security of an algorithm.</p>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a3d3?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Impact on Security Estimates",
                        "content": "<p>The possibility of hybrid attacks is a key reason why cryptographers are conservative when choosing security parameters. The security levels defined by NIST are designed to be resistant to the best-known *purely classical* and *purely quantum* attacks.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Defense in Depth</strong></div><p>The analysis of these hybrid attacks is an ongoing area of research. By choosing parameters with a large safety margin, cryptographers aim to ensure that even if a clever hybrid attack is discovered that provides a small speedup, the system will still remain well within the bounds of practical security. This defense-in-depth approach is essential for long-term confidence.</p></div>",
                        "image": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a quantum-classical hybrid attack?",
                        "options": [
                            "An attack that uses two different quantum computers.",
                            "An attack that combines the use of classical and quantum computers to solve a problem faster than either could alone.",
                            "An attack on a hybrid cryptosystem.",
                            "A purely theoretical attack."
                        ],
                        "correct": 1,
                        "explanation": "A hybrid attack leverages the different strengths of both types of computers, for example by using a classical supercomputer for a memory-intensive preprocessing step to aid a quantum computer."
                    },
                    {
                        "id": 2,
                        "question": "What is meant by a 'quantum-assisted' classical algorithm?",
                        "options": [
                            "A classical algorithm that is very fast.",
                            "A mostly classical algorithm that uses a quantum computer as a subroutine to speed up a specific, difficult part of the computation.",
                            "An algorithm that can run on both classical and quantum computers.",
                            "An algorithm designed to attack a quantum computer."
                        ],
                        "correct": 1,
                        "explanation": "This approach recognizes that quantum computers are not a panacea. It seeks to apply their specific speedups (like Grover's search) to the bottlenecks within our best classical attack algorithms."
                    },
                    {
                        "id": 3,
                        "question": "How do security researchers account for the potential of future hybrid attacks when choosing PQC parameters?",
                        "options": [
                            "They ignore the possibility.",
                            "They choose parameters with a large security margin to ensure the system remains secure even if new, faster attacks are discovered.",
                            "They assume hybrid attacks are impossible.",
                            "They only focus on purely quantum attacks."
                        ],
                        "correct": 1,
                        "explanation": "The entire field of security parameter selection is about being conservative. By setting the bar high, cryptographers aim to future-proof the standards against foreseeable advances in both classical and quantum cryptanalysis, including hybrid approaches."
                    }
                ]
            }
        },
        {
            "id": "lesson-44",
            "title": "PQC Hardware Implementations",
            "duration": "75 min",
            "objectives": [
                "Understand the design of PQC implementations on FPGAs",
                "Explore ASIC design considerations for PQC algorithms",
                "Analyze the role of post-quantum Hardware Security Modules (HSMs)",
                "Learn about cryptographic coprocessors for PQC",
                "Discuss the security of embedded processors in the PQC era"
            ],
            "content": {
                "overview": "For post-quantum cryptography to achieve high performance in demanding environments, specialized hardware is essential. This lesson explores the implementation of PQC algorithms directly into silicon, from reconfigurable FPGAs to custom-designed ASICs and the evolution of Hardware Security Modules.",
                "sections": [
                    {
                        "title": "FPGA PQC Implementations",
                        "content": "<p>FPGAs (Field-Programmable Gate Arrays) are integrated circuits that can be configured by a customer or a designer after manufacturing—hence 'field-programmable'. They are ideal for creating high-performance hardware accelerators for PQC.</p><p>A developer can design a hardware architecture that is perfectly tailored to the mathematical operations of a specific algorithm (like the polynomial multiplication in Kyber). This hardware implementation will be significantly faster and more power-efficient than running the same algorithm in software on a general-purpose CPU. FPGAs are often used in network devices and servers that need to handle a high volume of cryptographic operations.</p>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "ASIC Design Considerations",
                        "content": "<p>An ASIC (Application-Specific Integrated Circuit) is a chip designed for a single, specific purpose. An ASIC for Dilithium would contain circuits that do nothing but perform Dilithium signing and verification. ASICs offer the highest possible performance and the lowest power consumption, but they are very expensive to design and manufacture.</p><p>In the long term, we can expect to see PQC acceleration ASICs integrated directly into the main CPUs of servers and personal devices, just as AES acceleration (AES-NI) is a standard feature in modern processors today.</p>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee9 eighty-one c?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Post-Quantum Hardware Security Modules (HSMs)",
                        "content": "<p>HSMs are hardened, tamper-resistant devices that securely manage the entire lifecycle of cryptographic keys. They are the root of trust for Certificate Authorities, banks, and other high-security environments.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Upgrading the Root of Trust</strong></div><p>The entire PQC migration depends on the availability of HSMs that can securely generate, store, and use PQC keys. HSM vendors are currently in the process of updating their hardware and firmware to support the new NIST standards. This is a critical piece of infrastructure, as many organizations will be unable to complete their PQC transition until their HSMs are PQC-capable.</p></div>",
                        "image": "https://images.unsplash.com/photo-1630569772629-19593122c6a6?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary advantage of implementing a PQC algorithm on an FPGA?",
                        "options": [
                            "It makes the algorithm easier to understand.",
                            "It provides significantly higher performance and better energy efficiency than a pure software implementation.",
                            "It is cheaper than a software implementation.",
                            "It guarantees the algorithm is free from side-channel attacks."
                        ],
                        "correct": 1,
                        "explanation": "FPGAs allow for the creation of custom hardware logic that is perfectly tailored to the algorithm, which is much more efficient than running it on a general-purpose CPU."
                    },
                    {
                        "id": 2,
                        "question": "What is the key difference between an FPGA and an ASIC?",
                        "options": [
                            "There is no difference.",
                            "An FPGA is re-programmable, while an ASIC is a custom chip designed for a single, fixed purpose.",
                            "An FPGA is faster than an ASIC.",
                            "An ASIC is a type of general-purpose CPU."
                        ],
                        "correct": 1,
                        "explanation": "An ASIC offers the ultimate in performance and efficiency but is very expensive and time-consuming to create. FPGAs offer a flexible, reconfigurable alternative for hardware acceleration."
                    },
                    {
                        "id": 3,
                        "question": "Why is the upgrade of Hardware Security Modules (HSMs) to support PQC so critical?",
                        "options": [
                            "It is not critical.",
                            "HSMs are the root of trust for many high-security systems (like CAs), and these systems cannot migrate until their core key management hardware supports the new standards.",
                            "HSMs are only used for cryptocurrency.",
                            "HSMs are needed to design new PQC algorithms."
                        ],
                        "correct": 1,
                        "explanation": "The security of an entire PKI often depends on the security of its root keys, which are protected by HSMs. The PQC migration of the internet is therefore dependent on the availability of PQC-capable HSMs."
                    }
                ]
            }
        },
        {
            "id": "lesson-45",
            "title": "Threshold Cryptography in PQC",
            "duration": "75 min",
            "objectives": [
                "Understand the principles of post-quantum threshold schemes",
                "Explore post-quantum Distributed Key Generation (DKG)",
                "Analyze the design of post-quantum threshold signatures",
                "Learn about quantum-safe secret sharing protocols",
                "Discuss the challenges of building multi-party PQC protocols"
            ],
            "content": {
                "overview": "Threshold cryptography allows a group of parties to share control over a cryptographic secret, preventing any single party from acting alone. Migrating these powerful techniques to be post-quantum is a major challenge and a critical area of research. This lesson explores the design of PQC threshold signatures and distributed key generation.",
                "sections": [
                    {
                        "title": "Post-Quantum Threshold Schemes",
                        "content": "<p>A <strong>(t, n) threshold scheme</strong> is one where a cryptographic operation (like signing) is distributed among 'n' parties, and any 't' of them must cooperate to perform the operation. This is crucial for high-security applications like managing a cryptocurrency exchange's cold wallet.</p><p>Many classical threshold schemes are based on discrete logarithm assumptions and are not quantum-safe. The challenge is to redesign these schemes using post-quantum assumptions, like lattices. This is difficult because the linear structure of lattices can be tricky to work with in a multi-party setting without leaking secret information.</p>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Post-Quantum Threshold Signatures",
                        "content": "<p>A threshold signature scheme allows 'n' parties to share a secret signing key. Any 't' of them can interact to produce a single, valid signature on a message, but 't-1' of them can produce no information about the signature.</p><p>Designing efficient threshold versions of PQC signature schemes like Dilithium is a highly active area of research. The goal is to create a multi-party protocol that is secure against both quantum adversaries and malicious insiders (up to the threshold 't'). These schemes are essential for the future of decentralized finance and secure custody.",
                        "image": "https://images.unsplash.com/photo-1555949963-ff9808202532?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Challenges and Research",
                        "content": "<p>Building PQC multi-party protocols is harder than for classical ones.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>The Problem with Errors</strong></div><p>Many lattice-based schemes rely on small, secret error vectors or polynomials. In a multi-party setting, these secret errors must be generated in a distributed way. Designing a secure protocol where multiple parties can jointly generate a secret that has a specific, small statistical distribution (like a Gaussian distribution) without revealing the secret itself is a major cryptographic challenge.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the purpose of a (t, n) threshold signature scheme?",
                        "options": [
                            "To create 'n' signatures at once.",
                            "To allow any 't' out of 'n' parties to jointly create a single valid signature, preventing any single party from having full control.",
                            "To make the signature smaller.",
                            "To make the signature process faster."
                        ],
                        "correct": 1,
                        "explanation": "Threshold schemes are a powerful tool for decentralizing control and eliminating single points of failure, which is why they are so important for securing high-value assets."
                    },
                    {
                        "id": 2,
                        "question": "Why is it difficult to create threshold versions of many lattice-based PQC schemes?",
                        "options": [
                            "The lattices are too big.",
                            "The schemes rely on secret values with specific statistical properties (like small errors), and it is hard to generate these securely in a distributed manner.",
                            "Lattices are not compatible with multi-party computation.",
                            "It is not difficult."
                        ],
                        "correct": 1,
                        "explanation": "The need to jointly and secretly sample from specific error distributions is a fundamental challenge that requires new cryptographic techniques to solve efficiently and securely."
                    },
                    {
                        "id": 3,
                        "question": "Compared to classical MPC/threshold protocols, PQC versions often have what drawback?",
                        "options": [
                            "They are less secure.",
                            "They require more interaction between the parties and have higher communication overhead.",
                            "They are simpler.",
                            "They can only support two parties."
                        ],
                        "correct": 1,
                        "explanation": "The complexity of the underlying PQC mathematics often translates into multi-party protocols that require more rounds of communication and larger messages to be exchanged between the parties, making them less performant."
                    }
                ]
            }
        },
        {
            "id": "lesson-46",
            "title": "PQC Interoperability Standards",
            "duration": "60 min",
            "objectives": [
                "Understand the importance of cross-platform compatibility",
                "Learn about algorithm negotiation protocols",
                "Analyze the need for standardized key and signature formats",
                "Discuss the challenges of migration coordination",
                "Explore the role of backward compatibility in a multi-algorithm world"
            ],
            "content": {
                "overview": "For the PQC transition to succeed, it's not enough for everyone to use the same algorithms; they must use them in the same way. Interoperability standards are the detailed rules that ensure a Kyber implementation from one company can successfully communicate with an implementation from another. This lesson covers the crucial, detailed work of ensuring a compatible and interoperable quantum-safe ecosystem.",
                "sections": [
                    {
                        "title": "Cross-Platform Compatibility",
                        "content": "<p>Imagine two companies, A and B. Both use the NIST-standard Kyber algorithm. But if Company A's software formats the public key as a raw byte string, and Company B's software expects it to be in a structured JSON object, they won't be able to communicate. </p><p>Interoperability standards define these details:<ul><li>The exact byte-level representation of public keys, secret keys, and ciphertexts.</li><li>The object identifiers (OIDs) used to identify the algorithms in certificates.</li><li>The specific byte ordering (endianness) to be used.</li></ul>These low-level details are mundane but absolutely essential for a global network.</p>",
                        "image": "https://images.unsplash.com/photo-1554224155-169544351748?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Algorithm Negotiation",
                        "content": "<p>In a crypto-agile world, a client and server may support multiple different cryptographic algorithms. When they first connect, they must securely negotiate which algorithms to use. </p><p>For example, in a TLS handshake, the client sends a list of all the key exchange and signature algorithms it supports, in order of preference. The server picks the most preferred algorithm that it also supports and tells the client which one it has chosen. This negotiation process must itself be secure against downgrade attacks, where an attacker tries to trick the parties into choosing a weaker, older algorithm.</p>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Role of Test Vectors",
                        "content": "<p>To ensure interoperability, standards bodies like NIST publish official <strong>test vectors</strong>. These are a set of pre-computed inputs and their expected correct outputs for a given algorithm.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Known-Answer Tests (KATs)</strong></div><p>When a developer writes a new implementation of Kyber, they can run these Known-Answer Tests. If their code produces the exact same outputs as the official test vectors for all the given inputs, they can have high confidence that their implementation is correct and will be interoperable with other correct implementations. This is a fundamental part of the cryptographic development and validation process.</p></div>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary purpose of interoperability standards in cryptography?",
                        "options": [
                            "To make the algorithms more secure.",
                            "To ensure that different implementations of the same algorithm from different vendors can successfully communicate with each other.",
                            "To increase the speed of the algorithms.",
                            "To limit the number of available algorithms."
                        ],
                        "correct": 1,
                        "explanation": "Interoperability is about everyone agreeing on the low-level details of data formatting and protocol messages so that the global network can function as a cohesive whole."
                    },
                    {
                        "id": 2,
                        "question": "What is a 'downgrade attack'?",
                        "options": [
                            "An attack that makes a computer slower.",
                            "An attack where a man-in-the-middle tricks two parties into negotiating the use of a weaker, insecure algorithm that the attacker knows how to break.",
                            "An attack that reduces the size of a public key.",
                            "An attack that requires an older computer."
                        ],
                        "correct": 1,
                        "explanation": "This is a major threat during a migration period. Secure negotiation protocols are designed to prevent an attacker from forcing parties to 'downgrade' to a legacy algorithm when both are capable of using a stronger, modern one."
                    },
                    {
                        "id": 3,
                        "question": "What are 'test vectors' provided by NIST used for?",
                        "options": [
                            "To attack cryptographic systems.",
                            "To provide a standardized set of inputs and expected outputs that developers can use to verify that their implementation is correct and interoperable.",
                            "To generate random numbers.",
                            "To choose which algorithm to use."
                        ],
                        "correct": 1,
                        "explanation": "Test vectors, also called Known-Answer Tests (KATs), are an essential tool for developers to validate their code against the official standard, ensuring correctness and preventing interoperability issues."
                    }
                ]
            }
        },
        {
            "id": "lesson-47",
            "title": "Quantum Cryptography vs PQC",
            "duration": "60 min",
            "objectives": [
                "Clearly differentiate between Post-Quantum Cryptography (PQC) and Quantum Cryptography (QKD)",
                "Understand the principle of information-theoretic security",
                "Analyze the practical deployment challenges of Quantum Key Distribution (QKD)",
                "Discuss potential complementary applications of the two technologies",
                "Formulate a clear technology integration strategy"
            ],
            "content": {
                "overview": "The terms 'Post-Quantum Cryptography' and 'Quantum Cryptography' are often confused, but they refer to completely different technologies. This lesson clarifies the distinction, explaining what Quantum Key Distribution (QKD) is, its strengths and weaknesses, and how it relates to the PQC algorithms that are being standardized.",
                "sections": [
                    {
                        "title": "Defining the Terms",
                        "content": "<ul><li><strong>Post-Quantum Cryptography (PQC):</strong> This refers to **classical algorithms** that run on **classical computers** but are designed to be secure against **quantum computers**. The NIST standards (Kyber, Dilithium) are all PQC algorithms. PQC is a software-based solution.</li><li><strong>Quantum Cryptography:</strong> This typically refers to <strong>Quantum Key Distribution (QKD)</strong>. QKD uses the physics of quantum mechanics (e.g., sending polarized photons over a fiber optic cable) to allow two parties to agree on a secret key. QKD is a hardware-based solution.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a3d3?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum Key Distribution (QKD)",
                        "content": "<p>QKD's security is based on a principle of physics, not a mathematical problem. The act of an eavesdropper ('Eve') measuring the quantum state of the photons being sent between Alice and Bob will inevitably disturb that state. Alice and Bob can detect this disturbance, realize an eavesdropper is present, and discard the key.</p><h3>Information-Theoretic Security:</h3><p>This means QKD is secure even against an adversary with unlimited computational power. Its security is based on the laws of physics, not on the difficulty of a mathematical problem. This is a very powerful security guarantee.</p>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "PQC vs. QKD: Practical Considerations",
                        "content": "<p>While QKD offers a stronger theoretical security guarantee, it has major practical limitations that PQC does not.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Deployment Challenges of QKD</strong></div><p><ul><li><strong>Requires Special Hardware:</strong> QKD needs dedicated hardware, like single-photon detectors and a direct fiber optic link or line-of-sight laser. It cannot simply run over the public internet.</li><li><strong>Distance Limitations:</strong> Signal loss limits the distance of a direct QKD link (typically ~100 km). Longer distances require 'trusted repeaters', which re-introduces a security weakness.</li><li><strong>No Authentication:</strong> QKD only provides a secret key. It does not provide authentication. You still need a classical, authenticated channel (e.g., one using a PQC signature) to ensure you are talking to the right person.</li></ul>For these reasons, PQC is the practical and scalable solution for securing the vast majority of our existing digital infrastructure. QKD is a niche technology for specialized, high-security, point-to-point links.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Which statement correctly describes Post-Quantum Cryptography (PQC)?",
                        "options": [
                            "It requires a quantum computer to run.",
                            "It is a hardware-based solution using photons.",
                            "It is a set of classical algorithms that run on classical computers and are resistant to quantum attacks.",
                            "It is another name for Quantum Key Distribution."
                        ],
                        "correct": 2,
                        "explanation": "This is the key distinction. PQC is a software upgrade for our existing computers and networks to make them secure against future quantum threats."
                    },
                    {
                        "id": 2,
                        "question": "The security of Quantum Key Distribution (QKD) is based on what?",
                        "options": [
                            "The difficulty of a mathematical problem.",
                            "The principles of quantum physics (e.g., the observer effect).",
                            "The security of hash functions.",
                            "The use of very long keys."
                        ],
                        "correct": 1,
                        "explanation": "QKD's security is information-theoretic, meaning it's secure even against a computationally unbounded attacker because intercepting the key exchange would require breaking the laws of physics."
                    },
                    {
                        "id": 3,
                        "question": "What is a major practical limitation of Quantum Key Distribution (QKD)?",
                        "options": [
                            "It is not very secure.",
                            "It requires specialized hardware and has distance limitations, making it unsuitable for the public internet.",
                            "It is vulnerable to Shor's algorithm.",
                            "It is slower than PQC."
                        ],
                        "correct": 1,
                        "explanation": "The need for a dedicated physical channel (like a fiber optic cable) prevents QKD from being a general-purpose replacement for public-key cryptography on the internet. PQC is the scalable solution for that."
                    }
                ]
            }
        },
        {
            "id": "lesson-48",
            "title": "PQC Research Methodologies",
            "duration": "60 min",
            "objectives": [
                "Understand the formal methods of cryptographic research",
                "Learn about security analysis and proof frameworks",
                "Explore the academic peer review process",
                "Discuss the importance of open science and reproducible research",
                "Recognize the standards for ethical disclosure and collaboration"
            ],
            "content": {
                "overview": "The cryptographic algorithms we rely on are the product of a rigorous, global, and often adversarial academic research process. This lesson pulls back the curtain on the world of cryptographic research, explaining how new ideas are developed, how they are attacked and defended in the peer review process, and the principles of open, reproducible science that underpin it all.",
                "sections": [
                    {
                        "title": "The Cryptographic Research Cycle",
                        "content": "<p>The development of a new cryptosystem is a long and arduous process.</p><ol><li><strong>Design:</strong> A researcher proposes a new scheme based on a hard mathematical problem.</li><li><strong>Security Proof:</strong> They write a formal mathematical proof that 'reduces' the security of their scheme to the hardness of the underlying problem. This proof shows that if an attacker could break the scheme, they could also solve the hard problem.</li><li><strong>Peer Review:</strong> The researcher submits their paper to a top-tier academic conference (like CRYPTO or Eurocrypt). The paper is then reviewed by several other anonymous experts in the field who actively try to find flaws in the design or the proof.</li><li><strong>Publication and Scrutiny:</strong> If the paper is accepted, it is published and subjected to years of public scrutiny by the entire research community, who will continue to search for attacks. Only after a scheme has survived this process for many years is it considered for standardization.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Reproducible Research",
                        "content": "<p>In modern cryptanalysis, just publishing a paper describing an attack is not enough. The principle of <strong>reproducible research</strong> requires that the authors also publish their source code and the data needed for other researchers to independently verify their results.</p><p>This is crucial for validating claims. If a researcher claims to have a new, faster attack, publishing their code allows the community to test it, validate its correctness, and understand its precise impact on the security of a cryptosystem. The NIST PQC process heavily relied on this open model, with many teams publishing their implementations and attack code for public review.</p>",
                        "image": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Open Science and Collaboration",
                        "content": "<p>The entire PQC standardization process is one of the greatest examples of open science in action. It was a global, collaborative effort involving hundreds of researchers from academia, industry, and government. </p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Adversarial Collaboration</strong></div><p>The process is inherently adversarial—everyone is trying to break everyone else's algorithms. But this adversarial nature serves a common, collaborative goal: to find any and all weaknesses *before* a standard is chosen, so that the entire world can have confidence in the algorithms that ultimately survive. The open and public nature of this process is the primary source of our trust in the final PQC standards.</p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the purpose of a 'security reduction' in a cryptographic proof?",
                        "options": [
                            "To reduce the security of the scheme.",
                            "To formally prove that the cryptosystem is secure by showing that breaking it would be at least as hard as solving a well-known hard mathematical problem.",
                            "To reduce the size of the keys.",
                            "To reduce the number of rounds in a cipher."
                        ],
                        "correct": 1,
                        "explanation": "A security reduction is the cornerstone of modern provable security. It provides a formal link between a new cryptosystem and a problem we already believe is hard, giving us confidence in the new design."
                    },
                    {
                        "id": 2,
                        "question": "What is the role of academic peer review in cryptography?",
                        "options": [
                            "It is a formality.",
                            "It is a rigorous, adversarial process where experts try to find flaws in a new design before it is published.",
                            "It is a process to check for spelling and grammar.",
                            "It is done by the authors themselves."
                        ],
                        "correct": 1,
                        "explanation": "Peer review is the first line of defense against insecure cryptographic designs. The fact that a scheme has been accepted at a top conference indicates that it has survived intense scrutiny from leading experts."
                    },
                    {
                        "id": 3,
                        "question": "Why is 'reproducible research' particularly important in cryptanalysis?",
                        "options": [
                            "It makes the research papers longer.",
                            "It allows other researchers to independently verify the results of a claimed attack, confirming its validity and practical impact.",
                            "It is a legal requirement.",
                            "It keeps the attack code secret."
                        ],
                        "correct": 1,
                        "explanation": "By publishing the code for an attack, researchers allow for transparent and verifiable science. It's the difference between saying 'I can break this' and proving it in a way anyone can check."
                    }
                ]
            }
        },
        {
            "id": "lesson-49",
            "title": "Future Quantum Technologies Impact",
            "duration": "60 min",
            "objectives": [
                "Understand the concept of a fault-tolerant quantum computer",
                "Analyze the impact of quantum error correction on resource estimates",
                "Explore the security implications of quantum networking and the quantum internet",
                "Discuss the potential for next-generation quantum algorithms beyond Shor's",
                "Develop a mindset for continuous evolution in cryptographic security"
            ],
            "content": {
                "overview": "The quantum computers of today are noisy and error-prone. The true cryptographic threat will arrive with the advent of fault-tolerant quantum computing. This lesson looks even further into the future, exploring the impact of mature quantum technologies like error correction and the quantum internet on the security landscape.",
                "sections": [
                    {
                        "title": "Fault-Tolerant Quantum Computing",
                        "content": "<p>The qubits in today's quantum computers are very fragile and quickly lose their quantum state due to interaction with the environment (a process called 'decoherence'). A <strong>fault-tolerant quantum computer</strong> is one that uses <strong>quantum error correction (QEC)</strong> to overcome this problem.</p><p>QEC works by encoding the information of a single 'logical qubit' across many redundant 'physical qubits'. This allows the computer to detect and correct errors without destroying the underlying quantum computation. Building a fault-tolerant quantum computer is the single greatest engineering challenge in the field, and it is the prerequisite for building a machine that can break RSA-2048.</p>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a3d3?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Quantum Networking and the Quantum Internet",
                        "content": "<p>A <strong>quantum internet</strong> would be a network of quantum devices connected by links that can transmit entangled qubits over long distances. This would enable new applications that are impossible with the classical internet, such as secure communication based on the principles of quantum mechanics.</p><p>While this could enhance security for some applications (e.g., QKD), it also introduces new potential threats. A global quantum internet could allow multiple quantum computers to be linked together, potentially enabling larger-scale attacks. The security of the quantum internet itself is a major area of ongoing research.</p>",
                        "image": "https://images.unsplash.com/photo-1544383835-bda2bc66a22d?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Next-Generation Quantum Algorithms",
                        "content": "<p>Shor's and Grover's algorithms are from the 1990s. It is entirely possible that new, more powerful quantum algorithms will be discovered in the future. We may not yet know the full extent of what a mature quantum computer will be capable of.</p><div class='info-box tip'><div class='info-box-header'><i class='fas fa-lightbulb'></i><strong>The Need for Crypto-Agility</strong></div><p>This uncertainty is the ultimate argument for <strong>crypto-agility</strong>. We are standardizing PQC based on our current understanding of quantum algorithms. If a new quantum algorithm is discovered that breaks lattices, we must have the ability to migrate our systems to a different cryptographic family (like hash-based or code-based). Security is not a destination; it is a continuous process of research, adaptation, and evolution.</p></div>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary purpose of Quantum Error Correction (QEC)?",
                        "options": [
                            "To make quantum algorithms run faster.",
                            "To protect fragile qubits from errors caused by environmental noise and decoherence.",
                            "To correct errors in classical software.",
                            "To break PQC algorithms."
                        ],
                        "correct": 1,
                        "explanation": "QEC is the key technology needed to bridge the gap from today's noisy, intermediate-scale quantum (NISQ) devices to the large-scale, fault-tolerant quantum computers required for cryptanalysis."
                    },
                    {
                        "id": 2,
                        "question": "What is the defining feature of the 'quantum internet'?",
                        "options": [
                            "It is just a faster version of the classical internet.",
                            "It is a network that can transmit entangled qubits between distant quantum devices.",
                            "It is completely secure by default.",
                            "It is run by a single company."
                        ],
                        "correct": 1,
                        "explanation": "The ability to distribute entanglement globally is the key enabling feature of a quantum internet, which would allow for applications like secure QKD and distributed quantum computing."
                    },
                    {
                        "id": 3,
                        "question": "What is the ultimate strategic defense against the possibility of new, currently unknown quantum algorithms being discovered in the future?",
                        "options": [
                            "To assume it will never happen.",
                            "To build all systems to be crypto-agile, so they can be migrated to new cryptographic standards if necessary.",
                            "To turn off all computers.",
                            "To use only symmetric cryptography."
                        ],
                        "correct": 1,
                        "explanation": "Crypto-agility is the acknowledgment that our understanding of threats will evolve. It is the core principle of long-term security strategy, allowing us to adapt to unforeseen breakthroughs."
                    }
                ]
            }
        },
        {
            "id": "lesson-50",
            "title": "Strategic PQC Implementation",
            "duration": "75 min",
            "objectives": [
                "Develop a framework for an organizational transition strategy",
                "Understand the components of a quantum risk management framework",
                "Learn how to optimize the PQC migration timeline",
                "Discuss resource allocation and budgeting for a PQC transition",
                "Define success metrics for measuring migration progress"
            ],
            "content": {
                "overview": "The transition to post-quantum cryptography is one of the most significant security upgrades in the history of computing. For any large organization, it is a complex, multi-year strategic initiative. This final lesson provides a high-level, strategic framework for planning and executing a successful PQC migration.",
                "sections": [
                    {
                        "title": "The PQC Migration Framework",
                        "content": "<p>A successful migration can be broken down into key strategic phases:</p><ol><li><strong>Discovery:</strong> The first phase is to create a comprehensive 'crypto-inventory'. You can't protect what you don't know you have. This involves using automated tools and manual analysis to find every single application, system, and device in the organization that uses public-key cryptography.</li><li><strong>Risk Assessment & Prioritization:</strong> Analyze the inventory based on risk. Which systems protect data that needs to be secure for decades? Which are the most critical to the business? This allows you to prioritize the migration, focusing on the most urgent and important systems first.</li><li><strong>Standardization & Planning:</strong> Define the organization's PQC standards. Which of the NIST algorithms will you use? What will your hybrid strategy be? Develop a detailed migration roadmap with timelines and budgets.</li><li><strong>Execution & Migration:</strong> The long phase of actually upgrading, testing, and deploying the new quantum-safe systems, application by application.</li><li><strong>Validation & Monitoring:</strong> Continuously validating that the migration was successful and monitoring the new systems for any issues or new threats.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Resource Allocation and Governance",
                        "content": "<p>A PQC transition is not just an IT project; it's a major business initiative that requires executive sponsorship and proper governance.</p><ul><li><strong>Executive Buy-in:</strong> The CIO and CISO must secure a dedicated budget and communicate the importance of the project to the board and senior leadership.</li><li><strong>Cross-Functional Team:</strong> The project requires a dedicated team with members from IT, security, legal, compliance, and each major business unit.</li><li><strong>Center of Excellence:</strong> Many large organizations are creating a 'PQC Center of Excellence' to house the deep technical expertise, set standards for the rest of the organization, and track progress.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Measuring Success",
                        "content": "<p>Success in a PQC migration is not a single event, but a continuous process. Key metrics for tracking progress include:</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Key Performance Indicators (KPIs)</strong></div><p><ul><li><strong>Inventory Coverage:</strong> What percentage of the organization's IT assets have been scanned for cryptographic dependencies?</li><li><strong>Migration Progress:</strong> What percentage of critical applications have been migrated to a PQC-compliant state?</li><li><strong>Policy Compliance:</strong> Are new applications being built according to the new PQC standards?</li><li><strong>Team Readiness:</strong> Has the development team completed the required PQC training?</li></ul>These metrics help to turn a massive, abstract goal into a manageable, measurable project.</p></div>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is universally considered the most challenging first step in an enterprise PQC migration?",
                        "options": [
                            "Choosing an algorithm.",
                            "Getting a budget.",
                            "Creating a complete and accurate inventory of all cryptographic assets and dependencies.",
                            "Writing the code."
                        ],
                        "correct": 2,
                        "explanation": "Large organizations have vast, complex IT landscapes. Simply identifying every system that uses public-key cryptography is a massive undertaking that must be completed before any migration can begin."
                    },
                    {
                        "id": 2,
                        "question": "Which of the following is a key part of a strategic PQC plan?",
                        "options": [
                            "Waiting for a quantum computer to be announced before starting.",
                            "Migrating all systems at once on a single 'flag day'.",
                            "Performing a risk assessment to prioritize the migration of the most critical and vulnerable systems first.",
                            "Only upgrading new systems and leaving legacy systems as they are."
                        ],
                        "correct": 2,
                        "explanation": "A risk-based approach is essential. Not all systems are created equal. Prioritizing the systems that protect long-term secrets or are critical to business operations is a core part of an efficient and effective migration strategy."
                    },
                    {
                        "id": 3,
                        "question": "A PQC migration should be treated as what kind of project?",
                        "options": [
                            "A small IT task for a single developer.",
                            "A purely academic research project.",
                            "A major, cross-functional business initiative that requires executive sponsorship and a dedicated budget.",
                            "A marketing project."
                        ],
                        "correct": 2,
                        "explanation": "Because of its scope, cost, and importance, a PQC transition must be treated as a top-level strategic priority for the entire organization, not just a problem for the IT department."
                    }
                ]
            }
        }
    ]
}
      // =====================================================
      // GLOBAL VARIABLES
      // =====================================================
      let currentUser = null;
      let currentLessonIndex = 0;
      let courseProgress = {};
      let userStats = {};
      let quizState = {
        currentQuestion: 0,
        answers: [],
        score: 0,
        isComplete: false,
      };

      // Enhanced session tracking
      let courseSession = {
        startTime: null,
        totalStudyTime: 0,
        lessonsStarted: 0,
        lessonsCompleted: 0,
        pageLoadTime: new Date(),
        interactionCount: 0,
        lastActivityTime: new Date(),
      };

      // Music Player Variables
      let audioPlayer = new Audio();

      // Achievement notification system
      let notificationQueue = [];
      let isShowingNotification = false;

      // =====================================================
      // INITIALIZATION
      // =====================================================
      document.addEventListener("DOMContentLoaded", async () => {
        try {
          showLoadingScreen();
          await checkAuth();

          if (currentUser) {
            // Initialize session tracking
            startCourseSession();

            // Load course data and progress
            await loadCourseData();
            await loadUserProfile();
            await loadProgress();

            // Initialize UI
            initializeEventListeners();
            renderSidebar();
            loadLesson(currentLessonIndex);

            // Initialize additional features
            initMusicPlayer();
            addMusicIndicator();
            initializeActivityTracking();

            hideLoadingScreen();
          } else {
            openAuthModal();
          }
        } catch (error) {
          console.error("Error initializing course:", error);
          hideLoadingScreen();
          showToast("Failed to initialize course system", "error");
        }
      });

      // =====================================================
      // AUTHENTICATION SYSTEM
      // =====================================================
      async function checkAuth() {
        try {
          const {
            data: { session },
            error,
          } = await supabase.auth.getSession();

          if (error) {
            console.error("Auth error:", error);
            currentUser = null;
            openAuthModal();
            return;
          }

          if (!session) {
            currentUser = null;
            openAuthModal();
            return;
          }

          currentUser = session.user;
          updateUIWithUser();
        } catch (error) {
          console.error("Error checking auth:", error);
          currentUser = null;
          openAuthModal();
        }
      }

      function updateUIWithUser() {
        if (!currentUser) return;

        const name =
          currentUser.user_metadata?.full_name ||
          currentUser.email.split("@")[0];
        const userElements = document.querySelectorAll("[data-user-name]");
        userElements.forEach((el) => (el.textContent = name));
      }

      // =====================================================
      // USER PROFILE & STATS MANAGEMENT
      // =====================================================
      async function loadUserProfile() {
        try {
          // 🔍 Step 1: Try to fetch existing profile
          const { data: profile, error } = await supabase
            .from("profiles")
            .select("*")
            .eq("id", currentUser.id)
            .single();

          if (error && error.code !== "PGRST116") {
            throw error;
          }

          if (!profile) {
            // 🆕 Step 2: Create new profile if missing
            await createUserProfile();
          } else {
            // ✅ Step 3: Use existing profile
            userStats = profile;
            await updateLastActivity();
          }
        } catch (error) {
          console.error("❌ Error loading user profile:", error);
          showToast("Failed to load user profile", "error");
        }
      }
      async function createUserProfile() {
        try {
          const newProfile = {
            id: currentUser.id,
            full_name: currentUser.user_metadata?.full_name || "",
            email: currentUser.email,
            level: "Script Kiddie",
            total_points: 0,
            completed_courses: 0,
            current_streak: 0,
            total_certificates: 0,
            total_achievements: 0,
            sections_visited: 0,
            bonus_points: 0,
            in_progress_courses: 0,
            settings_changed: 0,
            midnight_sessions: 0,
            early_sessions: 0,
            weekend_sessions: 0,
            last_activity: new Date().toISOString(),
            created_at: new Date().toISOString(),
          };

          const { error } = await supabase
            .from("profiles")
            .insert([newProfile]);

          if (!error) {
            userStats = newProfile;
            // Award first login achievement
            setTimeout(() => checkAndUnlockAchievement("first_login"), 1000);
          }
        } catch (error) {
          console.error("Error creating user profile:", error);
        }
      }

      async function updateLastActivity() {
        try {
          const now = new Date();

          // Update activity tracking
          courseSession.lastActivityTime = now;

          // Update database
          await supabase
            .from("profiles")
            .update({
              last_activity: now.toISOString(),
            })
            .eq("id", currentUser.id);

          // Check time-based achievements
          await checkTimeBasedAchievements(now);
        } catch (error) {
          console.error("Error updating last activity:", error);
        }
      }

      // =====================================================
      // COURSE SESSION MANAGEMENT
      // =====================================================
      function startCourseSession() {
        courseSession.startTime = new Date();
        courseSession.pageLoadTime = new Date();

        logUserActivity("course_session_start", {
          course_id: COURSE_DATA.id,
          course_title: COURSE_DATA.title,
          user_agent: navigator.userAgent,
          screen_resolution: `${screen.width}x${screen.height}`,
        });

        // Check daily streak
        checkDailyStreak();
      }

      function initializeActivityTracking() {
        // Track page focus/blur for accurate study time
        document.addEventListener("visibilitychange", handleVisibilityChange);

        // Track user interactions
        ["click", "keydown", "scroll", "mousemove"].forEach((event) => {
          document.addEventListener(event, trackUserInteraction, {
            passive: true,
          });
        });

        // Periodic activity updates
        setInterval(updateStudyTime, 60000); // Every minute

        // Save session data before page unload
        window.addEventListener("beforeunload", saveSessionData);
      }

      function handleVisibilityChange() {
        if (document.hidden) {
          courseSession.lastActivityTime = new Date();
        } else {
          // Page became visible again - update activity
          updateLastActivity();
        }
      }

      function trackUserInteraction() {
        courseSession.interactionCount++;
        courseSession.lastActivityTime = new Date();

        // Throttle activity updates
        if (courseSession.interactionCount % 50 === 0) {
          updateLastActivity();
        }
      }

      function updateStudyTime() {
        if (!courseSession.startTime || document.hidden) return;

        const now = new Date();
        const sessionDuration = now - courseSession.startTime;
        courseSession.totalStudyTime = Math.floor(sessionDuration / 1000 / 60); // in minutes

        // Update progress with study time
        saveProgress();
      }

      function saveSessionData() {
        if (!currentUser || !courseSession.startTime) return;

        const sessionData = {
          user_id: currentUser.id,
          course_id: COURSE_DATA.id,
          session_duration: courseSession.totalStudyTime,
          lessons_viewed: courseSession.lessonsStarted,
          lessons_completed: courseSession.lessonsCompleted,
          interactions: courseSession.interactionCount,
          session_date: courseSession.startTime.toISOString().split("T")[0],
        };

        // Store in localStorage as backup
        localStorage.setItem(
          "course_session_backup",
          JSON.stringify(sessionData)
        );

        // Try to save to database
        logUserActivity("course_session_end", sessionData);
      }

      // =====================================================
      // COURSE DATA & PROGRESS MANAGEMENT
      // =====================================================
      async function loadCourseData() {
        try {
          // Update course info in UI
          document.getElementById("courseTitle").textContent =
            COURSE_DATA.title;
          document.getElementById("totalLessons").textContent =
            COURSE_DATA.lessons.length;

          // Log course access
          logUserActivity("course_access", {
            course_id: COURSE_DATA.id,
            course_title: COURSE_DATA.title,
          });
        } catch (error) {
          console.error("Error loading course data:", error);
        }
      }

      async function loadProgress() {
        try {
          // Get existing progress from database
          const { data, error } = await supabase
            .from("course_progress")
            .select("*")
            .eq("user_id", currentUser.id)
            .eq("course_id", COURSE_DATA.id)
            .single();

          if (error && error.code !== "PGRST116") {
            throw error;
          }

          if (data) {
            courseProgress = JSON.parse(data.lesson_progress || "{}");
            currentLessonIndex = data.current_lesson || 0;

            // Update session tracking
            courseSession.lessonsStarted = Object.keys(courseProgress).length;
            courseSession.lessonsCompleted = Object.values(
              courseProgress
            ).filter((p) => p.completed).length;
          } else {
            // Initialize new progress
            courseProgress = {};
            currentLessonIndex = 0;
            await saveProgress();

            // Award first course start achievement
            await checkAndUnlockAchievement("first_course_start");
          }

          updateProgressDisplay();
        } catch (error) {
          console.error("Error loading progress:", error);
          showToast("Failed to load progress", "error");
        }
      }

      async function saveProgress() {
        try {
          if (!currentUser) return;

          const now = new Date();
          const progressData = {
            user_id: currentUser.id,
            course_id: COURSE_DATA.id,
            course_title: COURSE_DATA.title,
            current_lesson: currentLessonIndex,
            lesson_progress: JSON.stringify(courseProgress),
            progress: calculateOverallProgress(),
            lessons_completed: getCompletedLessonsCount(),
            lessons_started: Object.keys(courseProgress).length,
            study_time_minutes: courseSession.totalStudyTime,
            last_accessed: now.toISOString(),
            updated_at: now.toISOString(),
          };

          // Upsert progress
          const { error } = await supabase
            .from("course_progress")
            .upsert([progressData]);

          if (error) throw error;

          // Update user stats
          await updateUserStats();

          updateProgressDisplay();
        } catch (error) {
          console.error("Error saving progress:", error);
          showToast("Failed to save progress", "error");
        }
      }

      async function updateUserStats() {
        try {
          // Get all course progress for this user
          const { data: allProgress, error } = await supabase
            .from("course_progress")
            .select("progress, course_id, study_time_minutes")
            .eq("user_id", currentUser.id);

          if (error) throw error;

          // Calculate stats
          const completedCourses =
            allProgress?.filter((p) => p.progress >= 100).length || 0;
          const inProgressCourses =
            allProgress?.filter((p) => p.progress > 0 && p.progress < 100)
              .length || 0;
          const totalStudyTime =
            allProgress?.reduce(
              (sum, p) => sum + (p.study_time_minutes || 0),
              0
            ) || 0;

          // Calculate points (500 per completed course + achievement points)
          const coursePoints = completedCourses * 500;
          const bonusPoints = userStats.bonus_points || 0;
          const totalPoints = coursePoints + bonusPoints;

          // Update profile
          const updateData = {
            completed_courses: completedCourses,
            in_progress_courses: inProgressCourses,
            total_points: totalPoints,
            total_study_time: totalStudyTime,
            last_activity: new Date().toISOString(),
          };

          const { error: updateError } = await supabase
            .from("profiles")
            .update(updateData)
            .eq("id", currentUser.id);

          if (updateError) throw updateError;

          // Update local stats
          Object.assign(userStats, updateData);
        } catch (error) {
          console.error("Error updating user stats:", error);
        }
      }

      function calculateOverallProgress() {
        const completedLessons = getCompletedLessonsCount();
        return Math.round(
          (completedLessons / COURSE_DATA.lessons.length) * 100
        );
      }

      function getCompletedLessonsCount() {
        return Object.values(courseProgress).filter(
          (lesson) => lesson.completed
        ).length;
      }

      function updateProgressDisplay() {
        const completedCount = getCompletedLessonsCount();
        const progressPercent = calculateOverallProgress();

        // Update UI elements
        const elements = {
          completedLessons: completedCount,
          courseProgressPercent: `${progressPercent}%`,
        };

        Object.entries(elements).forEach(([id, value]) => {
          const element = document.getElementById(id);
          if (element) element.textContent = value;
        });

        // Update progress bar
        const progressBar = document.getElementById("courseProgressFill");
        if (progressBar) progressBar.style.width = `${progressPercent}%`;
      }

      // =====================================================
      // LESSON MANAGEMENT
      // =====================================================
      function loadLesson(index) {
        if (index < 0 || index >= COURSE_DATA.lessons.length) return;

        const lesson = COURSE_DATA.lessons[index];
        currentLessonIndex = index;

        // Mark lesson as started and track activity
        if (!courseProgress[lesson.id]) {
          courseProgress[lesson.id] = {
            started: true,
            completed: false,
            startedAt: new Date().toISOString(),
            timeSpent: 0,
          };

          courseSession.lessonsStarted++;
          saveProgress();

          // Log lesson start
          logUserActivity("lesson_start", {
            lesson_id: lesson.id,
            lesson_title: lesson.title,
            lesson_index: index,
          });
        }

        // Update lesson start time for time tracking
        courseProgress[lesson.id].currentSessionStart = new Date();

        // Update sidebar and navigation
        renderSidebar();
        updateNavigationButtons();

        // Update header info
        updateLessonHeader(lesson, index);

        // Render lesson content
        renderLessonContent(lesson);

        // Smooth scroll and animations
        window.scrollTo({ top: 0, behavior: "smooth" });
        document.getElementById("contentBody").scrollTop = 0;

        const contentBody = document.getElementById("contentBody");
        contentBody.classList.add("fade-in");
        setTimeout(() => contentBody.classList.remove("fade-in"), 500);

        // Check lesson-based achievements
        checkLessonAchievements(index + 1);
      }

      function updateLessonHeader(lesson, index) {
        const elements = {
          currentLessonNumber: index + 1,
          currentLessonTitle: lesson.title,
          navInfo: `Lesson ${index + 1} of ${COURSE_DATA.lessons.length}`,
        };

        Object.entries(elements).forEach(([id, value]) => {
          const element = document.getElementById(id);
          if (element) element.textContent = value;
        });
      }

      function renderSidebar() {
        const lessonNav = document.getElementById("lessonNav");
        if (!lessonNav) return;

        lessonNav.innerHTML = "";

        COURSE_DATA.lessons.forEach((lesson, index) => {
          const lessonItem = document.createElement("div");
          lessonItem.className = "lesson-item";

          // Determine lesson status
          const lessonProgress = courseProgress[lesson.id];
          const isCompleted = lessonProgress?.completed || false;
          const isInProgress = lessonProgress?.started || false;
          const isLocked = !isCompleted && !canAccessLesson(index);
          const isActive = index === currentLessonIndex;

          // Apply status classes
          if (isCompleted) lessonItem.classList.add("completed");
          if (isLocked) lessonItem.classList.add("locked");
          if (isActive) lessonItem.classList.add("active");

          // Determine status display
          let statusClass = "not-started";
          let statusIcon = index + 1;

          if (isCompleted) {
            statusClass = "completed";
            statusIcon = "✓";
          } else if (isInProgress) {
            statusClass = "in-progress";
            statusIcon = "◐";
          }

          // Create lesson item HTML
          lessonItem.innerHTML = `
            <div class="lesson-status ${statusClass}">${statusIcon}</div>
            <div class="lesson-info">
                <div class="lesson-title">${lesson.title}</div>
                <div class="lesson-meta">
                    ${
                      isCompleted
                        ? "Completed"
                        : isInProgress
                        ? "In Progress"
                        : isLocked
                        ? "Locked"
                        : "Not Started"
                    }
                </div>
            </div>
            <div class="lesson-duration">${lesson.duration}</div>
        `;

          // Add click handler if not locked
          if (!isLocked) {
            lessonItem.addEventListener("click", () => {
              if (index !== currentLessonIndex) {
                // Track lesson time before switching
                trackLessonTime();

                currentLessonIndex = index;
                loadLesson(index);
                closeSidebar();
              }
            });
          }

          lessonNav.appendChild(lessonItem);
        });
      }

      function canAccessLesson(index) {
        if (index === 0) return true; // First lesson always accessible

        // Can access if previous lesson is completed
        const previousLessonId = COURSE_DATA.lessons[index - 1].id;
        return courseProgress[previousLessonId]?.completed || false;
      }

      function trackLessonTime() {
        const currentLesson = COURSE_DATA.lessons[currentLessonIndex];
        const lessonProgress = courseProgress[currentLesson.id];

        if (lessonProgress?.currentSessionStart) {
          const sessionTime = Math.floor(
            (new Date() - new Date(lessonProgress.currentSessionStart)) /
              1000 /
              60
          );
          lessonProgress.timeSpent =
            (lessonProgress.timeSpent || 0) + sessionTime;
          delete lessonProgress.currentSessionStart;
        }
      }

      // =====================================================
      // LESSON CONTENT RENDERING
      // =====================================================
      function renderLessonContent(lesson) {
        const contentContainer = document.getElementById("lessonContent");
        if (!contentContainer) return;

        let contentHTML = `
        <div class="content-section">
            <h2>Learning Objectives</h2>
            <ul>
                ${lesson.objectives.map((obj) => `<li>${obj}</li>`).join("")}
            </ul>
        </div>
        
        <div class="content-section">
            <h2>Overview</h2>
            <p>${lesson.content.overview}</p>
        </div>
    `;

        // Render content sections
        lesson.content.sections.forEach((section) => {
          contentHTML += `
            <div class="content-section">
                <h2>${section.title}</h2>
                ${section.content}
                ${
                  section.image
                    ? `<img src="${section.image}" alt="${section.title}" class="content-image" loading="lazy">`
                    : ""
                }
            </div>
        `;
        });

        // Render code examples
        if (
          lesson.content.codeExamples &&
          lesson.content.codeExamples.length > 0
        ) {
          contentHTML += `<div class="content-section"><h2>Code Examples</h2></div>`;

          lesson.content.codeExamples.forEach((example, index) => {
            const codeId = `code-${lesson.id}-${index}`;
            contentHTML += `
                <div class="code-section">
                    <div class="code-header">
                        <span class="code-title">${example.title}</span>
                        <button class="copy-btn" onclick="copyCode('${codeId}')">
                            <i class="fas fa-copy"></i> Copy
                        </button>
                    </div>
                    <pre class="code-block"><code id="${codeId}" class="language-${
              example.language
            }">${escapeHtml(example.code)}</code></pre>
                </div>
            `;
          });
        }

        // Render quiz
        contentHTML += renderQuiz(lesson.quiz);

        contentContainer.innerHTML = contentHTML;

        // Highlight code syntax if Prism is available
        setTimeout(() => {
          if (window.Prism) {
            Prism.highlightAll();
          }
        }, 100);
      }

      // =====================================================
      // QUIZ SYSTEM
      // =====================================================
      function renderQuiz(quiz) {
        const currentLessonProgress =
          courseProgress[COURSE_DATA.lessons[currentLessonIndex].id];
        const isCompleted = currentLessonProgress?.completed || false;

        let quizHTML = `
        <div class="quiz-section" id="quizSection">
            <div class="quiz-header">
                <h2 class="quiz-title">
                    <i class="fas fa-clipboard-check"></i>
                    Knowledge Check
                </h2>
                <p class="quiz-info">
                    Complete this quiz with ${quiz.passingScore}% or higher to unlock the next lesson.
                </p>
            </div>
    `;

        if (isCompleted) {
          const savedScore = currentLessonProgress.quizScore || 0;
          quizHTML += `
            <div class="quiz-results show">
                <div class="quiz-score pass">${savedScore}%</div>
                <div class="quiz-message">
                    <strong>Lesson Completed!</strong><br>
                    You've successfully passed this lesson's quiz.
                </div>
            </div>
        `;
        } else {
          // Render quiz questions
          quiz.questions.forEach((question, qIndex) => {
            quizHTML += `
                <div class="quiz-question ${
                  qIndex === 0 ? "active" : ""
                }" data-question="${qIndex}">
                    <div class="question-header">
                        <span class="question-number">Question ${
                          qIndex + 1
                        }</span>
                        <span class="question-progress">${qIndex + 1}/${
              quiz.questions.length
            }</span>
                    </div>
                    <div class="question-text">${question.question}</div>
                    <div class="question-options">
                        ${question.options
                          .map(
                            (option, oIndex) => `
                            <div class="option" data-option="${oIndex}" onclick="selectOption(${qIndex}, ${oIndex})">
                                <span class="option-letter">${String.fromCharCode(
                                  65 + oIndex
                                )}</span>
                                <span class="option-text">${option}</span>
                            </div>
                        `
                          )
                          .join("")}
                    </div>
                </div>
            `;
          });

          quizHTML += `
            <div class="quiz-controls">
                <button class="btn btn-secondary" id="prevQuestionBtn" onclick="previousQuestion()" disabled>
                    <i class="fas fa-chevron-left"></i>
                    Previous
                </button>
                <div>
                    <button class="btn btn-secondary" id="nextQuestionBtn" onclick="nextQuestion()" disabled>
                        Next
                        <i class="fas fa-chevron-right"></i>
                    </button>
                    <button class="btn btn-primary" id="submitQuizBtn" onclick="submitQuiz()" style="display: none;">
                        <i class="fas fa-check"></i>
                        Submit Quiz
                    </button>
                </div>
            </div>

            <div class="quiz-results" id="quizResults">
                <div class="quiz-score" id="quizScore">0%</div>
                <div class="quiz-message" id="quizMessage"></div>
                <div class="quiz-actions">
                    <button class="btn btn-primary" id="continueBtn" onclick="completeLesson()" style="display: none;">
                        <i class="fas fa-arrow-right"></i>
                        Continue to Next Lesson
                    </button>
                    <button class="btn btn-secondary" onclick="retakeQuiz()">
                        <i class="fas fa-redo"></i>
                        Retake Quiz
                    </button>
                </div>
            </div>
        `;
        }

        quizHTML += "</div>";
        return quizHTML;
      }

      // Quiz interaction functions
      function selectOption(questionIndex, optionIndex) {
        // Clear previous selections
        document
          .querySelectorAll(`[data-question="${questionIndex}"] .option`)
          .forEach((opt) => {
            opt.classList.remove("selected");
          });

        // Select current option
        const selectedOption = document.querySelector(
          `[data-question="${questionIndex}"] [data-option="${optionIndex}"]`
        );
        selectedOption.classList.add("selected");

        // Store answer
        quizState.answers[questionIndex] = optionIndex;

        // Update navigation
        const isLastQuestion =
          questionIndex ===
          COURSE_DATA.lessons[currentLessonIndex].quiz.questions.length - 1;
        if (isLastQuestion) {
          document.getElementById("submitQuizBtn").style.display =
            "inline-flex";
          document.getElementById("nextQuestionBtn").style.display = "none";
        } else {
          document.getElementById("nextQuestionBtn").disabled = false;
        }
      }

      function nextQuestion() {
        const currentQuestionEl = document.querySelector(
          ".quiz-question.active"
        );
        const nextQuestionEl = currentQuestionEl.nextElementSibling;

        if (
          nextQuestionEl &&
          nextQuestionEl.classList.contains("quiz-question")
        ) {
          currentQuestionEl.classList.remove("active");
          nextQuestionEl.classList.add("active");
          quizState.currentQuestion++;
          updateQuizNavigation();
        }
      }

      function previousQuestion() {
        const currentQuestionEl = document.querySelector(
          ".quiz-question.active"
        );
        const prevQuestionEl = currentQuestionEl.previousElementSibling;

        if (
          prevQuestionEl &&
          prevQuestionEl.classList.contains("quiz-question")
        ) {
          currentQuestionEl.classList.remove("active");
          prevQuestionEl.classList.add("active");
          quizState.currentQuestion--;
          updateQuizNavigation();
        }
      }

      function updateQuizNavigation() {
        const totalQuestions =
          COURSE_DATA.lessons[currentLessonIndex].quiz.questions.length;
        const prevBtn = document.getElementById("prevQuestionBtn");
        const nextBtn = document.getElementById("nextQuestionBtn");
        const submitBtn = document.getElementById("submitQuizBtn");

        prevBtn.disabled = quizState.currentQuestion === 0;

        const hasAnswer =
          quizState.answers[quizState.currentQuestion] !== undefined;
        const isLastQuestion = quizState.currentQuestion === totalQuestions - 1;

        if (isLastQuestion) {
          nextBtn.style.display = "none";
          submitBtn.style.display = hasAnswer ? "inline-flex" : "none";
        } else {
          nextBtn.style.display = "inline-flex";
          nextBtn.disabled = !hasAnswer;
          submitBtn.style.display = "none";
        }
      }

      async function submitQuiz() {
        const lesson = COURSE_DATA.lessons[currentLessonIndex];
        const quiz = lesson.quiz;
        let correctAnswers = 0;

        // Hide questions and controls
        document
          .querySelectorAll(".quiz-question")
          .forEach((q) => (q.style.display = "none"));
        document.querySelector(".quiz-controls").style.display = "none";

        // Calculate score and highlight answers
        quiz.questions.forEach((question, index) => {
          const userAnswer = quizState.answers[index];
          const correctAnswer = question.correct;
          const isCorrect = userAnswer === correctAnswer;

          if (isCorrect) correctAnswers++;

          // Highlight answers
          const questionEl = document.querySelector(
            `[data-question="${index}"]`
          );
          const options = questionEl.querySelectorAll(".option");

          options[correctAnswer].classList.add("correct");
          if (userAnswer !== correctAnswer && userAnswer !== undefined) {
            options[userAnswer].classList.add("incorrect");
          }
        });

        const score = Math.round(
          (correctAnswers / quiz.questions.length) * 100
        );
        const passed = score >= quiz.passingScore;

        // Update quiz results UI
        const quizScore = document.getElementById("quizScore");
        const quizMessage = document.getElementById("quizMessage");
        const quizActions = document.querySelector(".quiz-actions");

        quizScore.textContent = `${score}%`;
        quizScore.className = `quiz-score ${passed ? "pass" : "fail"}`;

        if (passed) {
          quizMessage.innerHTML = `
            <strong>Congratulations!</strong><br>
            You passed with ${score}%. You can now proceed to the next lesson.
        `;

          quizActions.innerHTML = `
            <button class="btn btn-primary" onclick="completeLesson()">
                <i class="fas fa-arrow-right"></i>
                Continue to Next Lesson
            </button>
        `;

          // Mark lesson as completed
          await markLessonComplete(score);
        } else {
          quizMessage.innerHTML = `
            <strong>Not quite there yet.</strong><br>
            You scored ${score}%. You need ${quiz.passingScore}% to pass. Review the material and try again.
        `;

          quizActions.innerHTML = `
            <button class="btn btn-secondary" onclick="retakeQuiz()">
                <i class="fas fa-redo"></i>
                Retake Quiz
            </button>
        `;
        }

        document.getElementById("quizResults").classList.add("show");

        // Log quiz completion
        logUserActivity("quiz_completed", {
          lesson_id: lesson.id,
          lesson_title: lesson.title,
          score: score,
          passed: passed,
          attempts: (courseProgress[lesson.id]?.quizAttempts || 0) + 1,
        });

        // Scroll to results
        document
          .getElementById("quizResults")
          .scrollIntoView({ behavior: "smooth" });
      }

      function retakeQuiz() {
        // Reset quiz state
        quizState = {
          currentQuestion: 0,
          answers: [],
          score: 0,
          isComplete: false,
        };

        // Track quiz retry
        const lessonId = COURSE_DATA.lessons[currentLessonIndex].id;
        if (!courseProgress[lessonId]) courseProgress[lessonId] = {};
        courseProgress[lessonId].quizAttempts =
          (courseProgress[lessonId].quizAttempts || 0) + 1;

        // Reload lesson content
        loadLesson(currentLessonIndex);

        // Scroll to quiz
        setTimeout(() => {
          document
            .getElementById("quizSection")
            .scrollIntoView({ behavior: "smooth" });
        }, 500);
      }

      async function markLessonComplete(score) {
        const lesson = COURSE_DATA.lessons[currentLessonIndex];
        const lessonId = lesson.id;

        // Track lesson completion time
        trackLessonTime();

        // Update lesson progress
        courseProgress[lessonId] = {
          ...courseProgress[lessonId],
          completed: true,
          quizScore: score,
          completedAt: new Date().toISOString(),
          finalScore: score,
        };

        // Update session tracking
        courseSession.lessonsCompleted++;

        // Save progress to database
        await saveProgress();

        // Update UI
        renderSidebar();
        updateNavigationButtons();

        // Check various achievements
        await checkLessonCompletionAchievements();
        await checkPerfectScoreAchievements(score);
        await checkStudyTimeAchievements();

        // Log lesson completion
        logUserActivity("lesson_completed", {
          lesson_id: lessonId,
          lesson_title: lesson.title,
          final_score: score,
          time_spent: courseProgress[lessonId].timeSpent || 0,
          lesson_number: currentLessonIndex + 1,
        });

        showToast("Lesson completed successfully!", "success");
      }

      async function completeLesson() {
        if (currentLessonIndex < COURSE_DATA.lessons.length - 1) {
          // Move to next lesson
          currentLessonIndex++;
          loadLesson(currentLessonIndex);
        } else {
          // Course completion
          await completeCourse();
        }
      }

      async function completeCourse() {
        try {
          const completionTime = courseSession.totalStudyTime;

          // Update course progress to 100% completed
          await supabase
            .from("course_progress")
            .update({
              progress: 100,
              completed_at: new Date().toISOString(),
              completion_time_minutes: completionTime,
            })
            .eq("user_id", currentUser.id)
            .eq("course_id", COURSE_DATA.id);

          // Award course completion achievements
          await checkAndUnlockAchievement("course_completion_1");
          await checkSpeedCompletionAchievements(completionTime);
          await checkCourseStreakAchievements();

          // Update user stats
          await updateUserStats();

          // Show completion message
          showToast(
            "Congratulations! Course completed successfully!",
            "certificate"
          );

          // Log course completion
          logUserActivity("course_completed", {
            course_id: COURSE_DATA.id,
            course_title: COURSE_DATA.title,
            completion_time_minutes: completionTime,
            total_lessons: COURSE_DATA.lessons.length,
            average_quiz_score: calculateAverageQuizScore(),
          });

          // Redirect to dashboard after delay
          setTimeout(() => {
            window.location.href = "/dashboard.html";
          }, 3000);
        } catch (error) {
          console.error("Error completing course:", error);
          showToast("Error marking course as complete", "error");
        }
      }

      // =====================================================
      // ACHIEVEMENT INTEGRATION SYSTEM
      // =====================================================
      async function checkAndUnlockAchievement(
        achievementId,
        skipNotification = false
      ) {
        try {
          // Prevent duplicate checks
          const cacheKey = `achievement_${achievementId}_${currentUser.id}`;
          if (sessionStorage.getItem(cacheKey)) return false;

          // Check if already unlocked
          const { data: existing, error } = await supabase
            .from("user_achievements")
            .select("id")
            .eq("user_id", currentUser.id)
            .eq("achievement_id", achievementId)
            .single();

          if (existing) {
            sessionStorage.setItem(cacheKey, "true");
            return false;
          }

          // Award achievement
          const achievementData = {
            user_id: currentUser.id,
            achievement_id: achievementId,
            unlocked_at: new Date().toISOString(),
            points_awarded: getAchievementPoints(achievementId),
            context: "course_system",
          };

          const { data, error: insertError } = await supabase
            .from("user_achievements")
            .insert([achievementData])
            .select()
            .single();

          if (insertError) {
            if (insertError.code === "23505") return false; // Already exists
            throw insertError;
          }

          // Update user points
          await supabase.rpc('increment_user_stats', {
  user_id_param: currentUser.id,
  points_to_add: achievementData.points_awarded,
  achievements_to_add: 1
});

          // Cache and queue notification
          sessionStorage.setItem(cacheKey, "true");

          if (!skipNotification) {
            queueAchievementNotification({
              id: achievementId,
              name: getAchievementName(achievementId),
              description: getAchievementDescription(achievementId),
              points: achievementData.points_awarded,
              rarity: getAchievementRarity(achievementId),
              icon: getAchievementIcon(achievementId),
            });
          }

          return true;
        } catch (error) {
          console.error("Error unlocking achievement:", error);
          return false;
        }
      }

      // Helper functions for achievement data (replace with your actual achievement definitions)
      function getAchievementPoints(id) {
        const pointsMap = {
          first_course_start: 75,
          first_lesson: 100,
          course_completion_1: 500,
          perfect_score: 250,
          speed_demon: 750,
          marathon_learner: 500,
          night_owl: 200,
          early_bird: 200,
          weekend_warrior: 150,
          // Add more as needed
        };
        return pointsMap[id] || 100;
      }

      function getAchievementName(id) {
        const nameMap = {
          first_course_start: "Learning Initiated",
          first_lesson: "First Steps",
          course_completion_1: "Course Conqueror",
          perfect_score: "Perfectionist",
          speed_demon: "Speed Demon",
          // Add more as needed
        };
        return nameMap[id] || "Achievement Unlocked";
      }

      function getAchievementDescription(id) {
        const descMap = {
          first_course_start: "Start your first cybersecurity course",
          first_lesson: "Complete your first lesson",
          course_completion_1: "Complete your first course",
          perfect_score: "Score 100% on any quiz",
          speed_demon: "Complete a course in under 2 hours",
          // Add more as needed
        };
        return descMap[id] || "Achievement description";
      }

      function getAchievementRarity(id) {
        const rarityMap = {
          first_course_start: "common",
          first_lesson: "bronze",
          course_completion_1: "silver",
          perfect_score: "gold",
          speed_demon: "legendary",
          // Add more as needed
        };
        return rarityMap[id] || "common";
      }

      function getAchievementIcon(id) {
        const iconMap = {
          first_course_start: "fas fa-play",
          first_lesson: "fas fa-baby",
          course_completion_1: "fas fa-trophy",
          perfect_score: "fas fa-star",
          speed_demon: "fas fa-rocket",
          // Add more as needed
        };
        return iconMap[id] || "fas fa-award";
      }

      async function checkLessonAchievements(lessonNumber) {
        if (lessonNumber === 1) {
          await checkAndUnlockAchievement("first_lesson");
        }

        // Check if user has completed multiple lessons in one day
        const today = new Date().toISOString().split("T")[0];
        const todayCompletions = Object.values(courseProgress).filter(
          (p) => p.completed && p.completedAt?.startsWith(today)
        ).length;

        if (todayCompletions >= 3) {
          await checkAndUnlockAchievement("lesson_marathon");
        }
      }

      async function checkLessonCompletionAchievements() {
        const completedCount = getCompletedLessonsCount();

        // Lesson-based achievements
        const lessonMilestones = [1, 5, 10, 25, 50, 100];
        for (const milestone of lessonMilestones) {
          if (completedCount >= milestone) {
            await checkAndUnlockAchievement(`lessons_${milestone}`);
          }
        }

        // Course completion achievements
        if (completedCount === COURSE_DATA.lessons.length) {
          await checkAndUnlockAchievement("course_completion_1");

          // Check for additional course completion achievements
          const { data: allCourses } = await supabase
            .from("course_progress")
            .select("course_id")
            .eq("user_id", currentUser.id)
            .eq("progress", 100);

          const totalCompleted = allCourses?.length || 0;

          const courseMilestones = [1, 5, 10, 25];
          for (const milestone of courseMilestones) {
            if (totalCompleted >= milestone) {
              await checkAndUnlockAchievement(`course_completion_${milestone}`);
            }
          }
        }
      }

      async function checkPerfectScoreAchievements(score) {
        if (score === 100) {
          await checkAndUnlockAchievement("perfect_score");

          // Check for consecutive perfect scores
          const recentScores = Object.values(courseProgress)
            .filter((p) => p.completed && p.quizScore)
            .slice(-5) // Last 5 completed
            .map((p) => p.quizScore);

          if (
            recentScores.length >= 3 &&
            recentScores.every((s) => s === 100)
          ) {
            await checkAndUnlockAchievement("perfectionist_streak");
          }
        }
      }

      async function checkSpeedCompletionAchievements(completionTime) {
        // Speed demon: Complete course in under 2 hours (120 minutes)
        if (completionTime <= 120) {
          await checkAndUnlockAchievement("speed_demon");
        }

        // Quick learner: Complete course in under 4 hours (240 minutes)
        if (completionTime <= 240) {
          await checkAndUnlockAchievement("quick_learner");
        }
      }

      async function checkStudyTimeAchievements() {
        const totalTime = courseSession.totalStudyTime;

        // Marathon learner: Study for 8+ hours in a course session
        if (totalTime >= 480) {
          // 8 hours
          await checkAndUnlockAchievement("marathon_learner");
        }

        // Dedicated learner: Study for 4+ hours
        if (totalTime >= 240) {
          // 4 hours
          await checkAndUnlockAchievement("dedicated_learner");
        }
      }

      async function checkCourseStreakAchievements() {
        try {
          // Check for consecutive course completions
          const { data: recentCourses, error } = await supabase
            .from("course_progress")
            .select("completed_at, course_id")
            .eq("user_id", currentUser.id)
            .eq("progress", 100)
            .order("completed_at", { ascending: false })
            .limit(10);

          if (error || !recentCourses) return;

          // Check for courses completed on consecutive days
          let consecutiveDays = 1;
          for (let i = 1; i < recentCourses.length; i++) {
            const prevDate = new Date(
              recentCourses[i - 1].completed_at
            ).toDateString();
            const currentDate = new Date(
              recentCourses[i].completed_at
            ).toDateString();
            const dayDiff =
              Math.abs(new Date(prevDate) - new Date(currentDate)) /
              (1000 * 60 * 60 * 24);

            if (dayDiff <= 1) {
              consecutiveDays++;
            } else {
              break;
            }
          }

          if (consecutiveDays >= 3) {
            await checkAndUnlockAchievement("learning_streak");
          }
        } catch (error) {
          console.error("Error checking course streak achievements:", error);
        }
      }

      // =====================================================
      // TIME-BASED ACHIEVEMENTS
      // =====================================================
      async function checkTimeBasedAchievements(timestamp) {
        const hour = timestamp.getHours();
        const dayOfWeek = timestamp.getDay();

        // Night owl (after midnight, before 6 AM)
        if (hour >= 0 && hour < 6) {
          await incrementTimeBasedCounter("midnight_sessions", "night_owl");
        }

        // Early bird (4 AM to 6 AM)
        if (hour >= 4 && hour < 6) {
          await incrementTimeBasedCounter("early_sessions", "early_bird");
        }

        // Weekend warrior (Saturday = 6, Sunday = 0)
        if (dayOfWeek === 0 || dayOfWeek === 6) {
          await incrementTimeBasedCounter(
            "weekend_sessions",
            "weekend_warrior"
          );
        }
      }

      async function incrementTimeBasedCounter(counterType, achievementId) {
        try {
          const dateStr = new Date().toISOString().split("T")[0];
          const cacheKey = `${counterType}_${currentUser.id}_${dateStr}`;

          // Prevent multiple increments per day
          if (sessionStorage.getItem(cacheKey)) return;

          // Update counter
       await supabase.rpc('increment_profile_counter', {
  user_id_param: currentUser.id,
  counter_field: counterType,
  increment_value: 1
});
          // Cache to prevent double counting
          sessionStorage.setItem(cacheKey, "true");

          // Check related achievement
          if (achievementId) {
            await checkAndUnlockAchievement(achievementId, true);
          }
        } catch (error) {
          console.error(`Error incrementing ${counterType}:`, error);
        }
      }

      async function checkDailyStreak() {
        try {
          const { data: profile, error } = await supabase
            .from("profiles")
            .select("last_activity, current_streak, last_streak_date")
            .eq("id", currentUser.id)
            .single();

          if (error) throw error;

          const now = new Date();
          const today = now.toDateString();
          const lastActivity = profile.last_activity
            ? new Date(profile.last_activity)
            : null;
          const lastStreakDate = profile.last_streak_date
            ? new Date(profile.last_streak_date).toDateString()
            : null;

          let newStreak = profile.current_streak || 0;
          let shouldUpdateStreak = false;

          if (!lastActivity) {
            // First time user
            newStreak = 1;
            shouldUpdateStreak = true;
          } else {
            const daysSinceLastActivity = Math.floor(
              (now - lastActivity) / (1000 * 60 * 60 * 24)
            );

            if (lastStreakDate === today) {
              // Already counted today's streak
              return newStreak;
            } else if (
              daysSinceLastActivity === 1 ||
              (daysSinceLastActivity === 0 && lastStreakDate !== today)
            ) {
              // Consecutive day or same day but not counted yet
              newStreak += 1;
              shouldUpdateStreak = true;
            } else if (daysSinceLastActivity > 1) {
              // Streak broken
              newStreak = 1;
              shouldUpdateStreak = true;
            }
          }

          if (shouldUpdateStreak) {
            await supabase
              .from("profiles")
              .update({
                current_streak: newStreak,
                last_activity: now.toISOString(),
                last_streak_date: now.toISOString(),
              })
              .eq("id", currentUser.id);

            userStats.current_streak = newStreak;

            showToast(`Daily streak: ${newStreak} days!`, "success");
            await checkStreakAchievements(newStreak);
          }

          return newStreak;
        } catch (error) {
          console.error("Error checking daily streak:", error);
          return 0;
        }
      }

      async function checkStreakAchievements(currentStreak) {
        const streakMilestones = [3, 7, 14, 30, 100, 365];

        for (const milestone of streakMilestones) {
          if (currentStreak >= milestone) {
            await checkAndUnlockAchievement(`streak_${milestone}`);
          }
        }
      }

      // =====================================================
      // USER ACTIVITY LOGGING
      // =====================================================
      async function logUserActivity(activityType, metadata = {}) {
        try {
          const now = new Date();

          const activityData = {
            user_id: currentUser.id,
            activity_type: activityType,
            timestamp: now.toISOString(),
            course_id: COURSE_DATA.id,
            lesson_index: currentLessonIndex,
            session_duration: courseSession.totalStudyTime,
            metadata: JSON.stringify(metadata),
            created_at: now.toISOString(),
          };

          // Try to log to activities table
          const { error } = await supabase
            .from("user_activities")
            .insert([activityData]);

          if (error && error.code !== "42P01") {
            console.warn("Activity logging failed:", error.message);
          }

          // Always update last activity in profile
          await supabase
            .from("profiles")
            .update({ last_activity: now.toISOString() })
            .eq("id", currentUser.id);
        } catch (error) {
          console.error("Error logging user activity:", error);
        }
      }

      // =====================================================
      // NAVIGATION SYSTEM
      // =====================================================
      function updateNavigationButtons() {
        const prevBtn = document.getElementById("prevLessonBtn");
        const nextBtn = document.getElementById("nextLessonBtn");

        if (!prevBtn || !nextBtn) return;

        // Previous button
        prevBtn.disabled = currentLessonIndex === 0;

        // Next button logic
        const isLastLesson =
          currentLessonIndex === COURSE_DATA.lessons.length - 1;
        const canGoNext =
          currentLessonIndex < COURSE_DATA.lessons.length - 1 &&
          canAccessLesson(currentLessonIndex + 1);

        if (isLastLesson) {
          const isCurrentCompleted =
            courseProgress[COURSE_DATA.lessons[currentLessonIndex].id]
              ?.completed;
          nextBtn.disabled = !isCurrentCompleted;
          nextBtn.innerHTML = '<i class="fas fa-trophy"></i> Complete Course';
        } else {
          nextBtn.disabled = !canGoNext;
          nextBtn.innerHTML = 'Next <i class="fas fa-chevron-right"></i>';
        }
      }

      // Navigation button event handlers
      function goToPreviousLesson() {
        if (currentLessonIndex > 0) {
          trackLessonTime(); // Track time spent on current lesson
          currentLessonIndex--;
          loadLesson(currentLessonIndex);
        }
      }

      function goToNextLesson() {
        if (currentLessonIndex < COURSE_DATA.lessons.length - 1) {
          const canGoNext = canAccessLesson(currentLessonIndex + 1);
          if (canGoNext) {
            trackLessonTime();
            currentLessonIndex++;
            loadLesson(currentLessonIndex);
          } else {
            showToast("Complete the current lesson quiz to proceed", "warning");
          }
        } else {
          // Complete course
          completeCourse();
        }
      }

      // =====================================================
      // SIDEBAR FUNCTIONS
      // =====================================================
      function toggleSidebar() {
        const sidebar = document.getElementById("sidebar");
        const overlay = document.getElementById("sidebarOverlay");

        if (sidebar) sidebar.classList.toggle("open");
        if (overlay) overlay.classList.toggle("active");
      }

      function closeSidebar() {
        const sidebar = document.getElementById("sidebar");
        const overlay = document.getElementById("sidebarOverlay");

        if (sidebar) sidebar.classList.remove("open");
        if (overlay) overlay.classList.remove("active");
      }

      // =====================================================
      // ACHIEVEMENT NOTIFICATION SYSTEM
      // =====================================================
      function queueAchievementNotification(achievement) {
        notificationQueue.push(achievement);
        processNotificationQueue();
      }

      function processNotificationQueue() {
        if (isShowingNotification || notificationQueue.length === 0) return;

        isShowingNotification = true;
        const achievement = notificationQueue.shift();
        showAchievementNotification(achievement);

        const duration =
          achievement.rarity === "mythic"
            ? 8000
            : achievement.rarity === "legendary"
            ? 7000
            : 6000;

        setTimeout(() => {
          isShowingNotification = false;
          processNotificationQueue();
        }, duration + 500);
      }

      function showAchievementNotification(achievement) {
        const notification = document.getElementById("achievementNotification");
        if (!notification) return;

        const icon = document.getElementById("notificationIcon");
        const title = document.getElementById("notificationTitle");
        const description = document.getElementById("notificationDescription");
        const points = document.getElementById("notificationPoints");

        if (icon) {
          icon.innerHTML = `<i class="${achievement.icon}"></i>`;
          icon.className = `notification-icon ${achievement.rarity}`;
        }
        if (title) title.textContent = achievement.name;
        if (description) description.textContent = achievement.description;
        if (points) points.textContent = `+${achievement.points} Points`;

        notification.className = `achievement-notification ${achievement.rarity}`;
        notification.classList.add("show");

        const duration =
          achievement.rarity === "mythic"
            ? 8000
            : achievement.rarity === "legendary"
            ? 7000
            : 6000;

        setTimeout(() => {
          notification.classList.remove("show");
        }, duration);
      }

      // =====================================================
      // UTILITY FUNCTIONS
      // =====================================================
      function calculateAverageQuizScore() {
        const scores = Object.values(courseProgress)
          .filter((p) => p.completed && p.quizScore)
          .map((p) => p.quizScore);

        if (scores.length === 0) return 0;
        return Math.round(
          scores.reduce((sum, score) => sum + score, 0) / scores.length
        );
      }

      async function resetProgress() {
        if (
          !confirm(
            "Are you sure you want to reset your progress? This action cannot be undone."
          )
        ) {
          return;
        }

        try {
          // Track lesson time before reset
          trackLessonTime();

          // Reset local state
          courseProgress = {};
          currentLessonIndex = 0;
          courseSession = {
            startTime: new Date(),
            totalStudyTime: 0,
            lessonsStarted: 0,
            lessonsCompleted: 0,
            pageLoadTime: new Date(),
            interactionCount: 0,
            lastActivityTime: new Date(),
          };

          // Delete from database
          await supabase
            .from("course_progress")
            .delete()
            .eq("user_id", currentUser.id)
            .eq("course_id", COURSE_DATA.id);

          // Log reset activity
          logUserActivity("progress_reset", {
            course_id: COURSE_DATA.id,
            reset_reason: "manual",
          });

          // Reinitialize
          await saveProgress();
          renderSidebar();
          loadLesson(0);

          showToast("Progress reset successfully", "success");
        } catch (error) {
          console.error("Error resetting progress:", error);
          showToast("Failed to reset progress", "error");
        }
      }

      function copyCode(codeId) {
        const codeElement = document.getElementById(codeId);
        if (!codeElement) return;

        const text = codeElement.textContent;

        navigator.clipboard
          .writeText(text)
          .then(() => {
            showToast("Code copied to clipboard!", "success");

            // Track code copy for achievements
            logUserActivity("code_copied", {
              code_section: codeId,
              lesson_id: COURSE_DATA.lessons[currentLessonIndex].id,
            });
          })
          .catch((err) => {
            console.error("Failed to copy code:", err);
            showToast("Failed to copy code", "error");

            // Fallback for older browsers
            const textArea = document.createElement("textarea");
            textArea.value = text;
            document.body.appendChild(textArea);
            textArea.select();
            try {
              document.execCommand("copy");
              showToast("Code copied to clipboard!", "success");
            } catch (fallbackError) {
              showToast(
                "Copy failed - please select and copy manually",
                "error"
              );
            }
            document.body.removeChild(textArea);
          });
      }

      function escapeHtml(text) {
        const div = document.createElement("div");
        div.textContent = text;
        return div.innerHTML;
      }

      // =====================================================
      // UI FEEDBACK SYSTEMS
      // =====================================================
      function showToast(message, type = "success") {
        const toast = document.getElementById("toast");
        const messageEl = document.getElementById("toastMessage");

        if (!toast || !messageEl) {
          console.log(`Toast: ${message} (${type})`);
          return;
        }

        messageEl.textContent = message;
        toast.className = `toast ${type} show`;

        setTimeout(() => {
          toast.classList.remove("show");
        }, 4000);
      }

      function showLoadingScreen() {
        const loadingScreen = document.getElementById("loadingScreen");
        if (loadingScreen) loadingScreen.classList.remove("hidden");
      }

      function hideLoadingScreen() {
        const loadingScreen = document.getElementById("loadingScreen");
        if (loadingScreen) {
          setTimeout(() => {
            loadingScreen.classList.add("hidden");
          }, 1000);
        }
      }

      // =====================================================
      // MUSIC PLAYER INTEGRATION
      // =====================================================
      function initMusicPlayer() {
        const btnText = document.getElementById("music-button-text");
        const icon = document.getElementById("music-icon");
        const dropdown = document.getElementById("music-dropdown-content");

        if (!btnText || !icon || !dropdown) return;

        function setUI(state) {
          btnText.textContent =
            state === "Playing" ? "PAUSE MUSIC" : "STUDY MUSIC";
          icon.className =
            state === "Playing"
              ? "fa-solid fa-circle-pause"
              : "fa-solid fa-music";
        }

        // Load saved music state
        const savedSrc = localStorage.getItem("cybersec_music_src");
        const savedTime = parseFloat(
          localStorage.getItem("cybersec_music_time") || 0
        );

        if (savedSrc) {
          audioPlayer.src = savedSrc;
          audioPlayer.currentTime = savedTime;
          audioPlayer
            .play()
            .then(() => setUI("Playing"))
            .catch(() => setUI("Stopped"));
        }

        // Music button click handler
        document
          .getElementById("music-dropdown")
          .querySelector("button")
          .addEventListener("click", (e) => {
            e.stopPropagation();
            dropdown.style.display =
              dropdown.style.display === "block" ? "none" : "block";

            if (audioPlayer.src && !audioPlayer.paused) {
              audioPlayer.pause();
              setUI("Stopped");
            } else if (audioPlayer.src) {
              audioPlayer.play().then(() => setUI("Playing"));
            }
          });

        // Music selection handlers
        dropdown.querySelectorAll("a[data-src]").forEach((link) => {
          link.addEventListener("click", (e) => {
            e.preventDefault();
            audioPlayer.src = link.dataset.src;
            audioPlayer.play().then(() => setUI("Playing"));
            localStorage.setItem("cybersec_music_src", link.dataset.src);
            dropdown.style.display = "none";
          });
        });

        // Stop music handler
        const stopLink = document.getElementById("stop-music-link");
        if (stopLink) {
          stopLink.addEventListener("click", (e) => {
            e.preventDefault();
            audioPlayer.pause();
            audioPlayer.currentTime = 0;
            audioPlayer.removeAttribute("src");
            setUI("Stopped");
            localStorage.removeItem("cybersec_music_src");
            localStorage.removeItem("cybersec_music_time");
            dropdown.style.display = "none";
          });
        }

        // Save music state before page unload
        window.addEventListener("beforeunload", () => {
          if (!audioPlayer.paused && audioPlayer.src) {
            localStorage.setItem("cybersec_music_src", audioPlayer.src);
            localStorage.setItem(
              "cybersec_music_time",
              audioPlayer.currentTime
            );
          }
        });

        // Close dropdown when clicking outside
        window.addEventListener("click", (e) => {
          if (!e.target.closest("#music-dropdown")) {
            dropdown.style.display = "none";
          }
        });
      }

      // Show initial music indicator
      function addMusicIndicator() {
        const musicBtn = document.querySelector("#music-dropdown");
        if (musicBtn) {
          const indicator = document.createElement("div");
          indicator.className = "music-indicator";
          indicator.innerHTML = `<i class="fa-solid fa-music"></i> Try Study Music!`;

          musicBtn.style.position = "relative";
          musicBtn.appendChild(indicator);

          setTimeout(() => indicator.remove(), 3000);
        }
      }

      // Initialize Event Listeners
      function initializeEventListeners() {
        // Navigation buttons
        document
          .getElementById("prevLessonBtn")
          ?.addEventListener("click", goToPreviousLesson);
        document
          .getElementById("nextLessonBtn")
          ?.addEventListener("click", goToNextLesson);

        // Reset progress button
        document
          .getElementById("resetProgressBtn")
          ?.addEventListener("click", resetProgress);

        // Mobile menu toggle
        document
          .getElementById("menuToggle")
          ?.addEventListener("click", toggleSidebar);
        document
          .getElementById("sidebarOverlay")
          ?.addEventListener("click", closeSidebar);

        // Keyboard shortcuts
        document.addEventListener("keydown", handleKeyboardShortcuts);

        // Window resize handler
        window.addEventListener("resize", handleWindowResize);

        // Content interaction tracking
        document.getElementById("contentBody")?.addEventListener(
          "scroll",
          debounce(() => {
            trackUserInteraction();
          }, 1000)
        );
      }

      // Keyboard Shortcuts
      function handleKeyboardShortcuts(e) {
        if (document.querySelector(".quiz-question.active")) return;

        if (e.key === "ArrowLeft" && e.ctrlKey) {
          e.preventDefault();
          goToPreviousLesson();
        } else if (e.key === "ArrowRight" && e.ctrlKey) {
          e.preventDefault();
          goToNextLesson();
        }
      }

      // Window Resize Handler
      function handleWindowResize() {
        if (window.innerWidth > 768) {
          closeSidebar();
        }
      }

      // Debounce Helper
      function debounce(func, wait) {
        let timeout;
        return function executedFunction(...args) {
          const later = () => {
            clearTimeout(timeout);
            func(...args);
          };
          clearTimeout(timeout);
          timeout = setTimeout(later, wait);
        };
      }

      // Initialize responsive behavior
      if (window.innerWidth <= 768) {
        document.getElementById("menuToggle").style.display = "inline-flex";
      }

      // Auto-save progress periodically
      setInterval(async () => {
        if (currentUser && Object.keys(courseProgress).length > 0) {
          await saveProgress();
        }
      }, 60000); // Save every minute

      // System initialization logging
      console.log("CyberSec Academy Course System Initialized");
      console.log("Current Course:", COURSE_DATA.title);
      console.log("Total Lessons:", COURSE_DATA.lessons.length);

      // Google OAuth Integration
      const googleBtn = document.querySelector(".btn-google");
      if (googleBtn) {
        googleBtn.addEventListener("click", async () => {
          const { data, error } = await supabase.auth.signInWithOAuth({
            provider: "google",
            options: {
              redirectTo: window.location.origin + "/courses/post-quantum-cryptography.html",
            },
          });

          if (error) {
            console.error("Google login error:", error.message);
            showToast("Google login failed: " + error.message, "error");
          }
        });
      }

      // Check for existing session
      supabase.auth.getSession().then(({ data }) => {
        if (data.session) {
          console.log("User already logged in:", data.session.user);
          currentUser = data.session.user;
          startCourseSession();
        }
      });

      // Add missing auth modal functions
      function openAuthModal() {
        const modal = document.getElementById("authModal");
        if (modal) {
          modal.style.display = "flex";
        }
      }

      function closeAuthModal() {
        const modal = document.getElementById("authModal");
        if (modal) {
          modal.style.display = "none";
        }
      }

      // Add auth tab switching functionality
      document.getElementById("tabSignIn")?.addEventListener("click", () => {
        document.getElementById("tabSignIn").classList.add("active");
        document.getElementById("tabSignUp").classList.remove("active");
        document.getElementById("signInForm").style.display = "block";
        document.getElementById("signUpForm").style.display = "none";
      });

      document.getElementById("tabSignUp")?.addEventListener("click", () => {
        document.getElementById("tabSignUp").classList.add("active");
        document.getElementById("tabSignIn").classList.remove("active");
        document.getElementById("signUpForm").style.display = "block";
        document.getElementById("signInForm").style.display = "none";
      });

      document
        .getElementById("closeAuthModal")
        ?.addEventListener("click", closeAuthModal);

      // Improved error handling for auth session
      supabase.auth.onAuthStateChange((event, session) => {
        if (event === "SIGNED_IN") {
          currentUser = session.user;
          closeAuthModal();
          startCourseSession();
        } else if (event === "SIGNED_OUT") {
          currentUser = null;
          openAuthModal();
        }
      });

      // Add form submission handlers
      document
        .getElementById("emailSignInForm")
        ?.addEventListener("submit", async (e) => {
          e.preventDefault();
          const email = document.getElementById("signInEmail").value;
          const password = document.getElementById("signInPassword").value;

          try {
            const { data, error } = await supabase.auth.signInWithPassword({
              email,
              password,
            });

            if (error) throw error;

            closeAuthModal();
          } catch (error) {
            showToast(error.message, "error");
          }
        });

      document
        .getElementById("emailSignUpForm")
        ?.addEventListener("submit", async (e) => {
          e.preventDefault();
          const email = document.getElementById("signUpEmail").value;
          const password = document.getElementById("signUpPassword").value;
          const name = document.getElementById("signUpName").value;

          try {
            const { data, error } = await supabase.auth.signUp({
              email,
              password,
              options: {
                data: {
                  full_name: name,
                },
              },
            });

            if (error) throw error;

            showToast(
              "Account created successfully! Please check your email.",
              "success"
            );
          } catch (error) {
            showToast(error.message, "error");
          }
        });
    </script>
  </body>
</html>

