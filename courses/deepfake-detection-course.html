


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- ========== Start: SEO & Schema Enhancement ========== -->
    <title>Deepfake Detection Course | CipherHall</title>
    <meta name="description" content="Enroll in our expert-led Deepfake Detection course. Learn to analyze and detect synthetic media using cutting-edge AI and forensic techniques." />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon.png" />
    <link rel="shortcut icon" href="/favicon.png" type="image/x-icon" />
    <link rel="apple-touch-icon" sizes="180x180" href="/favicon.png" />
    <link rel="canonical" href="https://www.cipherhall.com/courses/deepfake-detection-course" />

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Course",
      "name": "Deepfake Detection: A Comprehensive Guide",
      "description": "A 30-lesson roadmap on the fundamentals of deepfake detection, from core concepts and visual artifacts to advanced multimodal techniques and real-world challenges.",
      "provider": {
        "@type": "Organization",
        "name": "CipherHall",
        "sameAs": "https://www.cipherhall.com"
      },
      "hasCourseInstance": {
        "@type": "CourseInstance",
        "courseMode": "Online",
        "instructor": {
          "@type": "Person",
          "name": "Dr. Sofia Rossi"
        }
      }
    }
    </script>
    <!-- ========== End: SEO & Schema Enhancement ========== -->
    
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&family=Exo+2:wght@700;800;900&display=swap"
      rel="stylesheet"
    />
    <script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js@2.39.7/dist/umd/supabase.min.js"></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-okaidia.min.css"
    />
    <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />

    <link rel="stylesheet" href="assets/css/coursepages.css" />
</head>
  <body>
    <!-- Loading Screen -->
    <div id="loadingScreen" class="loading-screen">
      <div class="loader-icon">
        <i class="fas fa-graduation-cap"></i>
      </div>
      <div class="loader-text">Loading Course Content...</div>
    </div>

    <!-- Sidebar Overlay for Mobile -->
    <div class="sidebar-overlay" id="sidebarOverlay"></div>

    <!-- Music Player (fixed at top right) -->
    <div class="header-controls">
      <div class="music-dropdown" id="music-dropdown">
        <button class="btn btn-secondary">
          <span id="music-button-text">STUDY MUSIC</span>
          <i id="music-icon" class="fa-solid fa-music"></i>
        </button>
        <div class="music-dropdown-content" id="music-dropdown-content">
          <a
            href="#"
            data-src="https://www.learningcontainer.com/wp-content/uploads/2020/02/Kalimba.mp3"
            >1. Coffee Shop Vibes</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-16.mp3"
            >2. City Lights Lofi</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-15.mp3"
            >3. Mellow Thoughts</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-13.mp3"
            >4. Rainy Mood</a
          >
          <a
            href="#"
            data-src="https://www.soundhelix.com/examples/mp3/SoundHelix-Song-10.mp3"
            >5. Time Alone</a
          >
          <a href="#" id="stop-music-link"
            ><i class="fa-solid fa-stop-circle"></i> Stop Music</a
          >
        </div>
      </div>

      <button class="btn btn-secondary" id="resetProgressBtn">
        <i class="fas fa-redo"></i>
        Reset Progress
      </button>
    </div>

    <!-- Auth Modal -->
    <div id="authModal" class="modal" style="display: none">
      <div class="modal-overlay"></div>
      <div class="modal-content">
        <span class="close" id="closeAuthModal">&times;</span>
        <div class="auth-tabs">
          <button id="tabSignIn" class="auth-tab active">Sign In</button>
          <button id="tabSignUp" class="auth-tab">Sign Up</button>
        </div>
        <div
          id="authLoader"
          style="display: none; text-align: center; padding: 2rem"
        >
          <i
            class="fas fa-spinner fa-spin fa-2x"
            style="color: var(--color-green)"
          ></i>
          <p style="margin-top: 0.5rem">Authenticating...</p>
        </div>
        <div id="signInForm" class="auth-form">
          <h2>Sign In to CipherHall</h2>
          <button id="googleSignIn" class="btn btn-google">
            <i class="fab fa-google"></i> Continue with Google
          </button>
          <div class="divider"><span>or</span></div>
          <form id="emailSignInForm">
            <input
              type="email"
              id="signInEmail"
              placeholder="Email Address"
              required
            />
            <input
              type="password"
              id="signInPassword"
              placeholder="Password"
              required
            />
            <button type="submit" class="btn btn-primary">Sign In</button>
          </form>
        </div>
        <div id="signUpForm" class="auth-form" style="display: none">
          <h2>Join CipherHall</h2>
          <button id="googleSignUp" class="btn btn-google">
            <i class="fab fa-google"></i> Continue with Google
          </button>
          <div class="divider"><span>or</span></div>
          <form id="emailSignUpForm">
            <input
              type="text"
              id="signUpName"
              placeholder="Full Name"
              required
            />
            <input
              type="email"
              id="signUpEmail"
              placeholder="Email Address"
              required
            />
            <input
              type="password"
              id="signUpPassword"
              placeholder="Password (min. 8 characters)"
              required
            />
            <button type="submit" class="btn btn-secondary">
              Create Account
            </button>
          </form>
        </div>
        <div id="authMessage"></div>
      </div>
    </div>

    <!-- Sidebar -->
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-header">
        <div class="course-info">
          <h1 class="course-title" id="courseTitle">Loading...</h1>
          <div class="course-progress">
            <div class="progress-header">
              <span class="progress-label">Course Progress</span>
              <span class="progress-percentage" id="courseProgressPercent"
                >0%</span
              >
            </div>
            <div class="progress-bar">
              <div
                class="progress-fill"
                id="courseProgressFill"
                style="width: 0%"
              ></div>
            </div>
            <div class="progress-stats">
              <span id="completedLessons">0</span>
              <span id="totalLessons">0</span>
            </div>
          </div>
          <a href="/dashboard" class="back-to-dashboard">
            <i class="fas fa-arrow-left"></i>
            Back to Dashboard
          </a>
        </div>
      </div>

      <nav class="lesson-nav" id="lessonNav">
        <!-- Lessons will be loaded dynamically -->
      </nav>
    </aside>

    <!-- Main Content -->
    <main class="main-content">
      <header class="content-header">
        <div class="lesson-header">
          <div class="lesson-header-left">
            <span class="lesson-number" id="currentLessonNumber">1</span>
            <h1 class="lesson-header-title" id="currentLessonTitle">
              Loading...
            </h1>
          </div>
          <div class="lesson-actions">
            <button class="mbtn btn-primary" id="menuToggle">
              <i class="fas fa-bars"></i>
            </button>
          </div>
        </div>
      </header>

      <div class="content-body" id="contentBody">
        <div class="lesson-content" id="lessonContent">
          <!-- Lesson content will be loaded dynamically -->
        </div>
      </div>

      <footer class="lesson-navigation">
        <div class="nav-info" id="navInfo">Lesson 1 of 10</div>
        <div class="nav-controls">
          <button class="btn btn-secondary" id="prevLessonBtn" disabled>
            <i class="fas fa-chevron-left"></i>
            Previous
          </button>
          <button class="btn btn-primary" id="nextLessonBtn" disabled>
            Next
            <i class="fas fa-chevron-right"></i>
          </button>
        </div>
      </footer>
    </main>

    <!-- Achievement Notification -->
    <div id="achievementNotification" class="achievement-notification">
      <div class="notification-header">
        <div class="notification-icon" id="notificationIcon">
          <i class="fas fa-trophy"></i>
        </div>
        <div class="notification-content">
          <h3 id="notificationTitle">Achievement Unlocked!</h3>
          <p id="notificationDescription">Description here...</p>
        </div>
      </div>
      <div class="notification-reward" id="notificationReward">
        <i class="fas fa-coins"></i>
        <span id="notificationPoints">+100 Points</span>
      </div>
    </div>

    <!-- Toast Notification -->
    <div id="toast" class="toast">
      <span id="toastMessage"></span>
    </div>

    <script>
      // =====================================================
      // SYNCHRONIZED COURSE SYSTEM WITH DASHBOARD INTEGRATION
      // =====================================================

      // Supabase Configuration - Replace with your config
      const supabaseUrl = "https://lzcmzulemfubjaksoqor.supabase.co";
      const supabaseKey =
        "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imx6Y216dWxlbWZ1Ympha3NvcW9yIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTUzMzM5MDgsImV4cCI6MjA3MDkwOTkwOH0.crhPH4YWtRsr1BJTpAQcmPbWSpwIWRbzoI4sRb2_fPI";
      const supabase = window.supabase.createClient(supabaseUrl, supabaseKey);

      // =====================================================
      // COURSE DATA STRUCTURE - REPLACE WITH YOUR COURSE JSON
      // =====================================================
      const COURSE_DATA =

{
    "id": "deepfake-detection",
    "title": "Deepfake Detection: A Comprehensive Guide",
    "description": "A 30-lesson roadmap on the fundamentals of deepfake detection, from core concepts and visual artifacts to advanced multimodal techniques and real-world challenges.",
    "category": "ai-security-defensive",
    "difficulty": "Intermediate",
    "duration": "45 hours",
    "instructor": "Dr. Sofia Rossi",
    "lessons": [
        {
            "id": "lesson-1",
            "title": "Introduction to Synthetic Media",
            "duration": "60 min",
            "objectives": [
                "Define and differentiate between deepfakes, shallowfakes, and cheapfakes",
                "Understand the core generative technologies: GANs and Autoencoders [2, 3]",
                "Explore the history and rapid evolution of deepfake technology [4]",
                "Analyze the societal impact of realistic synthetic media"
            ],
            "content": {
                "overview": "This lesson introduces the world of synthetic media, establishing a clear definition for the term 'deepfake' and the underlying AI technologies that make it possible. We will trace the evolution of this technology from academic research to a widespread phenomenon and discuss its broader implications.",
                "sections": [
                    {
                        "title": "Defining Deepfakes, Shallowfakes, and Cheapfakes",
                        "content": "<p>Not all manipulated media is a deepfake. It's important to understand the distinctions:</p><ul><li><strong>Deepfake:</strong> A term derived from 'deep learning' and 'fake'. It refers to synthetic media that has been created or manipulated using sophisticated deep learning techniques, such as Generative Adversarial Networks (GANs), to create highly realistic forgeries.</li><li><strong>Shallowfake:</strong> Media that has been manipulated using simpler, more conventional video editing techniques, such as speeding up, slowing down, or splicing clips out of context.</li><li><strong>Cheapfake:</strong> A broader term that often encompasses shallowfakes and other low-tech methods of media manipulation. [1]</li></ul><p>This course will focus primarily on detecting true deepfakes generated by AI.</p>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a_3d_3?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Core Technologies: GANs and Autoencoders",
                        "content": "<p>Modern deepfakes are primarily created using two types of deep neural network architectures:</p><ul><li><strong>Generative Adversarial Networks (GANs):</strong> A GAN consists of two competing networks: a <strong>Generator</strong> that tries to create realistic fake images, and a <strong>Discriminator</strong> that tries to distinguish the fakes from real images. Through this adversarial process, the generator becomes incredibly good at producing photorealistic fakes. [2, 3]</li><li><strong>Autoencoders:</strong> An autoencoder is trained to compress an image into a low-dimensional representation (the 'latent space') and then reconstruct it. For face-swapping, two autoencoders are trained: one on the source face and one on the target face, often with a shared encoder. To perform the swap, the encoder processes the source face, but the decoder of the target face is used for reconstruction.</li></ul>",
                        "image": "https://i.imgur.com/3Z4h7k2.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Exploring_a_Simple_GAN_Architecture.ipynb",
                        "language": "python",
                        "code": "import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Reshape, Flatten\nfrom tensorflow.keras.models import Sequential\n\n# --- Generator Model ---\n# Takes a random noise vector and outputs an image.\ndef build_generator():\n    model = Sequential([\n        Dense(128, activation='relu', input_shape=(100,)),\n        Dense(784, activation='sigmoid'), # 784 = 28*28 pixels\n        Reshape((28, 28))\n    ])\n    return model\n\n# --- Discriminator Model ---\n# Takes an image and classifies it as 'real' or 'fake'.\ndef build_discriminator():\n    model = Sequential([\n        Flatten(input_shape=(28, 28)),\n        Dense(128, activation='relu'),\n        Dense(1, activation='sigmoid')\n    ])\n    return model\n\n# The GAN combines these two, training them in an adversarial loop."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the key characteristic of a 'deepfake'?",
                        "options": [
                            "It is always a video.",
                            "It is created using simple video editing software.",
                            "It is synthetic media generated using deep learning techniques like GANs. [2, 3]",
                            "It is easy to detect with the naked eye."
                        ],
                        "correct": 2,
                        "explanation": "The term 'deepfake' specifically refers to the use of deep learning models to create or manipulate media, which is what allows for such high realism."
                    },
                    {
                        "id": 2,
                        "question": "In a GAN, what is the role of the 'Discriminator'?",
                        "options": [
                            "To create fake images.",
                            "To act as a judge, trying to distinguish between real and fake images. [2]",
                            "To compress the images.",
                            "To add noise to the images."
                        ],
                        "correct": 1,
                        "explanation": "The discriminator's goal is to get better at spotting fakes, which in turn forces the generator to get better at creating them. This adversarial competition is what drives the learning process in a GAN."
                    },
                    {
                        "id": 3,
                        "question": "Editing a video to make a politician appear to say something they didn't by splicing clips together is an example of what?",
                        "options": [
                            "A deepfake",
                            "A shallowfake or cheapfake [1]",
                            "A GAN attack",
                            "An autoencoder"
                        ],
                        "correct": 1,
                        "explanation": "This is a form of manipulation that uses traditional video editing techniques rather than deep learning, so it falls under the category of a shallowfake."
                    }
                ]
            }
        },
        {
            "id": "lesson-2",
            "title": "The Deepfake Threat Landscape",
            "duration": "75 min",
            "objectives": [
                "Identify the primary malicious applications of deepfakes [2, 3, 5]",
                "Understand the top organizational risks, including financial fraud and reputation damage [2]",
                "Analyze real-world case studies of deepfake attacks",
                "Develop a threat model for a corporate system vulnerable to deepfakes"
            ],
            "content": {
                "overview": "Deepfakes are not just a technical curiosity; they are a powerful tool that can be used for a wide range of malicious purposes. This lesson explores the real-world threat landscape, from large-scale misinformation campaigns to highly targeted corporate fraud, and analyzes the specific risks that organizations face.",
                "sections": [
                    {
                        "title": "Malicious Applications",
                        "content": "<p>The potential for misuse of deepfake technology is vast and cuts across social, political, and financial domains.</p><h3>Key Threat Categories:</h3><ul><li><strong>Disinformation/Misinformation:</strong> Creating fake videos of political leaders to influence elections or incite unrest.</li><li><strong>Financial Fraud:</strong> Impersonating a CEO in a video call or a voicemail to authorize fraudulent wire transfers (a form of 'vishing' - voice phishing). [2]</li><li><strong>Reputation Damage:</strong> Creating fake, compromising videos of an individual to damage their personal or professional reputation.</li><li><strong>Social Engineering:</strong> Using a fake social media profile with a generated face to build trust with a target for espionage or hacking. [5]</li></ul>",
                        "image": "https://images.unsplash.com/photo-1588196749107-15d08b4be52a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Top Organizational Risks",
                        "content": "<p>For a business or other organization, the threats posed by deepfakes are concrete and immediate.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>Corporate Threats</strong></div><p><ul><li><strong>CEO Fraud:</strong> The most cited risk. An attacker uses a voice clone of the CEO to call an employee in the finance department and urgently request a large wire transfer to a fraudulent account. [2]</li><li><strong>Stock Market Manipulation:</strong> A fake video of a CEO announcing a product recall or a financial disaster could be released to crash a company's stock price.</li><li><strong>Erosion of Trust:</strong> In a world where any audio or video could be fake, the trust in digital communication itself is eroded, which can slow down business processes that require verification.</li></ul></p></div>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Threat_Model_for_a_Corporate_Video_Conferencing_System.md",
                        "language": "markdown",
                        "code": "# Threat Model: Corporate Video Conferencing\n\n**Asset:** Trust in executive communication.\n\n**Threat Agent:** External fraudster.\n\n**Attack Surface:** Live video calls, voicemail system.\n\n**Attack Scenarios:**\n1.  **Live Attack:** Attacker joins a video call impersonating the CFO using a real-time deepfake, and directs an employee to approve a fraudulent transaction.\n    -   **Impact:** Direct financial loss.\n2.  **Offline Attack:** Attacker uses a voice clone to leave a voicemail for a junior employee, pressuring them to provide their login credentials.\n    -   **Impact:** Account compromise, initial access for a larger hack.\n\n**Potential Mitigations:**\n-   Implement a multi-factor liveness detection system for video calls.\n-   Establish a strict out-of-band (e.g., text message) verification protocol for any financial transaction requested via voice or video."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "An attacker using a voice clone of a CEO to authorize a fraudulent wire transfer is an example of what?",
                        "options": [
                            "A disinformation campaign.",
                            "CEO fraud, a form of financial fraud. [2]",
                            "Reputation damage.",
                            "A shallowfake."
                        ],
                        "correct": 1,
                        "explanation": "This is a direct and highly plausible attack vector that poses a significant financial risk to organizations."
                    },
                    {
                        "id": 2,
                        "question": "Creating a fake video of a political candidate saying something inflammatory right before an election is an example of which threat category?",
                        "options": [
                            "Financial Fraud",
                            "Disinformation and Misinformation [5]",
                            "Social Engineering",
                            "A cheapfake"
                        ],
                        "correct": 1,
                        "explanation": "This use case aims to manipulate public opinion and interfere with democratic processes, a major societal-level threat from deepfake technology."
                    },
                    {
                        "id": 3,
                        "question": "The use of deepfakes for 'vishing' refers to what?",
                        "options": [
                            "Visual phishing with fake images.",
                            "Voice phishing using a cloned voice. [2]",
                            "Video-based phishing.",
                            "A type of spam."
                        ],
                        "correct": 1,
                        "explanation": "'Vishing' is a portmanteau of 'voice' and 'phishing'. Deepfake audio makes these attacks much more convincing than a simple phone call."
                    }
                ]
            }
        },
        {
            "id": "lesson-3",
            "title": "Fundamentals of Digital Media Forensics",
            "duration": "60 min",
            "objectives": [
                "Get an introduction to the principles of multimedia forensics [1]",
                "Understand the concept of digital provenance and its importance",
                "Learn how to perform basic metadata analysis on media files",
                "Differentiate between deepfake detection and traditional forgery detection"
            ],
            "content": {
                "overview": "Deepfake detection is a specialized subfield of a broader discipline: digital media forensics. This lesson introduces the foundational concepts of forensics, such as analyzing metadata and establishing provenance, which provide the context for the more advanced AI-based detection techniques we will cover later.",
                "sections": [
                    {
                        "title": "Multimedia Forensics",
                        "content": "<p><strong>Multimedia forensics</strong> is the science of analyzing digital media (images, video, audio) to determine its authenticity, origin, and history. [1] It aims to answer questions like:</p><ul><li>Is this image original or has it been tampered with?</li><li>What camera was used to take this photo?</li><li>Has this video been edited or altered?</li></ul><p>Traditional forensic techniques look for signs of editing in software like Photoshop, such as inconsistencies in compression artifacts or lighting. Deepfake detection uses AI to look for the specific, subtle artifacts left behind by generative models.</p>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Digital Provenance and Metadata Analysis",
                        "content": "<p><strong>Digital provenance</strong> refers to the history of a piece of digital content—its origin and the chain of custody. Establishing provenance is a key goal of forensics.</p><p>The first step in any forensic analysis is to examine the file's <strong>metadata</strong>. Metadata is the 'data about the data' that is embedded in the file. For an image, this is often stored in the EXIF (Exchangeable Image File Format) header and can include:<ul><li>The camera make and model.</li><li>The date and time the photo was taken.</li><li>GPS coordinates.</li><li>Thumbnail images.</li></ul><p>The absence of metadata, or metadata that is inconsistent with the content of the image, can be a strong indicator of manipulation.</p>",
                        "image": "https://i.imgur.com/u7y7o9o.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Metadata_Extraction_and_Analysis_with_ExifTool.py",
                        "language": "python",
                        "code": "import subprocess\nimport json\n\n# ExifTool is a powerful command-line tool for metadata analysis.\n# This script shows how to call it from Python.\n\ndef get_metadata(filepath):\n    try:\n        # Run ExifTool and get the output as JSON\n        process = subprocess.run(\n            ['exiftool', '-json', filepath],\n            capture_output=True, text=True, check=True\n        )\n        # The output is a list of JSON objects\n        metadata = json.loads(process.stdout)[0]\n        return metadata\n    except Exception as e:\n        print(f\"Error processing file: {e}\")\n        return None\n\n# Analyze an image\nimage_file = 'example.jpg'\nmetadata = get_metadata(image_file)\n\nif metadata:\n    print(f\"Camera Model: {metadata.get('Model', 'N/A')}\")\n    print(f\"Date/Time Original: {metadata.get('DateTimeOriginal', 'N/A')}\")\n    print(f\"Software: {metadata.get('Software', 'N/A')}\") # Can indicate Photoshop"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is multimedia forensics?",
                        "options": [
                            "The process of creating deepfakes.",
                            "The science of analyzing digital media to determine its authenticity and history. [1]",
                            "A type of software.",
                            "The study of social media."
                        ],
                        "correct": 1,
                        "explanation": "Multimedia forensics is the broader field that encompasses deepfake detection, focusing on all forms of digital media manipulation."
                    },
                    {
                        "id": 2,
                        "question": "What is 'digital provenance'?",
                        "options": [
                            "The file size of a piece of media.",
                            "The visual quality of a video.",
                            "The history of a piece of digital content, including its origin and chain of custody.",
                            "A type of deepfake."
                        ],
                        "correct": 2,
                        "explanation": "Establishing provenance is a key goal of forensics. It helps to verify that a piece of media is what it claims to be and has not been tampered with since its creation."
                    },
                    {
                        "id": 3,
                        "question": "The EXIF data in an image file typically contains what kind of information?",
                        "options": [
                            "The full pixel data of the image.",
                            "A text description of the image content.",
                            "Metadata such as the camera model, date, and time. ",
                            "The audio track for the image."
                        ],
                        "correct": 2,
                        "explanation": "EXIF is a standard for storing metadata directly in image files. Analyzing this data for inconsistencies is a fundamental step in forensic analysis."
                    }
                ]
            }
        },
        {
            "id": "lesson-4",
            "title": "Building a Detection Environment",
            "duration": "60 min",
            "objectives": [
                "Get an overview of key Python libraries for deepfake detection (OpenCV, TensorFlow, PyTorch) [6]",
                "Set up a local development environment",
                "Understand the role of Convolutional Neural Networks (CNNs) in detection [4, 7]",
                "Build a simple 'warm-up' image classifier"
            ],
            "content": {
                "overview": "Before we can start detecting deepfakes, we need to set up our workshop. This lesson covers the essential software tools and libraries that are the foundation of modern computer vision and deepfake detection, and guides you through setting up a functional development environment.",
                "sections": [
                    {
                        "title": "Key Tools and Libraries",
                        "content": "<p>Our work will be done in Python, using a set of powerful, open-source libraries:</p><ul><li><strong>TensorFlow & PyTorch:</strong> These are the two leading deep learning frameworks. They provide the tools to build, train, and evaluate complex neural networks. [6]</li><li><strong>OpenCV (Open Source Computer Vision Library):</strong> The de facto standard library for all things computer vision. We will use it for basic image and video processing tasks, such as reading video files, extracting frames, and detecting faces.</li><li><strong>Scikit-learn:</strong> A foundational machine learning library for tasks like data splitting and performance evaluation.</li><li><strong>Jupyter Notebooks:</strong> An interactive environment that is perfect for experimenting with data and models.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Role of CNNs in Detection",
                        "content": "<p>The workhorse of most deepfake detection models is the <strong>Convolutional Neural Network (CNN)</strong>. A CNN is a type of deep learning model that is specifically designed to find patterns in grid-like data, such as images. [4, 7]</p><p>It works by using a series of 'filters' that slide across the image to detect low-level features like edges and textures. Subsequent layers combine these features to detect higher-level features like eyes, noses, or the specific, subtle artifacts that are the tell-tale signs of a deepfake. We will start by building simple CNNs and move to more advanced, state-of-the-art architectures as the course progresses.</p>",
                        "image": "https://i.imgur.com/k2H1z5F.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Setting_Up_PyTorch_and_OpenCV_for_Image_Analysis.sh",
                        "language": "bash",
                        "code": "# It is highly recommended to use a virtual environment\npython3 -m venv deepfake_env\nsource deepfake_env/bin/activate\n\n# Install the core libraries using pip\npip install torch torchvision # PyTorch\npip install tensorflow # TensorFlow\npip install opencv-python\npip install scikit-learn\npip install jupyter"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Which two are the leading deep learning frameworks used for building neural networks?",
                        "options": [
                            "OpenCV and Scikit-learn",
                            "TensorFlow and PyTorch [6]",
                            "Jupyter and Python",
                            "ExifTool and FFmpeg"
                        ],
                        "correct": 1,
                        "explanation": "TensorFlow (developed by Google) and PyTorch (developed by Meta/Facebook) are the two dominant open-source frameworks for deep learning research and production."
                    },
                    {
                        "id": 2,
                        "question": "What is the primary purpose of the OpenCV library?",
                        "options": [
                            "To build neural networks.",
                            "To perform general-purpose computer vision tasks like reading images, detecting faces, and processing video.",
                            "To analyze audio files.",
                            "To create websites."
                        ],
                        "correct": 1,
                        "explanation": "OpenCV is a foundational computer vision library that provides a huge range of tools for interacting with and manipulating visual media, which we will use for data preprocessing."
                    },
                    {
                        "id": 3,
                        "question": "What type of neural network architecture is the standard choice for image analysis and deepfake detection?",
                        "options": [
                            "Recurrent Neural Networks (RNNs)",
                            "Convolutional Neural Networks (CNNs) [4, 7]",
                            "Simple Multi-Layer Perceptrons (MLPs)",
                            "Autoencoders"
                        ],
                        "correct": 1,
                        "explanation": "CNNs are specifically designed to learn spatial hierarchies of features from images, making them the most effective and widely used architecture for computer vision tasks, including deepfake detection."
                    }
                ]
            }
        },
        {
            "id": "lesson-5",
            "title": "Key Datasets and Benchmarks",
            "duration": "75 min",
            "objectives": [
                "Explore the essential public datasets for deepfake detection [6, 8, 9, 10]",
                "Understand the creation process and limitations of these datasets [8, 10]",
                "Learn about the Deepfake Detection Challenge (DFDC)",
                "Get an introduction to benchmarking platforms like DeepfakeBench [11, 12]"
            ],
            "content": {
                "overview": "High-quality, large-scale datasets are the lifeblood of deepfake detection research. This lesson provides an overview of the major public datasets that have been instrumental in advancing the field, discussing their contents, their limitations, and the benchmarks used to evaluate model performance.",
                "sections": [
                    {
                        "title": "Major Public Datasets",
                        "content": "<p>Several key datasets are used by researchers to train and test their detection models:</p><ul><li><strong>FaceForensics++ (FF++):</strong> One of the most widely used datasets. It contains thousands of videos manipulated with several different face-swapping and facial reenactment techniques. [9]</li><li><strong>Deepfake Detection Challenge (DFDC):</strong> A massive dataset released by Meta (Facebook) as part of a public competition. It features a large number of paid actors and a wide variety of deepfake techniques, making it very challenging. [8]</li><li><strong>Celeb-DF:</strong> A high-quality dataset featuring deepfaked videos of celebrities, designed to have fewer of the visual artifacts that make detection on older datasets too easy. [10]</li></ul>",
                        "image": "https://i.imgur.com/u7nL6Xk.png"
                    },
                    {
                        "title": "Dataset Challenges and Limitations",
                        "content": "<p>While essential, existing datasets have limitations that researchers must be aware of.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Generalization Problem</strong></div><p>A major issue is that models trained on one dataset (e.g., FF++) often perform poorly when tested on a different dataset (e.g., Celeb-DF). This is because the model may be 'overfitting' to the specific artifacts of the generation methods used in the first dataset. Creating detectors that can generalize to new, unseen deepfake techniques is the biggest challenge in the field. [8, 10]</p></div>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e_35f?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Loading_and_Preprocessing_FaceForensics++_Dataset.py",
                        "language": "python",
                        "code": "import os\nimport cv2\n\n# Conceptual code for loading a video from the FaceForensics++ dataset\ndef load_video_frames(video_path, num_frames=16):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    try:\n        while len(frames) < num_frames:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            # Convert color and resize\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame = cv2.resize(frame, (224, 224))\n            frames.append(frame)\n    finally:\n        cap.release()\n    return np.array(frames)\n\n# The dataset is typically organized by manipulation method\nff_path = '/path/to/faceforensicspp/manipulated_sequences/Deepfakes/'\nvideo_file = os.path.join(ff_path, '000_003.mp4')\n\n# Load frames for a single video\nvideo_frames = load_video_frames(video_file)\nprint(f\"Loaded {len(video_frames)} frames with shape {video_frames[0].shape}\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Which of the following is a major, widely used public dataset for deepfake detection research?",
                        "options": [
                            "ImageNet",
                            "FaceForensics++ [9]",
                            "MNIST",
                            "CIFAR-10"
                        ],
                        "correct": 1,
                        "explanation": "FaceForensics++ is one of the most foundational and commonly cited datasets in the field, providing a standardized benchmark for many early detection models."
                    },
                    {
                        "id": 2,
                        "question": "What is the 'generalization problem' in deepfake detection?",
                        "options": [
                            "Models are too general and cannot detect specific fakes.",
                            "Models trained on one dataset often fail to detect fakes from a different dataset created with new techniques. [8, 10]",
                            "The datasets are too small.",
                            "The models are too difficult to train."
                        ],
                        "correct": 1,
                        "explanation": "This is the core challenge. Models often learn the 'tells' of a specific forgery method, rather than a general concept of what is 'fake', so they don't generalize well to new methods."
                    },
                    {
                        "id": 3,
                        "question": "The Deepfake Detection Challenge (DFDC) was a large-scale competition and dataset released by which organization?",
                        "options": [
                            "Google",
                            "Meta (Facebook) [8]",
                            "Stanford University",
                            "The US Government"
                        ],
                        "correct": 1,
                        "explanation": "The DFDC was a major initiative by Meta to spur research and development in the deepfake detection community by providing a massive, high-quality dataset."
                    }
                ]
            }
        },
        {
            "id": "lesson-6",
            "title": "Detecting Spatial Inconsistencies",
            "duration": "75 min",
            "objectives": [
                "Understand how to analyze pixel-level artifacts and inconsistencies [5]",
                "Learn techniques for spotting unnatural facial features",
                "Explore the role of lighting and color mismatches as detection signals [10]",
                "Build a simple CNN to detect common deepfake artifacts"
            ],
            "content": {
                "overview": "This lesson begins our exploration of core detection techniques by focusing on spatial artifacts—the visual inconsistencies within a single image or frame that can betray a deepfake. We will look at the pixel-level clues left behind by the generation process and learn how to train a model to spot them.",
                "sections": [
                    {
                        "title": "Pixel-Level Artifacts",
                        "content": "<p>The process of generating and compositing a fake face onto a target video often leaves subtle traces at the pixel level.</p><h3>Common Artifacts:</h3><ul><li><strong>Unnatural Blurring:</strong> The boundary between the swapped face and the rest of the image is often unnaturally smooth or blurred to hide the seam.</li><li><strong>Compression Mismatches:</strong> The fake face, having been generated and re-compressed, might have different JPEG or MPEG compression artifacts than the original background video.</li><li><strong>Checkerboard Patterns:</strong> Some GAN architectures can leave faint, high-frequency checkerboard patterns in the generated image.</li></ul><p>A CNN can be trained to recognize these subtle textural differences. [5]</p>",
                        "image": "https://i.imgur.com/k2H1z5F.png"
                    },
                    {
                        "title": "Facial and Lighting Inconsistencies",
                        "content": "<p>Deepfake models, especially older ones, often struggle to perfectly replicate the complex details of a human face and its interaction with light.</p><ul><li><strong>Unnatural Details:</strong> Details like teeth, hair, and earrings can be difficult to generate correctly and may appear distorted or inconsistent.</li><li><strong>Lighting Mismatches:</strong> The lighting on the fake face (direction, color, shadows) might not match the lighting of the background environment. For example, the face might be lit from the left while a lamp in the background is on the right. [10]</li><li><strong>Inconsistent Reflections:</strong> The reflections in the subject's eyes or on their glasses might not accurately reflect the surrounding scene.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Pixel_Level_Inconsistency_Detection_with_CNNs.py",
                        "language": "python",
                        "code": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\n# A simple CNN architecture for deepfake detection\ndef build_simple_cnn(input_shape=(224, 224, 3)):\n    model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n        MaxPooling2D(pool_size=(2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(128, activation='relu'),\n        Dense(1, activation='sigmoid') # Output: 0 for real, 1 for fake\n    ])\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# This model would be trained on a dataset of real and fake image patches\n# to learn to differentiate the subtle textural artifacts."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "An unnaturally blurred seam between the face and the background is an example of what?",
                        "options": [
                            "A temporal inconsistency.",
                            "A spatial artifact. [5]",
                            "An audio artifact.",
                            "A metadata inconsistency."
                        ],
                        "correct": 1,
                        "explanation": "Spatial artifacts are inconsistencies that can be found by analyzing the pixels within a single frame or image."
                    },
                    {
                        "id": 2,
                        "question": "Which of the following would be a lighting inconsistency that could indicate a deepfake?",
                        "options": [
                            "The video is too dark.",
                            "The lighting on the fake face comes from a different direction than the lighting in the background scene. [10]",
                            "The video is too bright.",
                            "The subject is wearing glasses."
                        ],
                        "correct": 1,
                        "explanation": "Generative models often fail to perfectly match the complex physics of light and shadow, leading to mismatches that can be a powerful detection signal."
                    },
                    {
                        "id": 3,
                        "question": "What type of model is best suited for learning the subtle, low-level pixel and texture artifacts left by GANs?",
                        "options": [
                            "A decision tree.",
                            "A Convolutional Neural Network (CNN). [5]",
                            "A linear regression model.",
                            "A clustering algorithm."
                        ],
                        "correct": 1,
                        "explanation": "CNNs are specifically designed to learn hierarchical features from images, making them ideal for detecting the low-level textural patterns and compression artifacts that often betray a deepfake."
                    }
                ]
            }
        },
        {
            "id": "lesson-7",
            "title": "Detecting Temporal Inconsistencies",
            "duration": "75 min",
            "objectives": [
                "Understand how to analyze video frames for temporal anomalies",
                "Learn about detecting flickering and other inter-frame inconsistencies",
                "Explore temporal-based learning approaches for video analysis [8]",
                "Analyze how to use optical flow to detect unnatural movements"
            ],
            "content": {
                "overview": "Deepfakes often struggle with consistency over time. While a single generated frame might look perfect, the relationship between frames can be unnatural. This lesson focuses on temporal detection techniques, which analyze the motion and changes in a video sequence to find signs of manipulation.",
                "sections": [
                    {
                        "title": "Flickering and Inter-Frame Inconsistencies",
                        "content": "<p>One common temporal artifact is <strong>flickering</strong>. This occurs when a deepfake algorithm processes each frame independently, leading to subtle but unnatural variations in texture, lighting, or the shape of the face between consecutive frames. A human might perceive this as a slight 'jitter' or 'flicker' in the video.</p><p>A detection model can be trained to spot this by comparing a frame to the one immediately preceding it. Large, unexplained changes in certain regions of the face from one frame to the next can be a strong signal of a fake. [8]</p>",
                        "image": "https://i.imgur.com/kYq3Q6k.png"
                    },
                    {
                        "title": "Optical Flow Analysis",
                        "content": "<p><strong>Optical flow</strong> is the pattern of apparent motion of objects in a video. It's a way to calculate the motion of every pixel between two consecutive frames.</p><p>In a real video, the optical flow of a person's head and face should be smooth and consistent. In a deepfake, the flow might be inconsistent, especially at the boundary between the swapped face and the background. A model can be trained on the optical flow data, rather than the raw pixel data, to learn the patterns of natural vs. unnatural motion.</p>",
                        "image": "https://images.unsplash.com/photo-1587620962725-abab7fe55159?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Frame_by_Frame_Flicker_Detection_Script.py",
                        "language": "python",
                        "code": "import cv2\nimport numpy as np\n\n# Conceptual code to detect flicker by measuring frame-to-frame difference\ndef detect_flicker(video_path):\n    cap = cv2.VideoCapture(video_path)\n    ret, prev_frame = cap.read()\n    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n\n    frame_diffs = []\n    while True:\n        ret, current_frame = cap.read()\n        if not ret:\n            break\n        \n        current_frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)\n        \n        # Calculate the absolute difference between the current and previous frame\n        diff = cv2.absdiff(current_frame_gray, prev_frame_gray)\n        frame_diffs.append(np.mean(diff))\n        \n        prev_frame_gray = current_frame_gray\n\n    cap.release()\n    \n    # A high variance in the frame differences can indicate flickering\n    variance = np.var(frame_diffs)\n    print(f\"Variance of frame differences: {variance}\")\n    if variance > THRESHOLD:\n        print(\"Potential flicker detected.\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a 'temporal inconsistency' in the context of deepfake detection?",
                        "options": [
                            "An artifact within a single image.",
                            "An anomaly that can only be detected by analyzing the relationship and motion between multiple video frames. [8]",
                            "An error in the file's metadata.",
                            "A mismatch in the audio track."
                        ],
                        "correct": 1,
                        "explanation": "Temporal refers to time. These are inconsistencies in how the manipulated video evolves over time, such as unnatural movement or flickering."
                    },
                    {
                        "id": 2,
                        "question": "What is 'optical flow'?",
                        "options": [
                            "A measure of the video's brightness.",
                            "The pattern of apparent motion of pixels between consecutive frames.",
                            "A type of neural network.",
                            "A method for compressing video."
                        ],
                        "correct": 1,
                        "explanation": "Optical flow is a computer vision technique to quantify motion. Analyzing the optical flow can reveal unnatural or inconsistent motion patterns that are a hallmark of some deepfake generation methods."
                    },
                    {
                        "id": 3,
                        "question": "Flickering in a deepfake video is often caused by what?",
                        "options": [
                            "The camera being out of focus.",
                            "The deepfake algorithm processing each frame independently, leading to slight variations between them.",
                            "Poor lighting conditions.",
                            "The video being old."
                        ],
                        "correct": 1,
                        "explanation": "This lack of temporal consistency is a key weakness. The algorithm fails to ensure that the generated face is perfectly stable and consistent from one frame to the next, which a detection model can learn to spot."
                    }
                ]
            }
        },
        {
            "id": "lesson-8",
            "title": "Head and Face Analysis",
            "duration": "75 min",
            "objectives": [
                "Learn how to detect unnatural head poses and movements",
                "Analyze inconsistencies in facial landmark positions and orientations [10]",
                "Explore the use of 3D facial modeling for detection",
                "Implement facial landmark detection using a library like Dlib"
            ],
            "content": {
                "overview": "Deepfake generation often involves warping or transforming a source face to fit a target head. This process can introduce subtle but detectable inconsistencies in the geometry and dynamics of the head and face. This lesson explores how to analyze these structural and motion-based cues.",
                "sections": [
                    {
                        "title": "Facial Landmark Analysis",
                        "content": "<p><strong>Facial landmarks</strong> are a set of key points on a human face, such as the corners of the eyes, the tip of the nose, and the corners of the mouth. Specialized models can detect the precise 2D or 3D location of these landmarks in an image.</p><p>Deepfake detection can leverage this by analyzing the stability and geometric consistency of these landmarks over time. In a real video, the relative distances between landmarks on a person's face should remain largely consistent. In a deepfake, the warping process might introduce subtle 'jitter' or unnatural deformations in this facial structure. [10]</p>",
                        "image": "https://i.imgur.com/u7nL6Xk.png"
                    },
                    {
                        "title": "Head Pose Estimation",
                        "content": "<p><strong>Head pose estimation</strong> is the task of determining the 3D orientation of a person's head (their yaw, pitch, and roll) from a 2D image. This can be used as a powerful signal for detection.</p><p>A deepfake model might struggle to perfectly match the head pose of the source actor to the target actor, leading to a slight mismatch between the orientation of the synthesized face and the original head. Furthermore, an anomaly detection model can be trained on the sequence of head movements to learn what constitutes 'normal' human head motion, and flag any unnatural or robotic-looking movements that might be produced by a generative model.</p>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Facial_Landmark_Tracking_with_Dlib.py",
                        "language": "python",
                        "code": "import cv2\nimport dlib\n\n# Dlib is a popular library for face and landmark detection.\n# It requires a pre-trained model file.\n\n# Load the detector and predictor\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n\n# Load an image\nimage = cv2.imread('face.jpg')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Detect faces in the image\nfaces = detector(gray)\n\nfor face in faces:\n    # Get the landmarks/parts for the face in box d.\n    landmarks = predictor(gray, face)\n    \n    # Loop through the 68 landmarks and draw them on the face\n    for n in range(0, 68):\n        x = landmarks.part(n).x\n        y = landmarks.part(n).y\n        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n\n# Show the final image\ncv2.imshow(\"Landmarks\", image)\ncv2.waitKey(0)"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What are facial landmarks?",
                        "options": [
                            "Famous faces.",
                            "A set of key points on a face, like the corners of the eyes and mouth.",
                            "Marks or blemishes on a face.",
                            "The overall shape of a face."
                        ],
                        "correct": 1,
                        "explanation": "Facial landmarks provide a structured, geometric representation of the face, which is ideal for analyzing its shape and movement for signs of manipulation. [10]"
                    },
                    {
                        "id": 2,
                        "question": "How can head pose estimation be used to detect deepfakes?",
                        "options": [
                            "It cannot be used for detection.",
                            "By detecting unnatural, robotic-looking head movements or a mismatch between the pose of the synthesized face and the original head.",
                            "By identifying the person.",
                            "By measuring the size of the head."
                        ],
                        "correct": 1,
                        "explanation": "The dynamics of head motion are very complex. Generative models can struggle to replicate these movements perfectly, leading to detectable artifacts in the pose sequence."
                    },
                    {
                        "id": 3,
                        "question": "Analyzing the 'jitter' or instability of facial landmark positions over time is what kind of detection technique?",
                        "options": [
                            "A spatial technique.",
                            "A temporal technique.",
                            "A metadata analysis technique.",
                            "An audio analysis technique."
                        ],
                        "correct": 1,
                        "explanation": "This technique specifically looks at the consistency of the face's geometry *over time*, making it a temporal analysis method. It's designed to catch the frame-to-frame instabilities that are common in deepfakes."
                    }
                ]
            }
        },
        {
            "id": "lesson-9",
            "title": "Eye and Gaze Analysis",
            "duration": "75 min",
            "objectives": [
                "Understand why eyes are a common point of failure for deepfakes",
                "Learn how to analyze blinking patterns for unnatural behavior",
                "Explore the detection of unrealistic eye reflections (corneal glints)",
                "Analyze gaze direction for inconsistencies",
                "Implement a simple blink detection algorithm"
            ],
            "content": {
                "overview": "The human eye is an incredibly complex and expressive feature, and deepfake models often struggle to replicate it perfectly. This lesson focuses on the eyes as a rich source of forensic information, covering the detection of unnatural blinking, gaze, and reflections.",
                "sections": [
                    {
                        "title": "Blinking Patterns",
                        "content": "<p>Early deepfake models were often trained on datasets of still images, and as a result, the faces they generated didn't blink. This was a very simple and effective detection method.</p><p>Modern deepfakes do blink, but their blinking patterns can still be unnatural. Humans blink at a certain rate and with a certain duration. A deepfake might blink too often, too rarely, or for too long. An AI model can be trained to track the eyes in a video and analyze the temporal sequence of blinks to determine if it matches the statistical patterns of a real human.</p>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Eye Reflections (Corneal Glints)",
                        "content": "<p>The surface of the human eye (the cornea) is highly reflective. It acts like a distorted mirror, reflecting the light sources in the surrounding environment. In a real video of a person, the reflections in both of their eyes should be consistent with each other and with the visible lighting in the scene.</p><p>Deepfake models almost never generate geometrically correct or consistent reflections. A detection technique can involve analyzing the video to extract and compare the reflections from both eyes. If the reflections are inconsistent or don't match the scene, it's a strong sign of a fake.</p>",
                        "image": "https://i.imgur.com/u7y7o9o.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Blink_Detection_in_Video_Streams.py",
                        "language": "python",
                        "code": "from scipy.spatial import distance as dist\nimport cv2\nimport dlib\n\n# The Eye Aspect Ratio (EAR) is a simple formula based on landmark positions\ndef eye_aspect_ratio(eye_landmarks):\n    # Compute the euclidean distances between the vertical eye landmarks\n    A = dist.euclidean(eye_landmarks[1], eye_landmarks[5])\n    B = dist.euclidean(eye_landmarks[2], eye_landmarks[4])\n    # Compute the euclidean distance between the horizontal eye landmark\n    C = dist.euclidean(eye_landmarks[0], eye_landmarks[3])\n    # Compute the EAR\n    ear = (A + B) / (2.0 * C)\n    return ear\n\n# ... (setup dlib face and landmark detectors) ...\n\n# In the main video loop:\n# 1. Detect facial landmarks for the current frame.\n# 2. Extract the landmarks for the left and right eyes.\n# 3. Calculate the EAR for each eye.\n# 4. Average the two EAR scores.\n\n# If the EAR drops below a certain threshold for a number of consecutive frames,\n# we register it as a 'blink'."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Why was the lack of blinking a good detection signal for early deepfakes?",
                        "options": [
                            "Because humans never blink.",
                            "Because the models were often trained on datasets of still images where the eyes are open.",
                            "Because blinking is not important.",
                            "Because the models were trying to save energy."
                        ],
                        "correct": 1,
                        "explanation": "This is a classic example of a dataset artifact. The model simply didn't have enough examples of blinking faces in its training data, so it learned to generate faces with their eyes always open."
                    },
                    {
                        "id": 2,
                        "question": "Analyzing the reflections on the surface of the eye is a forensic technique because...",
                        "options": [
                            "It is easy to do.",
                            "Everyone has the same eye reflections.",
                            "Deepfake models often fail to generate reflections that are consistent between the two eyes and with the surrounding scene.",
                            "Reflections are not visible in real videos."
                        ],
                        "correct": 2,
                        "explanation": "The physics of light reflection are extremely complex. It is very difficult for a generative model to perfectly replicate these details, making it a powerful forensic clue."
                    },
                    {
                        "id": 3,
                        "question": "The Eye Aspect Ratio (EAR) is a metric used for what purpose?",
                        "options": [
                            "To measure the size of the eye.",
                            "To determine the color of the eye.",
                            "To detect blinks by measuring the ratio of the height to the width of the eye using facial landmarks.",
                            "To calculate the person's age."
                        ],
                        "correct": 2,
                        "explanation": "The EAR is a simple but effective heuristic. When the eye is open, the ratio is relatively constant. When it closes during a blink, the ratio drops rapidly towards zero. This makes it a great signal for automated blink detection."
                    }
                ]
            }
        },
        {
            "id": "lesson-10",
            "title": "Lip-Sync Analysis",
            "duration": "75 min",
            "objectives": [
                "Understand the technology behind lip-sync deepfakes [10]",
                "Learn how to detect discrepancies between audio and lip movements",
                "Explore the use of visemes for audio-visual correlation",
                "Analyze the challenges of real-time lip-sync detection"
            ],
            "content": {
                "overview": "Lip-sync deepfakes, where a target person's mouth is manipulated to match a new audio track, are a common and powerful form of disinformation. This lesson focuses on the detection of these fakes by analyzing the correlation (or lack thereof) between the visual evidence of the lip movements and the audio evidence of the spoken words.",
                "sections": [
                    {
                        "title": "Lip-Sync Deepfake Technology",
                        "content": "<p>Lip-syncing is a type of facial reenactment. A generative model is trained to map audio features to the corresponding mouth shapes. Given a video of a target person and a new audio track, the model generates a new mouth region for the target person that matches the new audio, and then composites it onto the original video.</p><p>While modern techniques are incredibly realistic, they can still introduce subtle errors, especially with difficult sounds or rapid speech. The goal of a detector is to find the audio-visual inconsistencies that betray the fake. [10]</p>",
                        "image": "https://images.unsplash.com/photo-1587620962725-abab7fe55159?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Audio-Visual Correlation",
                        "content": "<p>The core of lip-sync detection is correlating the two data streams: audio and video.</p><h3>The Process:</h3><ol><li><strong>Audio Analysis:</strong> The audio track is broken down into <strong>phonemes</strong>, which are the basic units of sound in a language (e.g., /k/, /æ/, /t/ in 'cat').</li><li><strong>Video Analysis:</strong> The video track is analyzed to extract the mouth region. This is broken down into <strong>visemes</strong>, which are the visual equivalent of phonemes—the basic mouth shapes (e.g., the shape for /m/ vs. the shape for /o/).</li><li><strong>Correlation:</strong> A deep learning model, often a two-stream network, is trained to learn the correct mapping between phonemes and visemes. It is trained on a large dataset of real, synchronized speech. When given a deepfake, the model will detect that the sequence of visemes does not correctly match the sequence of phonemes, and will flag it as a fake.</li></ol>",
                        "image": "https://i.imgur.com/k2H1z5F.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Audio_Visual_Lip_Sync_Correlation.py",
                        "language": "python",
                        "code": "# Conceptual logic for a lip-sync detection model\nimport tensorflow as tf\n\n# Assume we have functions to extract audio features (MFCCs) and\n# video features (a crop of the mouth region) for a given time window.\n\ndef build_lip_sync_detector():\n    # --- Audio Stream ---\n    audio_input = tf.keras.Input(shape=audio_feature_shape)\n    # ... (e.g., a 1D CNN or LSTM for the audio) ...\n    audio_features = some_audio_model(audio_input)\n\n    # --- Video Stream ---\n    video_input = tf.keras.Input(shape=video_feature_shape)\n    # ... (e.g., a 2D or 3D CNN for the video) ...\n    video_features = some_video_model(video_input)\n\n    # --- Fusion ---\n    # Concatenate the features from both streams\n    fused_features = tf.keras.layers.concatenate([audio_features, video_features])\n    \n    # A few dense layers to classify the fused features\n    dense1 = tf.keras.layers.Dense(128, activation='relu')(fused_features)\n    output = tf.keras.layers.Dense(1, activation='sigmoid')(dense1)\n\n    model = tf.keras.Model(inputs=[audio_input, video_input], outputs=output)\n    model.compile(optimizer='adam', loss='binary_crossentropy')\n    return model"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary goal of a lip-sync deepfake?",
                        "options": [
                            "To change a person's entire face.",
                            "To manipulate a person's mouth to match a new, fake audio track. [10]",
                            "To improve the audio quality of a video.",
                            "To change the background of a video."
                        ],
                        "correct": 1,
                        "explanation": "Lip-syncing is a specific type of facial reenactment that focuses on manipulating the mouth region to create the illusion that the person is speaking the words from a different audio source."
                    },
                    {
                        "id": 2,
                        "question": "What are 'visemes'?",
                        "options": [
                            "The basic units of sound in a language.",
                            "The basic, visual mouth shapes that correspond to speech sounds.",
                            "A type of deepfake detection model.",
                            "A type of audio file."
                        ],
                        "correct": 1,
                        "explanation": "Visemes are the visual counterpart to phonemes. Lip-sync detection is fundamentally about checking if the sequence of visemes in the video correctly matches the sequence of phonemes in the audio."
                    },
                    {
                        "id": 3,
                        "question": "What kind of neural network architecture is well-suited for lip-sync detection?",
                        "options": [
                            "A single model that only looks at the video.",
                            "A single model that only listens to the audio.",
                            "A multi-stream model that processes the audio and video in parallel and then fuses their features to make a final decision.",
                            "A simple linear regression model."
                        ],
                        "correct": 2,
                        "explanation": "Because the task is about finding a mismatch between two different data modalities (audio and video), a two-stream architecture that can learn from and correlate both inputs is the natural and most effective approach."
                    }
                ]
            }
        },
        {
            "id": "lesson-11",
            "title": "Physiological Signal Analysis",
            "duration": "75 min",
            "objectives": [
                "Understand the concept of detecting subtle, involuntary biological signals",
                "Learn about remote photoplethysmography (rPPG) for heart rate detection [5]",
                "Explore the analysis of microexpressions as a detection signal",
                "Analyze the challenges and limitations of physiological detection",
                "Implement a simple rPPG signal extraction from a facial video"
            ],
            "content": {
                "overview": "Deepfakes are getting better at mimicking what we see, but they often fail to replicate the subtle, involuntary physiological signals of a living human. This lesson explores the cutting-edge of detection research, which focuses on extracting signals like a person's heartbeat directly from a video to verify that it is of a real, live person.",
                "sections": [
                    {
                        "title": "Remote Photoplethysmography (rPPG)",
                        "content": "<p>When your heart beats, it pumps blood through your body. This causes tiny, imperceptible changes in the color of your skin as blood flows through the capillaries. <strong>Remote Photoplethysmography (rPPG)</strong> is a technique that can measure these tiny color changes in a facial video to recover a person's heart rate signal. [5]</p><p>Deepfake models are not trained to replicate this subtle, physiological signal. Therefore, a video of a deepfaked face will not contain a realistic rPPG signal. A detector can analyze a video, extract the rPPG signal, and if the signal is absent or does not have the properties of a real human heartbeat, it can be flagged as a fake. This is a powerful form of liveness detection.</p>",
                        "image": "https://i.imgur.com/fgS4fG3.png"
                    },
                    {
                        "title": "Microexpressions and Other Signals",
                        "content": "<p>Researchers are exploring other involuntary biological signals as well.</p><ul><li><strong>Microexpressions:</strong> These are very brief, involuntary facial expressions that can reveal a person's true emotion. Generative models may struggle to replicate these fast and subtle muscle movements correctly.</li><li><strong>Breathing Patterns:</strong> The subtle movements of the chest and shoulders associated with breathing can also be analyzed for their naturalness.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Remote_PPG_Signal_Extraction_from_Facial_Video.py",
                        "language": "python",
                        "code": "import cv2\nimport numpy as np\n\n# Highly simplified conceptual code for rPPG signal extraction\ndef extract_rppg(video_path):\n    cap = cv2.VideoCapture(video_path)\n    # We will track the average green channel value in a region of interest (e.g., the forehead)\n    signal = []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Assume 'forehead_roi' is a bounding box for the forehead\n        forehead = frame[forehead_roi.y:forehead_roi.y+forehead_roi.h, forehead_roi.x:forehead_roi.x+forehead_roi.w]\n        \n        # The green channel is often the most informative for rPPG\n        avg_green = np.mean(forehead[:, :, 1])\n        signal.append(avg_green)\n\n    cap.release()\n    \n    # The raw signal then needs to be filtered and processed with a Fourier Transform\n    # to find the dominant frequency, which corresponds to the heart rate.\n    # A deepfake video would likely have a very noisy or flat signal.\n    return signal"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is remote photoplethysmography (rPPG)?",
                        "options": [
                            "A type of deepfake.",
                            "A technique to measure a person's heart rate by analyzing tiny color changes in their skin from a video. [5]",
                            "A method for analyzing eye blinking.",
                            "A way to measure head pose."
                        ],
                        "correct": 1,
                        "explanation": "rPPG is a powerful liveness detection technique because it measures an involuntary physiological signal that generative models are not designed to replicate."
                    },
                    {
                        "id": 2,
                        "question": "Why is physiological signal analysis a promising direction for deepfake detection?",
                        "options": [
                            "Because the signals are easy to see with the naked eye.",
                            "Because deepfake models are not trained on the underlying biology and often fail to reproduce these subtle, involuntary signals.",
                            "Because all deepfakes have a heartbeat.",
                            "Because it is very fast."
                        ],
                        "correct": 1,
                        "explanation": "This approach tries to find a signal that is fundamentally linked to the subject being a live human. Since GANs are only trained to reproduce visual appearance, they are unlikely to correctly synthesize these hidden physiological signals."
                    },
                    {
                        "id": 3,
                        "question": "Which of the following is a major challenge for detection methods based on physiological signals?",
                        "options": [
                            "They work on all videos.",
                            "The signals are very strong and easy to measure.",
                            "The signals can be very weak and are highly sensitive to noise from video compression and lighting changes.",
                            "The signals are the same for every person."
                        ],
                        "correct": 2,
                        "explanation": "The color changes from blood flow are extremely subtle. Heavy video compression or poor lighting can easily destroy this signal, which makes these techniques difficult to apply robustly in real-world, unconstrained videos."
                    }
                ]
            }
        },
        {
            "id": "lesson-12",
            "title": "Advanced Architectures: Vision Transformers (ViT)",
            "duration": "75 min",
            "objectives": [
                "Understand the limitations of CNNs for deepfake detection",
                "Learn the high-level architecture of a Vision Transformer (ViT)",
                "Analyze why ViTs may offer better generalization against novel fakes [7]",
                "Implement a simple ViT for image classification",
                "Compare the performance of a CNN vs. a ViT on a cross-forgery dataset"
            ],
            "content": {
                "overview": "While Convolutional Neural Networks (CNNs) have been the workhorse of deepfake detection, they have limitations. This lesson introduces the Vision Transformer (ViT), a newer architecture that is showing great promise in computer vision and may be key to solving the difficult 'generalization' problem in deepfake detection.",
                "sections": [
                    {
                        "title": "Limitations of CNNs",
                        "content": "<p>CNNs are excellent at finding local, textural patterns in an image. This is why they are so good at detecting low-level artifacts like blurring or compression mismatches. However, this is also a weakness.</p><p>A CNN might learn to associate a specific type of blurring artifact with a specific deepfake method. When it sees a new deepfake method that has a different type of artifact, it fails to generalize. It has learned the 'artifact', not the abstract concept of 'fake'. [7]</p>",
                        "image": "https://i.imgur.com/k2H1z5F.png"
                    },
                    {
                        "title": "Vision Transformers (ViT)",
                        "content": "<p>The <strong>Vision Transformer (ViT)</strong> is an architecture that was adapted from the field of Natural Language Processing. Instead of looking at local regions with convolutional filters, a ViT breaks an image down into a sequence of smaller patches.</p><p>It then uses a mechanism called <strong>self-attention</strong> to learn the relationships between all of these patches simultaneously. This allows the model to learn much more global, long-range dependencies in the image, rather than just local textures. The hope is that by learning these more global, structural relationships, a ViT might be better at learning the more fundamental inconsistencies of a deepfake (like an unnatural facial shape) rather than just the low-level pixel artifacts. [7]</p>",
                        "image": "https://i.imgur.com/6J7V2xT.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Implementing_a_Simple_ViT_for_Deepfake_Classification.py",
                        "language": "python",
                        "code": "# Using a pre-built ViT model from a library like Hugging Face's 'transformers' is common.\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport torch\n\n# Assume 'image' is a PIL Image object of a face\n\n# Load the processor and a pre-trained ViT model\nprocessor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n\n# Preprocess the image\ninputs = processor(images=image, return_tensors=\"pt\")\n\n# Make a prediction\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n# The logits can then be used to determine the class\npredicted_class_idx = logits.argmax(-1).item()\nprint(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a major limitation of CNNs for deepfake detection?",
                        "options": [
                            "They are too slow.",
                            "They tend to overfit on low-level, local artifacts, which harms their ability to generalize to new, unseen fake types. [7]",
                            "They can only be used for images, not video.",
                            "They are not a type of deep learning."
                        ],
                        "correct": 1,
                        "explanation": "This is the core of the generalization problem. A CNN might learn the specific 'tells' of one GAN, but fail completely when faced with a GAN that has different artifacts."
                    },
                    {
                        "id": 2,
                        "question": "What is the key mechanism used by a Vision Transformer (ViT)?",
                        "options": [
                            "Convolutional filters",
                            "Recurrent connections",
                            "Self-attention, which learns the relationships between all patches of an image simultaneously.",
                            "A linear decision boundary."
                        ],
                        "correct": 2,
                        "explanation": "Self-attention is the core innovation of the Transformer architecture. It allows the model to weigh the importance of different parts of the input when making a decision, enabling it to learn more global features."
                    },
                    {
                        "id": 3,
                        "question": "What is the primary hope for using ViTs in deepfake detection?",
                        "options": [
                            "That they will be faster than CNNs.",
                            "That by learning more global, structural features, they will be better at generalizing to new and unseen deepfake methods. [7]",
                            "That they will require less data to train.",
                            "That they are easier to implement."
                        ],
                        "correct": 1,
                        "explanation": "The hypothesis is that ViTs might learn more fundamental inconsistencies related to facial geometry or physics, rather than just the pixel-level artifacts of a specific generator, which would make them more robust detectors."
                    }
                ]
            }
        },
        {
            "id": "lesson-13",
            "title": "Introduction to Audio Deepfakes (Voice Cloning)",
            "duration": "75 min",
            "objectives": [
                "Understand the technology behind voice cloning and audio deepfakes [3, 13]",
                "Analyze the unique challenges posed by audio-only fakes [5]",
                "Learn about the threat of voice-based phishing ('vishing') [2]",
                "Explore the use of spectrograms for audio analysis"
            ],
            "content": {
                "overview": "The threat of deepfakes extends beyond video to the realm of audio. Modern AI can clone a person's voice from just a few seconds of sample audio, creating a powerful tool for fraud and impersonation. This lesson introduces the technology behind audio deepfakes and the unique forensic challenges they present.",
                "sections": [
                    {
                        "title": "Voice Cloning Technology",
                        "content": "<p><strong>Voice cloning</strong> (or text-to-speech synthesis) models are trained on a large corpus of a target person's speech. The model learns the unique characteristics of that person's voice: their pitch, cadence, and accent.</p><p>Once trained, the model can be given any new text, and it will generate an audio clip of that text being spoken in the target person's voice. The realism of these clones has improved dramatically in recent years, making them very difficult to distinguish from a real recording. [3, 13]</p>",
                        "image": "https://i.imgur.com/k2H1z5F.png"
                    },
                    {
                        "title": "The Threat of Vishing",
                        "content": "<p>The most direct threat from voice cloning is its use in <strong>vishing</strong> (voice phishing). An attacker can use a cloned voice to leave a voicemail or even conduct a live phone call to impersonate someone the victim trusts.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Impersonation Attack</strong></div><p>The classic scenario is CEO fraud. An attacker clones the CEO's voice. They call an employee in the finance department, and the employee sees the call is coming from the CEO's real phone number (using caller ID spoofing). The cloned voice then says, 'I'm in a meeting and can't talk, but I need you to urgently process a wire transfer to this new client'. The combination of a trusted voice and a sense of urgency can be extremely effective. [2]</p></div>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Spectrogram Analysis",
                        "content": "<p>To detect audio deepfakes, we need to analyze the audio signal. A common way to do this is to convert the audio into a <strong>spectrogram</strong>. A spectrogram is a visual representation of the spectrum of frequencies in a sound as they vary over time.</p><p>It turns an audio analysis problem into an image analysis problem. We can then use a powerful image classification model, like a CNN, to learn the subtle patterns and artifacts in the spectrogram that differentiate a real human voice from a synthetic one.</p>",
                        "image": "https://i.imgur.com/u7y7o9o.png"
                    }
                ]
            },
            "codeExamples": [
                {
                    "title": "Spectrogram_Analysis_of_Real_vs_Cloned_Voice.py",
                    "language": "python",
                    "code": "import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Librosa is a powerful library for audio analysis\n\ndef plot_spectrogram(audio_file):\n    # Load the audio file\n    y, sr = librosa.load(audio_file)\n    \n    # Create a spectrogram (specifically, a Mel-spectrogram)\n    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n    log_S = librosa.power_to_db(S, ref=np.max)\n    \n    # Plot the spectrogram\n    plt.figure(figsize=(12, 4))\n    librosa.display.specshow(log_S, sr=sr, x_axis='time', y_axis='mel')\n    plt.title(f'Mel-spectrogram for {audio_file}')\n    plt.colorbar(format='%+2.0f dB')\n    plt.tight_layout()\n    plt.show()\n\n# Compare a real vs. a fake audio file\nplot_spectrogram('real_voice.wav')\nplot_spectrogram('cloned_voice.wav')\n# A detector would train a CNN on these spectrogram images."
                }
            ],
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is 'voice cloning'?",
                        "options": [
                            "A method for recording voices.",
                            "The use of AI to generate speech in a specific target person's voice. [3, 13]",
                            "A type of video deepfake.",
                            "A way to change the pitch of a voice."
                        ],
                        "correct": 1,
                        "explanation": "Voice cloning, or audio synthesis, allows an attacker to create new, synthetic audio of a person saying anything the attacker desires."
                    },
                    {
                        "id": 2,
                        "question": "What is 'vishing'?",
                        "options": [
                            "Visual phishing",
                            "Video phishing",
                            "Voice phishing, a social engineering attack over the phone. [2]",
                            "A type of computer virus."
                        ],
                        "correct": 2,
                        "explanation": "Vishing uses voice communication to trick a victim. Deepfake audio makes these attacks much more potent by allowing the attacker to impersonate a trusted individual."
                    },
                    {
                        "id": 3,
                        "question": "What is a spectrogram?",
                        "options": [
                            "A transcript of the audio.",
                            "A measure of the audio's volume.",
                            "A visual representation of the frequencies in an audio signal over time.",
                            "A type of audio file format."
                        ],
                        "correct": 2,
                        "explanation": "Spectrograms are a key tool in audio forensics. They convert the audio detection problem into an image detection problem, allowing us to use powerful computer vision models like CNNs."
                    }
                ]
            }
        },
        {
            "id": "lesson-14",
            "title": "Detecting Audio Artifacts",
            "duration": "75 min",
            "objectives": [
                "Learn to identify common artifacts in synthetic audio",
                "Analyze tonal shifts and timing anomalies [5, 13]",
                "Explore the detection of incorrect background noise",
                "Understand how to use acoustic analysis for detection [3]",
                "Build a simple detector based on audio features"
            ],
            "content": {
                "overview": "While audio deepfakes can be incredibly convincing, the generation process often leaves behind subtle acoustic artifacts that are imperceptible to a human listener but can be detected by an AI model. This lesson covers the specific audio 'tells' that can be used to distinguish a real voice from a fake one.",
                "sections": [
                    {
                        "title": "Common Audio Artifacts",
                        "content": "<p>Even state-of-the-art voice cloning models can introduce subtle errors and artifacts into the audio they generate.</p><h3>Telltale Signs:</h3><ul><li><strong>Robotic or Monotonous Speech:</strong> The generated speech might lack the natural intonation, emotion, and prosody of a real human voice.</li><li><strong>Tonal and Spectral Artifacts:</strong> The model might introduce faint, high-frequency noises or other unnatural patterns into the audio's spectrogram. [13]</li><li><strong>Incorrect Pacing or Cadence:</strong> The rhythm and timing of the speech might be slightly off or unnatural. [5]</li><li><strong>Unnatural Breathing:</strong> The model might fail to generate realistic breathing sounds between phrases.</li></ul>",
                        "image": "https://i.imgur.com/u7y7o9o.png"
                    },
                    {
                        "title": "Background Noise Analysis",
                        "content": "<p>Another powerful detection signal is the background noise. In a real recording made on a mobile phone, there will be a consistent level of background noise (the 'noise floor').</p><p>A simple audio deepfake is often created by generating only the speech, and then pasting it onto a silent track. This can result in an unnaturally 'clean' recording with no background noise. A more sophisticated attacker might add fake background noise, but it may be inconsistent with the acoustic properties of the room or the type of device the speaker is supposedly using.</p>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Detecting_Audio_Anomalies_with_Librosa.py",
                        "language": "python",
                        "code": "import librosa\nfrom sklearn.ensemble import IsolationForest\n\n# Librosa can extract many acoustic features from an audio file.\n# We can use these features to train an anomaly detection model.\n\ndef extract_features(audio_file):\n    y, sr = librosa.load(audio_file)\n    \n    # Extract a variety of features\n    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y=y))\n    \n    return np.hstack((mfccs, spectral_centroid, zero_crossing_rate))\n\n# Assume 'real_audio_files' is a list of paths to clean, authentic audio\n# Train an anomaly detector on the features of the real audio\nreal_features = [extract_features(f) for f in real_audio_files]\nanomaly_model = IsolationForest().fit(real_features)\n\n# Test a new audio file\nnew_audio_file = 'potential_fake.wav'\nnew_features = extract_features(new_audio_file).reshape(1, -1)\n\nprediction = anomaly_model.predict(new_features)\n\nif prediction == -1:\n    print(f\"'{new_audio_file}' is detected as an anomaly (potential fake).\")\nelse:\n    print(f\"'{new_audio_file}' appears to be normal.\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "A synthesized voice that lacks natural intonation and emotion is exhibiting what kind of artifact?",
                        "options": [
                            "Background noise.",
                            "A common artifact of text-to-speech models that struggle with prosody.",
                            "A file format error.",
                            "A sign of a good deepfake."
                        ],
                        "correct": 1,
                        "explanation": "Prosody—the rhythm, stress, and intonation of speech—is very complex and is one of the hardest things for a generative model to replicate perfectly, making it a good detection signal. [13]"
                    },
                    {
                        "id": 2,
                        "question": "Why is analyzing the background noise of an audio clip useful for detection?",
                        "options": [
                            "Because all real recordings are perfectly silent.",
                            "Because a simple audio deepfake might be unnaturally clean, or the added fake background noise might be inconsistent.",
                            "Because background noise is easy to edit.",
                            "It is not useful."
                        ],
                        "correct": 1,
                        "explanation": "Every real recording has a unique noise fingerprint from the room and the microphone. A mismatch or absence of this noise is a strong forensic indicator of manipulation. [3]"
                    },
                    {
                        "id": 3,
                        "question": "What is the general approach for building an AI-based audio deepfake detector?",
                        "options": [
                            "To have a human listen to every audio file.",
                            "To convert the audio to text and analyze the words.",
                            "To extract a set of acoustic features (like MFCCs) or a spectrogram from the audio and train a classifier on those features.",
                            "To check the file's metadata."
                        ],
                        "correct": 2,
                        "explanation": "This is the standard approach. The problem is converted into a feature-based classification or anomaly detection task, where the features are designed to capture the unique acoustic properties of real vs. fake speech."
                    }
                ]
            }
        },
        {
            "id": "lesson-15",
            "title": "Multimodal Detection Strategies",
            "duration": "75 min",
            "objectives": [
                "Understand the principle of combining multiple data streams for detection [14]",
                "Learn how multimodal systems can cross-reference audio, video, and text",
                "Analyze the architecture of a multimodal deepfake detector",
                "Explore the benefits of fusion in improving detection accuracy",
                "Build a simple multimodal detector concept"
            ],
            "content": {
                "overview": "Why rely on just one clue when you can use all of them? Multimodal detection is a powerful approach that combines and cross-references signals from different data streams (e.g., audio and video) to make a more robust and accurate decision. This lesson explores the principles and architectures of multimodal deepfake detectors.",
                "sections": [
                    {
                        "title": "The Principle of Multimodal Detection",
                        "content": "<p>A <strong>multimodal</strong> system is one that processes and relates information from multiple different modalities (types of data). For deepfake detection, the primary modalities are the visual stream (video) and the auditory stream (audio).</p><p>The core idea is that an attacker might be able to create a very good fake in *one* modality, but it is much harder to create a fake that is perfectly consistent *across* multiple modalities. A multimodal detector is designed to find these cross-modal inconsistencies. [14]</p>",
                        "image": "https://i.imgur.com/u7nL6Xk.png"
                    },
                    {
                        "title": "Cross-Modal Inconsistencies",
                        "content": "<p>A multimodal system can look for contradictions between the different data streams:</p><ul><li><strong>Lip-Sync Mismatch:</strong> As discussed in Lesson 10, this is the classic example. The lip movements (video) do not match the spoken sounds (audio).</li><li><strong>Emotion Mismatch:</strong> The tone of voice in the audio might be angry, but the facial expression in the video might be neutral.</li><li><strong>Speaker Mismatch:</strong> A facial recognition model on the video might identify the speaker as Person A, while a voice recognition model on the audio identifies the speaker as Person B.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Multimodal Architectures",
                        "content": "<p>A multimodal detector typically uses a multi-stream neural network architecture.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Early vs. Late Fusion</strong></div><p>There are two main ways to combine the data:<ul><li><strong>Early Fusion:</strong> The raw features from the audio and video streams are concatenated at the beginning and fed into a single, large model.</li><li><strong>Late Fusion:</strong> A separate model is trained for each modality (an audio model and a video model). The outputs (predictions or high-level features) of these models are then combined by a final model to make the decision.</li></ul>Late fusion is often more robust and is a common approach for building these systems. [14]</p></div>",
                        "image": "https://i.imgur.com/6J7V2xT.png"
                    }
                ]
            },
            "codeExamples": [
                {
                    "title": "Correlating_Audio_Speech_with_Video_Lip_Movements.py",
                    "language": "python",
                    "code": "# This is the same conceptual model from Lesson 10, now framed as a multimodal system\nimport tensorflow as tf\n\ndef build_multimodal_lip_sync_detector():\n    # --- Modality 1: Audio Stream ---\n    audio_input = tf.keras.Input(shape=audio_feature_shape, name=\"audio_input\")\n    audio_model = some_audio_cnn(audio_input)\n\n    # --- Modality 2: Video Stream ---\n    video_input = tf.keras.Input(shape=video_feature_shape, name=\"video_input\")\n    video_model = some_video_cnn(video_input)\n\n    # --- Fusion Layer ---\n    # Late fusion by concatenating the high-level features from each stream\n    fused_features = tf.keras.layers.concatenate([audio_model, video_model])\n    \n    # --- Classifier Head ---\n    dense_layer = tf.keras.layers.Dense(128, activation='relu')(fused_features)\n    output = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(dense_layer)\n\n    model = tf.keras.Model(inputs=[audio_input, video_input], outputs=output)\n    model.compile(optimizer='adam', loss='binary_crossentropy')\n    return model"
                }
            ],
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a multimodal deepfake detection system?",
                        "options": [
                            "A system that only analyzes video.",
                            "A system that combines and cross-references information from multiple data types, like audio and video. [14]",
                            "A system that uses multiple different video codecs.",
                            "A system that is very slow."
                        ],
                        "correct": 1,
                        "explanation": "Multimodal systems leverage multiple streams of data (modalities) to make a more informed and robust decision."
                    },
                    {
                        "id": 2,
                        "question": "Detecting that a person's facial expression looks happy while their tone of voice sounds sad is an example of what?",
                        "options": [
                            "A spatial artifact.",
                            "A cross-modal inconsistency.",
                            "A temporal artifact.",
                            "A metadata error."
                        ],
                        "correct": 1,
                        "explanation": "This is a contradiction between the visual modality (the face) and the auditory modality (the voice), which is a powerful signal that the media has been manipulated."
                    },
                    {
                        "id": 3,
                        "question": "What is 'late fusion' in a multimodal architecture?",
                        "options": [
                            "Combining the raw data at the very beginning.",
                            "A system that makes decisions very slowly.",
                            "Training a separate model for each data stream and then combining their high-level outputs to make a final decision. [14]",
                            "Using only one type of data."
                        ],
                        "correct": 2,
                        "explanation": "Late fusion is a common and effective strategy. It allows for the use of specialized models that are best-suited for each individual modality, and then a simpler model can learn how to combine their expert opinions."
                    }
                ]
            }
        },
        
        {
            "id": "lesson-16",
            "title": "Liveness and Presentation Attack Detection",
            "duration": "75 min",
            "objectives": [
                "Differentiate between a live person and a digital representation [13]",
                "Understand the concept of a Presentation Attack (spoofing)",
                "Learn about different liveness detection techniques [3]",
                "Implement a conceptual challenge-response liveness test"
            ],
            "content": {
                "overview": "How does a system know it's interacting with a live person and not just a very good deepfake video being played on a screen? This lesson focuses on liveness and presentation attack detection, the crucial security layer that prevents an attacker from spoofing a biometric system with a digital forgery.",
                "sections": [
                    {
                        "title": "Presentation Attack Detection",
                        "content": "<p>A <strong>Presentation Attack</strong> is an attempt to spoof a biometric system (like a facial recognition login) by presenting it with a fake artifact. This could be a printed photo, a video played on a screen, or a sophisticated real-time deepfake.</p><p><strong>Liveness Detection</strong> is the set of technologies used to counter these attacks. The goal is to determine if the biometric being presented is from a live, physically present human. It is a critical component for any system that uses facial recognition for authentication or identity verification. [3]</p>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Liveness Detection Techniques",
                        "content": "<p>There are two main categories of liveness detection:</p><ul><li><strong>Active Liveness:</strong> This method requires the user to perform an action. It sends a 'challenge' to the user, such as 'smile', 'turn your head to the left', or 'move closer to the camera'. The system then analyzes the video to see if the user responded correctly. This is effective but can add friction to the user experience.</li><li><strong>Passive Liveness:</strong> This method works in the background without requiring any action from the user. It uses AI to analyze the video for subtle signs of life, such as eye blinking, microexpressions, or physiological signals like heart rate (rPPG). It can also look for artifacts that indicate a presentation attack, like the borders of a screen or reflections from a printed photo. [13]</li></ul>",
                        "image": "https://i.imgur.com/u7y7o9o.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Implementing_a_Challenge_Response_Liveness_Test.py",
                        "language": "python",
                        "code": "import cv2\nimport random\n\n# Conceptual code for a simple, active liveness test\ndef active_liveness_check():\n    # 1. Choose a random challenge\n    challenges = [\"smile\", \"blink\", \"turn_left\"]\n    challenge = random.choice(challenges)\n    print(f\"Liveness Challenge: Please {challenge.replace('_', ' ')}!\")\n\n    # 2. Capture video from the webcam\n    cap = cv2.VideoCapture(0)\n    # ... (code to capture a few seconds of video) ...\n    cap.release()\n\n    # 3. Analyze the captured video with a specific model for that challenge\n    if challenge == \"smile\":\n        # Use a smile detection model to see if the user smiled\n        # success = smile_detection_model.predict(video_frames)\n        pass\n    elif challenge == \"blink\":\n        # Use a blink detection model\n        # success = blink_detection_model.predict(video_frames)\n        pass\n\n    # 4. Return whether the challenge was completed successfully\n    # if success:\n    #     print(\"Liveness check passed!\")\n    # else:\n    #     print(\"Liveness check failed.\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a 'Presentation Attack'?",
                        "options": [
                            "A lecture about deepfakes.",
                            "An attempt to spoof a biometric system by presenting it with a fake artifact, like a photo or a video. [3]",
                            "A type of deepfake generation method.",
                            "A defense against deepfakes."
                        ],
                        "correct": 1,
                        "explanation": "This is the formal term for spoofing. The attacker 'presents' a fake biometric to the sensor."
                    },
                    {
                        "id": 2,
                        "question": "What is the primary goal of liveness detection?",
                        "options": [
                            "To identify the person.",
                            "To determine if the biometric being presented is from a live, physically present human. [13]",
                            "To check if the user is paying attention.",
                            "To measure the quality of the video."
                        ],
                        "correct": 1,
                        "explanation": "Liveness detection is the core countermeasure against presentation attacks. It is designed to distinguish a live person from a fake artifact."
                    },
                    {
                        "id": 3,
                        "question": "Requiring a user to turn their head to the left during a facial recognition login is an example of what?",
                        "options": [
                            "A passive liveness test.",
                            "An active, challenge-response liveness test.",
                            "A deepfake detection algorithm.",
                            "A biometric enrollment process."
                        ],
                        "correct": 1,
                        "explanation": "This is an 'active' test because it requires the user to perform a specific, challenged action, which is difficult for a simple presentation attack (like a photo) to replicate."
                    }
                ]
            }
        },
        {
            "id": "lesson-17",
            "title": "The Generalization Problem",
            "duration": "75 min",
            "objectives": [
                "Understand why detectors trained on one deepfake type fail on new methods [7, 15]",
                "Analyze the concept of overfitting to generation artifacts",
                "Explore the importance of cross-dataset evaluation",
                "Discuss strategies for improving model generalization"
            ],
            "content": {
                "overview": "This lesson addresses the single biggest challenge in deepfake detection: generalization. We will explore why a model that achieves 99% accuracy on one dataset can fail completely on another, and discuss the strategies researchers are using to build more robust and general-purpose detectors.",
                "sections": [
                    {
                        "title": "The Core Challenge",
                        "content": "<p>The <strong>generalization problem</strong> refers to the fact that most deepfake detectors learn the specific artifacts of the generation methods in their training data, rather than a general, abstract concept of what is 'fake'.</p><p>This means a model trained exclusively on deepfakes from Generator A will be very good at detecting fakes from Generator A. However, when a new, more advanced Generator B is released, its artifacts will be different, and the model trained on A will likely fail to detect fakes from B. [15]</p>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e_35f?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Cross-Dataset Evaluation",
                        "content": "<p>To properly measure a model's ability to generalize, we must perform <strong>cross-dataset evaluation</strong>. This involves training a model on one dataset (e.g., FaceForensics++) and testing it on a completely different dataset with different forgery methods (e.g., Celeb-DF). A large drop in accuracy during cross-dataset testing is a clear sign that the model has overfit to the artifacts of its training set and is not a robust, general-purpose detector. [7]</p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Strategies for Improving Generalization",
                        "content": "<p>Solving the generalization problem is the primary goal of modern deepfake detection research.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>Research Directions</strong></div><p><ul><li><strong>Data Augmentation:</strong> Using a wide variety of data augmentations during training (like aggressive compression, noise, and blurring) can force the model to learn more robust features.</li><li><strong>Diverse Training Data:</strong> Training a single model on a large and diverse dataset that combines many different forgery types.</li><li><strong>Focusing on Intrinsic Signals:</strong> Developing models that focus on signals that are fundamental to all forgeries, such as physiological signals (heart rate) or physical inconsistencies (lighting), rather than specific generator artifacts.</li><li><strong>Advanced Architectures:</strong> Using models like Vision Transformers (ViTs) that may be better at learning global, structural inconsistencies rather than local, textural artifacts. [7]</li></ul></p></div>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    }
                ]
            },
            "codeExamples": [
                {
                    "title": "Cross_Dataset_Evaluation_of_a_Detection_Model.py",
                    "language": "python",
                    "code": "# Conceptual code for cross-dataset evaluation\n\n# Assume 'model' is a trained deepfake detector\n# Assume test_loader_A is from the same dataset family as the training data (e.g., FF++)\n# Assume test_loader_B is from a completely different dataset (e.g., Celeb-DF)\n\ndef evaluate_model(model, data_loader):\n    # ... (code to loop through data, get predictions, and calculate accuracy) ...\n    return accuracy\n\n# 1. Train the model on the training set of Dataset A\nmodel.fit(train_loader_A, ...)\n\n# 2. Evaluate on the test set of Dataset A (in-distribution)\naccuracy_A = evaluate_model(model, test_loader_A)\nprint(f\"Accuracy on seen forgery types (Dataset A): {accuracy_A * 100:.2f}%\")\n\n# 3. Evaluate on the test set of Dataset B (out-of-distribution)\naccuracy_B = evaluate_model(model, test_loader_B)\nprint(f\"Accuracy on unseen forgery types (Dataset B): {accuracy_B * 100:.2f}%\")\n\n# A large drop from accuracy_A to accuracy_B indicates poor generalization."
                }
            ],
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the 'generalization problem' in deepfake detection?",
                        "options": [
                            "Models are too general and cannot detect specific fakes.",
                            "Models trained on one dataset often fail to detect fakes from a different dataset created with new techniques. [15]",
                            "The datasets are too small.",
                            "The models are too difficult to train."
                        ],
                        "correct": 1,
                        "explanation": "This is the core challenge. Models often learn the 'tells' of a specific forgery method, rather than a general concept of what is 'fake', so they don't generalize well to new methods."
                    },
                    {
                        "id": 2,
                        "question": "What is the purpose of cross-dataset evaluation?",
                        "options": [
                            "To make the model train faster.",
                            "To specifically measure a model's ability to generalize to new, unseen forgery types. [7]",
                            "To increase the size of the training set.",
                            "To test the model's accuracy on the data it was trained on."
                        ],
                        "correct": 1,
                        "explanation": "This is the standard and most honest way to evaluate a detector's real-world utility. A model is not robust if it only works on the specific types of fakes it has already seen."
                    },
                    {
                        "id": 3,
                        "question": "Which of the following is a strategy to improve a model's generalization?",
                        "options": [
                            "Training on a very small, clean dataset.",
                            "Using only one type of data augmentation.",
                            "Training on a large and diverse dataset that includes many different forgery types.",
                            "Using a very simple, linear model."
                        ],
                        "correct": 2,
                        "explanation": "Diversity is key. By showing the model a wide variety of different fakes, we can encourage it to learn more general and robust features that are common to all forgeries, rather than the quirks of a single one."
                    }
                ]
            }
        },
        {
            "id": "lesson-18",
            "title": "Adversarial Attacks Against Detectors",
            "duration": "75 min",
            "objectives": [
                "Understand how deepfake detectors can themselves be attacked",
                "Learn the concept of 'anti-forensics' [10]",
                "Explore how adversarial perturbations can be used to fool a detector",
                "Analyze the challenges of defending against these attacks",
                "Implement a simple adversarial attack on a detection model"
            ],
            "content": {
                "overview": "The deepfake arms race is not one-sided. Just as we use AI to detect fakes, attackers can use AI to attack our detectors. This lesson explores the concept of 'anti-forensics', where an adversary intentionally crafts a deepfake that is designed to bypass a specific detection algorithm.",
                "sections": [
                    {
                        "title": "The Anti-Forensic Challenge",
                        "content": "<p>A deepfake detector is just another machine learning model. This means it is vulnerable to the same adversarial evasion attacks that we have studied in other contexts.</p><p>An attacker with white-box or black-box access to a detection model can use attacks like PGD to create an <strong>adversarial deepfake</strong>. This is a deepfake that has been slightly perturbed in a way that causes the detector to misclassify it as 'real', while still appearing as a convincing fake to a human. [10]</p>",
                        "image": "https://i.imgur.com/kYq3Q6k.png"
                    },
                    {
                        "title": "Attacking the Detector",
                        "content": "<p>The process of attacking a detector is a classic evasion attack.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Attacker's Process</strong></div><p><ol><li>The attacker starts with a deepfake video that is correctly identified by the target detector.</li><li>The attacker defines their goal: to find the smallest possible perturbation that, when added to the deepfake, causes the detector's output to flip from 'fake' to 'real'.</li><li>The attacker uses a standard adversarial attack algorithm (like PGD) to solve this optimization problem. The 'loss function' is defined to maximize the detector's 'real' score.</li><li>The result is a new video that is still a deepfake, but is no longer caught by the detector.</li></ol></p></div>",
                        "image": "https://images.unsplash.com/photo-1599508704512-2f19efd1e_35f?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Generating_Adversarial_Perturbations_to_Bypass_a_Detector.py",
                        "language": "python",
                        "code": "# This is a standard PGD attack, but the goal is to fool our OWN detector.\n\n# Assume 'deepfake_detector_model' is our trained classifier.\n# Assume 'original_deepfake_image' is a fake image that our model correctly detects.\n# The 'true_label' for this image is 'FAKE' (e.g., 1).\n# The attacker's TARGET label is 'REAL' (e.g., 0).\n\ndef attack_the_detector(detector_model, original_deepfake_image):\n    # The true label of this image is FAKE\n    true_label = 1\n    \n    # The attacker's desired label is REAL\n    target_label_for_attack = 0\n    \n    # Use a targeted PGD attack to find a perturbation that pushes the image\n    # towards the 'REAL' class in the detector's eyes.\n    adversarial_image = targeted_pgd_attack(\n        model=detector_model, \n        image=original_deepfake_image, \n        target_label=target_label_for_attack,\n        ...\n    )\n\n    # The new 'adversarial_image' is still a deepfake, but the detector will now\n    # likely classify it as 'real'.\n    prediction = detector_model.predict(adversarial_image)\n    print(f\"Detector's prediction on the adversarial deepfake: {prediction}\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is 'anti-forensics' in the context of deepfakes?",
                        "options": [
                            "A method for detecting fakes.",
                            "The practice of intentionally crafting a deepfake to evade detection by a specific forensic tool. [10]",
                            "A legal term.",
                            "A type of digital watermark."
                        ],
                        "correct": 1,
                        "explanation": "Anti-forensics is the offensive side of the arms race. It involves creating forgeries that are aware of the detection methods and are specifically designed to bypass them."
                    },
                    {
                        "id": 2,
                        "question": "An attack that slightly modifies a deepfake video to make a detection model classify it as 'real' is an example of what?",
                        "options": [
                            "A data poisoning attack.",
                            "An adversarial evasion attack.",
                            "A model stealing attack.",
                            "A physical attack."
                        ],
                        "correct": 1,
                        "explanation": "This is a classic evasion attack. The target model is the detector, and the attacker's goal is to cause a misclassification from 'fake' to 'real'."
                    },
                    {
                        "id": 3,
                        "question": "What is the most effective defense against these adversarial attacks?",
                        "options": [
                            "To use a very simple detection model.",
                            "To use adversarial training, where the detector is trained on adversarial deepfakes designed to fool it.",
                            "To only use black-box detection models.",
                            "There are no defenses."
                        ],
                        "correct": 1,
                        "explanation": "The best defense is a proactive one. By showing the detector the kinds of attacks that can fool it during training, it can learn a more robust decision boundary, making it harder to attack in the future. This is the core concept of adversarial training."
                    }
                ]
            }
        },
        {
            "id": "lesson-19",
            "title": "Proactive Defense: Digital Watermarking and Provenance",
            "duration": "75 min",
            "objectives": [
                "Understand the principles of digital watermarking",
                "Explore techniques for embedding invisible watermarks to verify authenticity",
                "Learn how blockchain can be used for immutable records of media origin [4, 6]",
                "Analyze the challenges of deploying a provenance-based system at scale"
            ],
            "content": {
                "overview": "Instead of trying to detect fakes after they are created, can we proactively mark real content as authentic at the time of its creation? This lesson explores proactive defenses like digital watermarking and provenance, which aim to create a verifiable chain of custody for digital media, making it much easier to spot unverified fakes.",
                "sections": [
                    {
                        "title": "Digital Watermarking",
                        "content": "<p>A <strong>digital watermark</strong> is a secret signal or pattern that is embedded directly into a piece of media. It is designed to be imperceptible to a human but detectable by a special algorithm.</p><h3>How it Works for Authentication:</h3><ol><li>A camera or a social media platform could automatically embed a secret, invisible watermark into every photo or video it creates.</li><li>When a piece of media is uploaded, a verifier can check for the presence of this valid watermark.</li><li>If the watermark is present and correct, the media is likely authentic. If the watermark is absent or has been corrupted (which would happen if the media is manipulated), it can be flagged as potentially fake.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1526374965328-7f61d4dc18c5?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Provenance and Blockchain",
                        "content": "<p><strong>Provenance</strong> is the documented history of a piece of media. A robust provenance system aims to create an immutable log of a file's origin and any changes made to it. Blockchain is a powerful technology for creating such a log. [4, 6]</p><h3>The C2PA Standard:</h3><p>The <strong>Coalition for Content Provenance and Authenticity (C2PA)</strong> is an industry-wide standard being developed by companies like Adobe, Microsoft, and Intel. When a user captures a photo on a C2PA-enabled device, a manifest of information is created and cryptographically signed. This manifest includes details about how, when, and where the content was created. Any subsequent edits are also cryptographically signed and added to the manifest. This creates a verifiable, tamper-evident trail of the content's history that anyone can inspect.</p>",
                        "image": "https://images.unsplash.com/photo-1639322537228-f710d846310a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Embedding_and_Detecting_a_Digital_Watermark_in_an_Image.py",
                        "language": "python",
                        "code": "# This is a highly simplified conceptual example of a spatial watermark\nimport cv2\nimport numpy as np\n\ndef embed_watermark(image, watermark_pattern):\n    # Add a low-visibility pattern to the image\n    # In a real system, this would be done in the frequency domain (e.g., DCT)\n    alpha = 0.01 # Controls visibility\n    watermarked_image = cv2.addWeighted(image, 1.0, watermark_pattern, alpha, 0)\n    return watermarked_image\n\ndef detect_watermark(image, original_pattern):\n    # A simple detection method could be to check the correlation\n    # between the image's noise and the original pattern.\n    # ... (complex detection logic) ...\n    # A high correlation suggests the watermark is present.\n    # A low correlation suggests the image is not authentic or has been tampered with.\n    pass"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a digital watermark?",
                        "options": [
                            "A visible logo on an image.",
                            "An imperceptible signal embedded in media that can be used to verify its authenticity.",
                            "A type of deepfake.",
                            "A method for compressing images."
                        ],
                        "correct": 1,
                        "explanation": "A robust digital watermark is designed to be invisible to humans but detectable by an algorithm. It acts as a hidden signature to prove authenticity."
                    },
                    {
                        "id": 2,
                        "question": "What is the primary goal of a content provenance system like C2PA?",
                        "options": [
                            "To detect deepfakes using AI.",
                            "To create a secure, tamper-evident history for a piece of media, from creation to final edit. [4]",
                            "To make images look better.",
                            "To share images on social media."
                        ],
                        "correct": 1,
                        "explanation": "Provenance is a proactive defense. Instead of detecting fakes, it focuses on providing a way to prove what is real. Media without a valid provenance trail can then be treated with suspicion."
                    },
                    {
                        "id": 3,
                        "question": "How does blockchain technology enhance content provenance?",
                        "options": [
                            "It makes the images smaller.",
                            "It can be used to store an immutable, decentralized log of the media's creation and edit history. [6]",
                            "It makes the detection process faster.",
                            "It does not enhance provenance."
                        ],
                        "correct": 1,
                        "explanation": "Blockchain's core property of immutability makes it an ideal technology for creating a chain of custody that cannot be secretly altered or deleted, providing a strong foundation for a trusted provenance system."
                    }
                ]
            }
        },
        {
            "id": "lesson-20",
            "title": "Continual Learning for Detection",
            "duration": "75 min",
            "objectives": [
                "Understand the need for adaptive detection systems [15]",
                "Learn the principles of continual (lifelong) learning [16]",
                "Explore how to update a model with new deepfake examples without catastrophic forgetting",
                "Analyze the architecture of an adaptive detection system",
                "Discuss the security risks of continual learning"
            ],
            "content": {
                "overview": "The deepfake arms race is constantly evolving. A detector trained today will be obsolete tomorrow. This lesson explores the concept of continual learning, where detection models are designed to adapt to new, unseen deepfake techniques over time without having to be completely retrained from scratch.",
                "sections": [
                    {
                        "title": "The Need for Adaptive Systems",
                        "content": "<p>The generalization problem means that static detectors are doomed to fail. As new and better generative models are released, our detectors must be able to adapt and learn from these new threats. [15]</p><p>The naive solution is to simply retrain the entire model from scratch every time a new deepfake method appears. However, this is computationally expensive and inefficient. <strong>Continual Learning</strong> (also known as Lifelong Learning) is a field of ML research focused on this problem. [16]</p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Catastrophic Forgetting",
                        "content": "<p>A major challenge in continual learning is <strong>catastrophic forgetting</strong>. If you take a model that is already trained on Fake Type A and then train it only on new examples of Fake Type B, it will often completely 'forget' how to detect Type A. Its performance on the old task will drop catastrophically.</p><h3>Overcoming Forgetting:</h3><p>Continual learning techniques are designed to prevent this. Some common strategies include:<ul><li><strong>Rehearsal:</strong> Storing a small number of examples from old tasks and mixing them in with the new data during training.</li><li><strong>Elastic Weight Consolidation (EWC):</strong> A technique that identifies the weights in the neural network that are most important for the old task and penalizes the model for changing them too much.</li></ul>",
                        "image": "https://i.imgur.com/8a6R2aU.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Simulating_a_Continual_Learning_Scenario_for_Detectors.py",
                        "language": "python",
                        "code": "# Conceptual code demonstrating the catastrophic forgetting problem\n\n# 1. Train a model on Task A (e.g., detecting Deepfakes from Generator A)\nmodel = build_detector()\nmodel.fit(data_A, labels_A, epochs=10)\nacc_A_after_A = model.evaluate(test_data_A)\nprint(f\"Accuracy on Task A after training on A: {acc_A_after_A}\")\n\n# 2. Now, fine-tune the SAME model ONLY on Task B (Generator B)\nmodel.fit(data_B, labels_B, epochs=10)\nacc_B_after_B = model.evaluate(test_data_B)\nprint(f\"Accuracy on Task B after training on B: {acc_B_after_B}\")\n\n# 3. CRITICAL TEST: Re-evaluate the model on Task A.\nacc_A_after_B = model.evaluate(test_data_A)\nprint(f\"Accuracy on Task A after training on B: {acc_A_after_B}\")\n\n# We will likely see that acc_A_after_B is much lower than acc_A_after_A.\n# The model has 'forgotten' how to do the first task."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Why is continual learning important for deepfake detection?",
                        "options": [
                            "It is not important.",
                            "Because deepfake generation methods are constantly evolving, detectors must be able to adapt to new threats. [15]",
                            "It makes the models smaller.",
                            "It is a type of attack."
                        ],
                        "correct": 1,
                        "explanation": "A static detector is a losing strategy in the deepfake arms race. The system must be adaptive and able to learn from new forgery techniques as they appear."
                    },
                    {
                        "id": 2,
                        "question": "What is 'catastrophic forgetting' in continual learning?",
                        "options": [
                            "A model that cannot learn anything.",
                            "When a model trained on a new task completely loses its ability to perform a previously learned task. [16]",
                            "A user forgetting their password.",
                            "A hardware failure."
                        ],
                        "correct": 1,
                        "explanation": "This is the central challenge of continual learning. The goal is to learn new things without overwriting and destroying the knowledge of old things."
                    },
                    {
                        "id": 3,
                        "question": "What is 'rehearsal' as a continual learning strategy?",
                        "options": [
                            "Having the model practice the same task over and over.",
                            "Storing a small number of old examples and mixing them in with new data during training to remind the model of past tasks.",
                            "A method for making the model smaller.",
                            "A type of data augmentation."
                        ],
                        "correct": 1,
                        "explanation": "Rehearsal, or replay, is one of the simplest and most effective strategies to mitigate catastrophic forgetting. By periodically re-exposing the model to old data, it helps to reinforce the knowledge it previously learned."
                    }
                ]
            }
        },
        {
            "id": "lesson-21",
            "title": "Explainable AI (XAI) for Detection",
            "duration": "75 min",
            "objectives": [
                "Understand why explainability is crucial for deepfake detection",
                "Learn how to use XAI to understand a model's decision process [6, 14]",
                "Explore techniques like Grad-CAM for generating visual explanations (heatmaps)",
                "Analyze how explainability can enhance trust and transparency"
            ],
            "content": {
                "overview": "A detector that just outputs 'real' or 'fake' is a black box. For an analyst to trust and act on a model's prediction, they need to know *why* the model made its decision. This lesson covers the field of Explainable AI (XAI) and its application to deepfake detection, allowing us to peer inside the model and see the evidence it is using.",
                "sections": [
                    {
                        "title": "The Need for Explainability",
                        "content": "<p>In a high-stakes context like identifying disinformation, a simple binary output is not enough. An analyst or a user needs to see the evidence. Explainable AI (XAI) techniques are used to make a model's decision process more transparent. [14]</p><h3>Benefits of XAI:</h3><ul><li><strong>Trust and Transparency:</strong> It allows a human to verify that the model is making a sensible decision based on real artifacts.</li><li><strong>Debugging:</strong> If a model is making a mistake, an explanation can help us understand why. It might be focusing on a spurious correlation in the background instead of the face.</li><li><strong>Discovering New Artifacts:</strong> XAI can sometimes highlight novel artifacts that human researchers were not previously aware of.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Visual Explanations with Grad-CAM",
                        "content": "<p><strong>Grad-CAM (Gradient-weighted Class Activation Mapping)</strong> is a popular XAI technique for computer vision models. It produces a 'heatmap' that highlights the regions of the input image that were most important for the model's final decision.</p><p>In deepfake detection, we can use Grad-CAM to visualize what the model is 'looking at'. If a model correctly classifies an image as a fake, the Grad-CAM heatmap might highlight the specific, subtle artifacts around the mouth or eyes that are inconsistent. This provides a powerful visual explanation that an analyst can use to confirm the model's finding. [6]</p>",
                        "image": "https://i.imgur.com/kYq3Q6k.png"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Generating_Heatmaps_to_Visualize_Deepfake_Artifacts_with_Grad-CAM.py",
                        "language": "python",
                        "code": "# Grad-CAM is complex to implement from scratch, but libraries exist.\n# This is a conceptual example of its usage.\nfrom tf_keras_vis.gradcam import Gradcam\nfrom tf_keras_vis.utils.model_weights import load_model_weights\nfrom tf_keras_vis.utils.scores import CategoricalScore\n\n# Assume 'model' is our trained CNN detector\n# Assume 'image_to_explain' is the input image we want to analyze\n\n# 1. Define the score we want to explain. For a binary classifier,\n#    the score for the 'fake' class is simply the model's output.\nscore = CategoricalScore([1]) # Index 1 for the 'fake' class\n\n# 2. Create a Gradcam object\n# 'model.get_layer('last_conv_layer').output' points to the last convolutional layer\ngradcam = Gradcam(model, model_modifier=None, clone=True)\n\n# 3. Generate the heatmap\ncam = gradcam(score, image_to_explain, penultimate_layer=-1)\n\n# 4. Superimpose the heatmap on the original image for visualization\nheatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\nresult_image = cv2.addWeighted(np.uint8(image_to_explain), 0.7, heatmap, 0.3, 0)\n\n# Display the result\ncv2.imshow(\"Explanation Heatmap\", result_image)\ncv2.waitKey(0)"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary goal of Explainable AI (XAI) in deepfake detection?",
                        "options": [
                            "To make the model more complex.",
                            "To provide a simple 'real' or 'fake' output.",
                            "To make the model's decision-making process transparent and understandable to humans. [14]",
                            "To hide the model's reasoning."
                        ],
                        "correct": 2,
                        "explanation": "XAI is all about opening the 'black box' so that users can trust, debug, and act upon the model's outputs with confidence."
                    },
                    {
                        "id": 2,
                        "question": "What does a Grad-CAM heatmap show?",
                        "options": [
                            "The temperature of the image.",
                            "A random pattern of colors.",
                            "The regions of the input image that were most important for the model's decision. [6]",
                            "The file size of the image."
                        ],
                        "correct": 2,
                        "explanation": "Grad-CAM provides a visual explanation. The 'hot' areas on the map are the pixels that the model was paying the most attention to when it made its classification."
                    },
                    {
                        "id": 3,
                        "question": "How can XAI help to build trust in a deepfake detector?",
                        "options": [
                            "It cannot build trust.",
                            "By showing an analyst the specific visual artifacts the model is detecting, which allows the analyst to verify that the model is making a sensible decision.",
                            "By proving the model is 100% accurate.",
                            "By hiding all the details of the model."
                        ],
                        "correct": 1,
                        "explanation": "Trust comes from understanding. When a model can 'show its work' with an explanation, a human expert is much more likely to trust its conclusions and use it as an effective tool."
                    }
                ]
            }
        },
        {
            "id": "lesson-22",
            "title": "Performance Metrics for Detection",
            "duration": "60 min",
            "objectives": [
                "Understand the importance of using appropriate evaluation metrics [9]",
                "Define and differentiate between Accuracy, Precision, Recall, and AUC",
                "Analyze the challenge of class imbalance in deepfake detection",
                "Learn how to interpret a confusion matrix",
                "Calculate and interpret AUC scores for a detector"
            ],
            "content": {
                "overview": "Simply measuring 'accuracy' is not enough to understand a deepfake detector's true performance. Real-world data is highly imbalanced, and we need more nuanced metrics to measure a model's ability to catch fakes without overwhelming us with false alarms. This lesson provides a deep dive into the essential performance metrics for classification.",
                "sections": [
                    {
                        "title": "The Problem with Accuracy and Class Imbalance",
                        "content": "<p>In the real world, the vast majority of media is authentic. A dataset might have 999 real videos for every 1 fake video. This is a <strong>class imbalance</strong> problem.</p><p>A naive model could achieve 99.9% accuracy by simply guessing 'real' every single time. It would be 99.9% accurate, but it would be completely useless because it would never detect a single fake. This is why accuracy is a misleading metric for imbalanced problems.</p>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Precision, Recall, and the Confusion Matrix",
                        "content": "<p>To get a better picture, we use a <strong>confusion matrix</strong> and the metrics derived from it:</p><ul><li><strong>True Positives (TP):</strong> Fake videos correctly identified as fake.</li><li><strong>False Positives (FP):</strong> Real videos incorrectly identified as fake (false alarms).</li><li><strong>True Negatives (TN):</strong> Real videos correctly identified as real.</li><li><strong>False Negatives (FN):</strong> Fake videos that the model missed.</li></ul><p>From these, we calculate:<ul><li><strong>Precision:</strong> How many of the 'fake' alerts were actually fakes? <code>TP / (TP + FP)</code>. High precision is needed to avoid analyst fatigue.</li><li><strong>Recall (Sensitivity):</strong> Of all the actual fakes, how many did we catch? <code>TP / (TP + FN)</code>. High recall is needed to catch as many threats as possible.</li></ul></p>",
                        "image": "https://i.imgur.com/8a6R2aU.png"
                    },
                    {
                        "title": "AUC Score",
                        "content": "<p>There is often a trade-off between precision and recall. The <strong>AUC (Area Under the ROC Curve)</strong> is a single metric that summarizes a classifier's performance across all possible classification thresholds. The ROC curve plots the True Positive Rate (Recall) against the False Positive Rate.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>A Holistic Measure</strong></div><p>An AUC score ranges from 0.5 (a random guess) to 1.0 (a perfect classifier). It measures the model's ability to distinguish between the positive and negative classes, regardless of the specific threshold chosen. It is a very common and robust metric for evaluating binary classifiers on imbalanced data. [9]</p></div>",
                        "image": "https://i.imgur.com/kYq3Q6k.png"
                    }
                ]
            },
            "codeExamples": [
                {
                    "title": "Calculating_and_Interpreting_AUC_Scores_for_a_Detector.py",
                    "language": "python",
                    "code": "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\nimport matplotlib.pyplot as plt\n\n# Assume y_true contains the true labels (0 for real, 1 for fake)\n# Assume y_pred_proba contains the model's predicted probabilities for the 'fake' class\n\n# --- Precision and Recall ---\n# We first need to choose a threshold to make a binary decision\nthreshold = 0.5\ny_pred_binary = (y_pred_proba > threshold).astype(int)\nprint(classification_report(y_true, y_pred_binary))\n\n# --- AUC Score ---\nauc_score = roc_auc_score(y_true, y_pred_proba)\nprint(f\"\\nAUC Score: {auc_score:.4f}\")\n\n# --- Plot ROC Curve ---\nfpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\nplt.plot(fpr, tpr, label=f'Model (AUC = {auc_score:.2f})')\nplt.plot([0, 1], [0, 1], 'k--') # Dashed line for random guesser\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()"
                }
            ],
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "Why is 'accuracy' a poor metric for evaluating a deepfake detector on real-world data?",
                        "options": [
                            "It is too hard to calculate.",
                            "Real-world data is highly imbalanced, and a model can achieve high accuracy by always guessing the majority class ('real').",
                            "It is not a standard metric.",
                            "It is the same as precision."
                        ],
                        "correct": 1,
                        "explanation": "Class imbalance makes accuracy a misleading metric. A model that never catches a fake can still have >99% accuracy, making it useless in practice."
                    },
                    {
                        "id": 2,
                        "question": "A model's ability to find most of the actual deepfakes is measured by which metric?",
                        "options": [
                            "Precision",
                            "Accuracy",
                            "Recall (True Positive Rate)",
                            "False Positive Rate"
                        ],
                        "correct": 2,
                        "explanation": "Recall answers the question: 'Of all the fakes that exist, what fraction did our model successfully identify?'. A high recall is critical for minimizing missed threats."
                    },
                    {
                        "id": 3,
                        "question": "What does an AUC score of 0.5 typically represent?",
                        "options": [
                            "A perfect classifier.",
                            "A classifier that is no better than a random guess.",
                            "A very good classifier.",
                            "A classifier that always predicts 'fake'."
                        ],
                        "correct": 1,
                        "explanation": "The AUC score measures the model's ability to separate the classes. An AUC of 0.5 means the model has no separation power and is equivalent to flipping a coin. A perfect classifier has an AUC of 1.0. [9]"
                    }
                ]
            }
        },
        {
            "id": "lesson-23",
            "title": "Building a Real-Time Detection System",
            "duration": "75 min",
            "objectives": [
                "Understand the architectural challenges of real-time detection [6]",
                "Analyze the trade-offs between latency and accuracy",
                "Explore the role of edge computing in reducing latency [14]",
                "Learn how to optimize a detection model for inference speed"
            ],
            "content": {
                "overview": "Detecting a deepfake in a pre-recorded video is one challenge. Detecting it in a live video stream, like a video conference, is another entirely. This lesson covers the architectural and performance challenges of building a real-time detection system, where decisions must be made in milliseconds.",
                "sections": [
                    {
                        "title": "Architectural Challenges",
                        "content": "<p>A real-time system has very strict performance requirements.</p><ul><li><strong>Low Latency:</strong> The total time from when a video frame is captured to when a prediction is made must be very low (e.g., under 100 milliseconds) to be useful in a live conversation.</li><li><strong>High Throughput:</strong> The system must be able to process a continuous stream of frames (e.g., 30 frames per second) without falling behind.</li></ul><p>This often involves a trade-off. A very large, complex, and accurate model might be too slow for real-time inference. A smaller, faster model might fit the latency budget but be less accurate.</p>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Model Optimization for Inference",
                        "content": "<p>To meet these performance demands, detection models must be heavily optimized for speed.</p><ul><li><strong>Model Compression:</strong> Techniques like quantization and pruning are used to create smaller, faster versions of the model.</li><li><strong>Hardware Acceleration:</strong> The model is run on specialized hardware like GPUs or TPUs that can perform AI computations much faster than a CPU.</li><li><strong>Efficient Architectures:</strong> Using lightweight model architectures (like MobileNet) that are specifically designed for high-speed inference.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Role of Edge Computing",
                        "content": "<p>Sending a live video stream to the cloud for analysis, and waiting for the result, can introduce significant network latency. <strong>Edge computing</strong> is a solution where the AI model is run closer to the source of the data.</p><div class='info-box note'><div class='info-box-header'><i class='fas fa-info-circle'></i><strong>On-Device Detection</strong></div><p>For a video conferencing application, a small, highly optimized deepfake detection model could run directly on the user's laptop or mobile phone. This allows for near-instantaneous detection, as there is no network round-trip. This on-device (edge) model could then escalate potential fakes to a more powerful cloud-based model for a second, more thorough analysis. [14]</p></div>",
                        "image": "https://images.unsplash.com/photo-1587560699334-cc426240a24a?w=800&h=400&fit=crop"
                    }
                ]
            },
            "codeExamples": [
                {
                    "title": "Real_Time_Deepfake_Detection_from_Webcam_Feed.py",
                    "language": "python",
                    "code": "import cv2\nimport tensorflow as tf\n\n# Assume 'detection_model' is a pre-trained and OPTIMIZED model\n\ncap = cv2.VideoCapture(0) # Open the default webcam\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # 1. Preprocess the frame for the model\n    input_tensor = preprocess_frame(frame)\n\n    # 2. Get a prediction (this needs to be very fast!)\n    prediction_score = detection_model.predict(input_tensor)[0][0]\n    \n    # 3. Display the result on the frame\n    if prediction_score > 0.5:\n        label = f\"FAKE ({prediction_score:.2f})\"\n        color = (0, 0, 255) # Red\n    else:\n        label = f\"REAL ({prediction_score:.2f})\"\n        color = (0, 255, 0) # Green\n\n    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n    cv2.imshow(\"Deepfake Detection\", frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()"
                }
            ],
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary challenge when building a *real-time* deepfake detector?",
                        "options": [
                            "Finding enough training data.",
                            "The model must have very low latency and high throughput to keep up with a live video stream. [6]",
                            "The models are too simple.",
                            "The video quality is too high."
                        ],
                        "correct": 1,
                        "explanation": "In a real-time system, speed is as important as accuracy. The entire detection pipeline, from frame capture to prediction, must execute in a fraction of a second."
                    },
                    {
                        "id": 2,
                        "question": "What is the benefit of using 'edge computing' for real-time detection?",
                        "options": [
                            "It makes the model more accurate.",
                            "It increases network traffic.",
                            "It reduces network latency by running the model directly on the user's device, enabling faster decisions. [14]",
                            "It requires a more powerful cloud server."
                        ],
                        "correct": 2,
                        "explanation": "Edge AI is a key enabler for real-time applications. By eliminating the network round-trip to a cloud server, it can significantly reduce the overall detection latency."
                    },
                    {
                        "id": 3,
                        "question": "Techniques like model quantization and using lightweight architectures like MobileNet are used to do what?",
                        "options": [
                            "To make the model more explainable.",
                            "To optimize a model for high-speed, low-latency inference.",
                            "To increase the model's accuracy.",
                            "To make the model larger."
                        ],
                        "correct": 1,
                        "explanation": "These are performance optimization techniques. They are crucial for creating models that are small and efficient enough to run in real-time or on resource-constrained edge devices."
                    }
                ]
            }
        },
        {
            "id": "lesson-24",
            "title": "Integrating Commercial Detection APIs",
            "duration": "60 min",
            "objectives": [
                "Get an overview of the commercial deepfake detection landscape",
                "Learn how to integrate a third-party detection API into an application",
                "Analyze the pros and cons of using a commercial service vs. building in-house",
                "Compare the features of different commercial solutions [17, 18]"
            ],
            "content": {
                "overview": "Building and maintaining a state-of-the-art deepfake detection system is a massive undertaking. For many organizations, a more practical solution is to use a commercial, third-party detection service. This lesson provides an overview of the commercial landscape and how to integrate these services via an API.",
                "sections": [
                    {
                        "title": "The Commercial Landscape",
                        "content": "<p>A number of companies now offer deepfake detection as a cloud-based service. These services provide a simple API where a developer can submit a video or image and receive a score indicating the likelihood that it is a deepfake.</p><h3>Leading Providers:</h3><p>Companies like <strong>Sensity AI</strong>, <strong>Hive AI</strong>, and <strong>Deepware</strong> have developed sophisticated, proprietary detection models that are constantly updated to handle the latest generation techniques. [17, 18] They are often used by social media platforms, news organizations, and financial institutions for content moderation and fraud prevention.</p>",
                        "image": "https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Pros and Cons of Using an API",
                        "content": "<h3>Pros:</h3><ul><li><strong>Expertise:</strong> You get access to a state-of-the-art model built by a specialized team, without needing to hire your own AI researchers.</li><li><strong>Maintenance:</strong> The service provider is responsible for continuously updating the model to keep pace with new deepfake techniques.</li><li><strong>Simplicity:</strong> Integration is typically straightforward via a simple REST API.</li></ul><h3>Cons:</h3><ul><li><strong>Cost:</strong> These services are typically priced on a per-query basis, which can be expensive for high-volume applications.</li><li><strong>Privacy:</strong> You must be comfortable sending your data (the media to be analyzed) to a third-party service.</li><li><strong>Lack of Control:</strong> You have no control over the model's architecture or its detection threshold.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Making_a_Request_to_a_Deepfake_Detection_API.py",
                        "language": "python",
                        "code": "import requests\nimport base64\n\n# Conceptual code for calling a commercial detection API\n\nAPI_ENDPOINT = \"https://api.commercialdetector.com/v1/detect\"\nAPI_KEY = \"your_api_key_here\"\n\n\ndef detect_deepfake_with_api(filepath):\n    # Read the file and encode it as base64\n    with open(filepath, \"rb\") as f:\n        encoded_file = base64.b64encode(f.read()).decode('utf-8')\n\n    # Prepare the API request payload\n    payload = {\n        \"content\": encoded_file\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\"\n    }\n\n    try:\n        # Make the POST request\n        response = requests.post(API_ENDPOINT, json=payload, headers=headers)\n        response.raise_for_status() # Raise an exception for bad status codes\n        \n        # Return the JSON result\n        return response.json()\n    except requests.exceptions.RequestException as e:\n        print(f\"API request failed: {e}\")\n        return None\n\n# Use the function\nresult = detect_deepfake_with_api(\"my_video.mp4\")\nif result:\n    # The result might look like: {'is_fake': True, 'confidence': 0.98}\n    print(f\"Detection result: {result}\")"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a primary advantage of using a commercial deepfake detection API?",
                        "options": [
                            "It is free to use.",
                            "You have full control over the model.",
                            "You get access to a state-of-the-art model that is maintained and updated by a specialized team. [17, 18]",
                            "You do not need to send your data to a third party."
                        ],
                        "correct": 2,
                        "explanation": "The main value proposition is outsourcing the deep AI expertise. You are paying to access a model that is likely more advanced and up-to-date than one you could build and maintain in-house."
                    },
                    {
                        "id": 2,
                        "question": "What is a major potential drawback of using a third-party detection API?",
                        "options": [
                            "The models are not very accurate.",
                            "The API is too easy to integrate.",
                            "Data privacy concerns, as you must send your content to the third-party provider for analysis.",
                            "The models are never updated."
                        ],
                        "correct": 2,
                        "explanation": "For applications involving sensitive or private media, the requirement to upload the data to an external service can be a significant privacy and security concern."
                    },
                    {
                        "id": 3,
                        "question": "For which type of organization is using a commercial API the most practical solution?",
                        "options": [
                            "A large tech company with its own AI research lab.",
                            "An academic researcher developing new detection techniques.",
                            "An organization that needs a reliable detection capability but does not have a dedicated in-house team of AI security researchers.",
                            "An organization that needs full control over the detection model."
                        ],
                        "correct": 2,
                        "explanation": "Commercial APIs are ideal for organizations that want to integrate a 'good enough' or state-of-the-art detection solution into their products without making the massive investment required to build one from scratch."
                    }
                ]
            }
        },
        {
            "id": "lesson-25",
            "title": "Open-Source Detection Platforms",
            "duration": "60 min",
            "objectives": [
                "Explore the landscape of open-source tools for deepfake detection",
                "Understand the role of platforms that centralize and test different models",
                "Learn how to use an open-source platform to benchmark a model's performance",
                "Discuss the benefits of contributing to open-source detection projects [19, 20]"
            ],
            "content": {
                "overview": "The fight against deepfakes is a collaborative effort. This lesson explores the world of open-source deepfake detection, focusing on the platforms and tools that allow researchers to share code, benchmark models, and work together to advance the state of the art.",
                "sections": [
                    {
                        "title": "The Importance of Open Source",
                        "content": "<p>Open-source software is critical for deepfake detection research for several reasons:</p><ul><li><strong>Reproducibility:</strong> It allows researchers to independently verify the results of a published paper by running the code themselves.</li><li><strong>Collaboration:</strong> It provides a common codebase that researchers can build upon, preventing everyone from having to reinvent the wheel.</li><li><strong>Benchmarking:</strong> It enables the creation of standardized platforms for fairly comparing the performance of different detection models.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1515879218367-8466d910aaa4?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Open-Source Platforms and Tools",
                        "content": "<p>A number of open-source projects have been created to support the detection community.</p><p>These platforms often provide a 'model zoo'—a collection of pre-trained implementations of many different detection algorithms from well-known academic papers. They also provide standardized scripts for testing these models on common datasets like FaceForensics++. This makes it much easier for a new researcher to get started and to benchmark their own new model against the current state of the art. [19, 20]</p>",
                        "image": "https://images.unsplash.com/photo-1555949963-aa79dcee981c?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Using_DeepSafe_to_Benchmark_Detection_Models.py",
                        "language": "python",
                        "code": "# DeepSafe is an example of an open-source platform.\n# This is a conceptual example of how such a tool might be used from the command line.\n\n# --- Training a model using the platform's tools ---\n# The platform provides a standardized way to train a model on a common dataset\npython train.py --model_name MesoNet --dataset FaceForensics++ --output_path ./mesonet_model.pth\n\n# --- Evaluating a model ---\n# The platform provides a standardized evaluation script\n# This ensures that all models are benchmarked in exactly the same way\npython evaluate.py --model_path ./mesonet_model.pth --dataset Celeb-DF\n\n# --- Expected Output ---\n# Evaluating model MesoNet on dataset Celeb-DF...\n# Accuracy: 0.6521\n# AUC: 0.6890\n# ... and other metrics"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a primary benefit of open-source software in deepfake research?",
                        "options": [
                            "It is always more secure than closed-source software.",
                            "It enables reproducibility, allowing other researchers to verify and build upon published work. [19]",
                            "It is always faster than commercial software.",
                            "It comes with professional customer support."
                        ],
                        "correct": 1,
                        "explanation": "Reproducibility is a cornerstone of the scientific method. Open source allows for the transparency and validation that is essential for building a trustworthy body of research."
                    },
                    {
                        "id": 2,
                        "question": "What is a 'model zoo' in the context of a detection platform?",
                        "options": [
                            "A collection of images of animals.",
                            "A collection of pre-trained implementations of various detection models from different research papers.",
                            "A physical server room.",
                            "A list of commercial detection APIs."
                        ],
                        "correct": 1,
                        "explanation": "A model zoo provides a valuable resource for researchers, giving them easy access to a wide range of baseline models to test and compare against."
                    },
                    {
                        "id": 3,
                        "question": "How do open-source benchmarking platforms help to advance the field?",
                        "options": [
                            "By keeping the best models secret.",
                            "By providing a standardized and fair environment to compare the performance of different detection methods. [20]",
                            "By making deepfakes.",
                            "By charging a fee for their use."
                        ],
                        "correct": 1,
                        "explanation": "Standardized benchmarking is crucial for measuring progress. It allows the community to objectively identify which new techniques are actually advancing the state of the art."
                    }
                ]
            }
        },
        {
            "id": "lesson-26",
            "title": "Legal and Ethical Considerations",
            "duration": "60 min",
            "objectives": [
                "Discuss existing and proposed legislation around deepfakes [14]",
                "Understand the ethical responsibilities of researchers and developers",
                "Learn the principles of responsible disclosure",
                "Analyze the ethical dilemmas of creating deepfakes for research"
            ],
            "content": {
                "overview": "The development and use of deepfake technology is fraught with complex legal and ethical challenges. This lesson moves beyond the technical details to explore the societal context, from new legislation aimed at curbing malicious fakes to the ethical responsibilities of the researchers who build and fight against them.",
                "sections": [
                    {
                        "title": "The Legal Landscape",
                        "content": "<p>Governments around the world are grappling with how to regulate deepfakes. Legislation is emerging that aims to criminalize the creation and distribution of malicious deepfakes, particularly those related to election interference, non-consensual pornography, and fraud.</p><p>However, this is a delicate balancing act, as lawmakers must also protect freedom of speech and the legitimate, artistic uses of synthetic media. The legal landscape is evolving rapidly and varies significantly by jurisdiction. [14]</p>",
                        "image": "https://images.unsplash.com/photo-1556742502-ec7c0e9f34b1?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Ethical Responsibilities and Responsible Disclosure",
                        "content": "<p>Researchers and developers in this field have a significant ethical responsibility.</p><ul><li><strong>Responsible Disclosure:</strong> If a researcher finds a vulnerability in a commercial detection product, they have a responsibility to privately disclose it to the vendor before publishing it, giving them time to fix the issue.</li><li><strong>Data Privacy:</strong> Researchers must handle the sensitive biometric data in deepfake datasets with extreme care to protect the privacy of the individuals depicted.</li><li><strong>Preventing Misuse:</strong> Developers of generative models must consider the potential for their technology to be misused and build in appropriate safeguards.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Ethics of Creating Deepfakes for Research",
                        "content": "<p>To build a better detector, you often need to create better fakes to train it on. This creates an ethical dilemma.</p><div class='info-box warning'><div class='info-box-header'><i class='fas fa-exclamation-triangle'></i><strong>The Dual-Use Problem</strong></div><p>A researcher who creates a new, more realistic deepfake generator to test their defenses has also created a new tool that could be used by malicious actors. The ethical framework in the research community requires a careful balancing of the benefits of improved detection against the risks of advancing the state of malicious generation. This often involves keeping the code for new generative models private while publishing the details of the detection method.</p></div>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a8a86a?w=800&h=400&fit=crop"
                    }
                ]
            },
            "codeExamples": [
                {
                    "title": "Ethical_Guidelines_for_Deepfake_Research.md",
                    "language": "markdown",
                    "code": "# Ethical Framework for Deepfake Research\n\n1.  **Clear Benefit:** The research should have a clear and significant defensive benefit (e.g., creating a more robust detector).\n2.  **Consent and Privacy:** Use only data for which you have consent. Anonymize data wherever possible. Use datasets of consenting actors rather than scraping public figures without permission.\n3.  **Risk of Misuse:** Carefully consider whether publishing a new generative model poses more risk than benefit. Consider withholding the code or data for the generative part of the research.\n4.  **Transparency:** Be transparent in your publications about the ethical considerations and the steps you have taken to mitigate risks.\n5.  **Responsible Disclosure:** If your research uncovers vulnerabilities in existing systems, follow responsible disclosure practices."
                }
            ],
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary challenge for lawmakers trying to regulate deepfakes?",
                        "options": [
                            "The technology is not very advanced.",
                            "Balancing the need to prevent malicious use with the protection of free speech and artistic expression. [14]",
                            "There are no malicious uses of deepfakes.",
                            "The technology is too expensive."
                        ],
                        "correct": 1,
                        "explanation": "This is a classic dilemma. A law that is too broad could stifle creativity and parody, while a law that is too narrow will be ineffective. Finding the right balance is a major legal challenge."
                    },
                    {
                        "id": 2,
                        "question": "The fact that a new generative model created for defensive research could also be used by attackers is an example of what?",
                        "options": [
                            "A bug in the model.",
                            "The 'dual-use' problem.",
                            "A successful outcome.",
                            "A legal requirement."
                        ],
                        "correct": 1,
                        "explanation": "The dual-use problem is a core ethical challenge in many scientific fields. It requires researchers to weigh the potential benefits of their work against the potential for misuse."
                    },
                    {
                        "id": 3,
                        "question": "What is 'responsible disclosure'?",
                        "options": [
                            "Publishing a vulnerability on social media immediately.",
                            "Keeping a vulnerability a secret forever.",
                            "Privately reporting a vulnerability to the affected vendor to give them time to fix it before you publish your findings.",
                            "Selling a vulnerability to an attacker."
                        ],
                        "correct": 2,
                        "explanation": "Responsible disclosure is a standard ethical framework in the cybersecurity community. It is designed to ensure that vulnerabilities get fixed and the public gets informed, all while minimizing the risk of the vulnerability being exploited by malicious actors."
                    }
                ]
            }
        },
        {
            "id": "lesson-27",
            "title": "Human-in-the-Loop Detection",
            "duration": "60 min",
            "objectives": [
                "Understand the principle of combining automated detection with human expertise [5]",
                "Learn how to design effective workflows for human review",
                "Analyze the strengths and weaknesses of human vs. machine detection",
                "Build a simple user interface for reviewing flagged content",
                "Discuss the future of human-AI collaboration in forensics"
            ],
            "content": {
                "overview": "Automated detectors are fast, but humans possess contextual understanding that machines lack. The most effective deepfake detection systems are not fully automated, but are 'human-in-the-loop' systems that combine the scalability of AI with the nuanced judgment of a human expert. This lesson explores the design of these collaborative systems.",
                "sections": [
                    {
                        "title": "Combining Human and Machine Strengths",
                        "content": "<p>AI and humans have complementary strengths and weaknesses in deepfake detection:</p><ul><li><strong>AI Strengths:</strong> Can analyze millions of videos at machine speed; can detect subtle, low-level artifacts that are invisible to the human eye.</li><li><strong>AI Weaknesses:</strong> Can be brittle and fail on unseen fakes; lacks a deep understanding of real-world context.</li><li><strong>Human Strengths:</strong> Possesses a lifetime of experience with what 'normal' human behavior and physics look like; can understand context, nuance, and intent.</li><li><strong>Human Weaknesses:</strong> Is slow, gets tired, and cannot spot subtle pixel-level inconsistencies.</li></ul><p>A <strong>human-in-the-loop</strong> system aims to get the best of both worlds. [5]</p>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Review Workflow",
                        "content": "<p>In such a system, the AI acts as a first-level filter and an assistant to the human analyst.</p><div class='info-box tip'><div class='info-box-header'><i class='fas fa-lightbulb'></i><strong>A Typical Workflow</strong></div><p><ol><li>The AI detector scans all incoming media and flags a small percentage of it as 'potentially fake', assigning a confidence score.</li><li>This flagged content is sent to a human review queue.</li><li>A human analyst reviews the content. The system helps them by providing an explainable AI output (like a heatmap) that highlights the specific artifacts the model found suspicious.</li><li>The analyst makes the final determination.</li><li>This final decision is then fed back into the system to retrain and improve the AI model over time.</li></ol></p></div>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a_8a86a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Building_a_Simple_UI_for_Human_Review_of_Flagged_Content.py",
                        "language": "python",
                        "code": "# This example uses Gradio, a simple library for creating web UIs for ML models.\nimport gradio as gr\n\n# Assume 'detect_and_explain(video)' is a function that returns a score\n# and an explanation heatmap video.\n\ndef review_interface(video_file):\n    # The input 'video_file' comes from the Gradio UI upload.\n    if video_file is None:\n        return None, None, \"Please upload a video.\"\n\n    # Run our detection and explanation model\n    score, explanation_video = detect_and_explain(video_file.name)\n\n    if score > 0.8:\n        status = f\"HIGH RISK (Score: {score:.2f}) - Recommended for review.\"\n    else:\n        status = f\"Low Risk (Score: {score:.2f}) - Likely authentic.\"\n    \n    return explanation_video, None, status\n\n# Create the Gradio interface\niface = gr.Interface(\n    fn=review_interface,\n    inputs=gr.Video(label=\"Upload Video for Analysis\"),\n    outputs=[\n        gr.Video(label=\"Explanation Heatmap\"),\n        gr.Textbox(label=\"Analyst Notes\"),\n        gr.Textbox(label=\"AI Assessment\")\n    ],\n    title=\"Human-in-the-Loop Deepfake Review\"\n)\n\niface.launch()"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary motivation for a 'human-in-the-loop' detection system?",
                        "options": [
                            "To fully automate the detection process.",
                            "To combine the speed and scale of AI with the contextual understanding and judgment of a human expert. [5]",
                            "To replace the need for AI.",
                            "To make the detection process slower."
                        ],
                        "correct": 1,
                        "explanation": "This approach recognizes that both AI and humans have unique strengths, and it aims to create a collaborative system that is more effective than either one alone."
                    },
                    {
                        "id": 2,
                        "question": "In a typical human-in-the-loop workflow, what is the role of the AI model?",
                        "options": [
                            "To make the final, irreversible decision.",
                            "To act as a first-level filter, flagging suspicious content for a human analyst to review.",
                            "To review the decisions made by the human.",
                            "To have no role."
                        ],
                        "correct": 1,
                        "explanation": "The AI is used to manage the scale of the problem. It sifts through the massive volume of content to find the small number of items that are worthy of a human expert's valuable time."
                    },
                    {
                        "id": 3,
                        "question": "How does the human analyst's decision improve the AI model over time?",
                        "options": [
                            "It doesn't.",
                            "The analyst's final judgment ('this was a real fake' or 'this was a false positive') is used as new, high-quality labeled data to retrain and improve the model.",
                            "The analyst manually updates the model's code.",
                            "The analyst tells the model what to do."
                        ],
                        "correct": 1,
                        "explanation": "This feedback loop is crucial for creating an adaptive system. The AI learns from its mistakes as corrected by the human expert, leading to a smarter and more accurate model over time."
                    }
                ]
            }
        },
        {
            "id": "lesson-28",
            "title": "Awareness Training and 'Secure Humans'",
            "duration": "60 min",
            "objectives": [
                "Understand the importance of training employees and the public to be critical of media [2]",
                "Learn the principles of security awareness training",
                "Explore how to encourage critical thinking to question unusual requests [2]",
                "Design a conceptual deepfake awareness training module"
            ],
            "content": {
                "overview": "Technology is only part of the solution. The ultimate line of defense against deepfake-powered fraud and disinformation is a skeptical, well-informed human. This lesson focuses on the non-technical side of defense: security awareness training designed to create a resilient 'human firewall'.",
                "sections": [
                    {
                        "title": "The Importance of Awareness Training",
                        "content": "<p>Even the best detector can be fooled. The goal of awareness training is to equip people with the knowledge and critical thinking skills to spot the signs of a deepfake and, more importantly, to question unusual requests regardless of the medium. [2]</p><p>An employee who receives an urgent voice call from their 'CEO' asking for a wire transfer should be trained to be skeptical by default and to follow a strict out-of-band verification process, even if the voice sounds perfectly real.</p>",
                        "image": "https://images.unsplash.com/photo-1521791136064-7986c2920216?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Key Training Points",
                        "content": "<p>An effective deepfake awareness module should teach people to look for:</p><ul><li><strong>Technical Flaws:</strong> Teach the common visual and audio artifacts (unnatural blinking, flickering, robotic voice) so people know what to look for.</li><li><strong>Contextual Red Flags:</strong> The context is often more important than the content. Is the request unusual or unexpected? Is there a sense of extreme urgency? Is the person asking you to bypass a normal security procedure?</li><li><strong>The Verification Habit:</strong> The single most important lesson. For any sensitive request (financial, passwords, data access) that comes via an un-trusted channel, always verify the request through a different, trusted channel. If you get a suspicious email, call the person on their known phone number. If you get a suspicious voicemail, contact them via a verified internal chat.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1573495627361-ab2d_50a_8a86a?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Phishing_Simulation_with_Deepfake_Voicemail.md",
                        "language": "markdown",
                        "code": "# Phishing Simulation Campaign Plan: Deepfake Vishing\n\n**Objective:** Test employee resilience to a deepfake-powered CEO fraud attempt.\n\n**Methodology:**\n1.  **Target Selection:** A small, controlled group of employees in the finance department.\n2.  **Voice Clone Creation:** Use a commercial voice cloning tool to create a clone of the CEO's voice from a public speech.\n3.  **Scenario:** The cloned voice will be used to leave a voicemail on the target's work phone. The message will create a sense of urgency (e.g., 'I'm about to board a flight, I need you to process this invoice for a new vendor immediately, I've emailed you the details').\n4.  **Phishing Email:** The corresponding email will contain a link to a fake invoice page.\n5.  **Measurement:** Track how many employees click the link and how many report the incident to the security team without clicking. The goal is to maximize reporting.\n6.  **Follow-up:** All participants receive immediate feedback and a link to a refresher training module."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary goal of deepfake awareness training?",
                        "options": [
                            "To teach people how to make deepfakes.",
                            "To train people to be critical of media and to verify unusual requests through a separate channel. [2]",
                            "To replace the need for technical detectors.",
                            "To fulfill a compliance requirement."
                        ],
                        "correct": 1,
                        "explanation": "The goal is to build a 'human firewall'. A well-trained, skeptical employee is the best line of defense against social engineering attacks, including those that use deepfakes."
                    },
                    {
                        "id": 2,
                        "question": "What is the single most effective procedure to teach employees for dealing with a suspicious, urgent request from their 'CEO'?",
                        "options": [
                            "To immediately do what the CEO says.",
                            "To ignore the request.",
                            "To forward the request to a colleague.",
                            "To attempt to verify the request through a different, trusted communication channel (like an internal chat or a known phone number)."
                        ],
                        "correct": 3,
                        "explanation": "Out-of-band verification is the gold standard for defeating social engineering. It breaks the attacker's chain of impersonation and is a simple, effective habit to teach."
                    },
                    {
                        "id": 3,
                        "question": "Which of the following is a 'contextual red flag' that an employee should be trained to spot?",
                        "options": [
                            "A video that looks slightly blurry.",
                            "A request that creates an extreme sense of urgency and asks the employee to bypass a standard security policy.",
                            "An email that has perfect grammar.",
                            "A phone call that comes from the correct number."
                        ],
                        "correct": 1,
                        "explanation": "Attackers use psychology. Urgency and pressure are classic social engineering tactics designed to make a victim act before they think. Training people to recognize these psychological tricks is crucial."
                    }
                ]
            }
        },
        {
            "id": "lesson-29",
            "title": "The Future of Generation and Detection",
            "duration": "60 min",
            "objectives": [
                "Explore the next wave of generative models",
                "Understand the shift towards multimodal, real-time, and explainable detection [14]",
                "Discuss the long-term arms race between generation and detection",
                "Brainstorm future deepfake threats and potential defenses",
                "Analyze the societal impact of widespread synthetic media"
            ],
            "content": {
                "overview": "The field of generative AI is advancing at an incredible pace. This final technical lesson looks to the future, exploring the next generation of synthesis techniques and the corresponding evolution of detection methods that will be required to keep pace in this perpetual arms race.",
                "sections": [
                    {
                        "title": "The Future of Generation",
                        "content": "<p>Deepfake generation will continue to improve, becoming faster, more realistic, and more accessible.</p><ul><li><strong>Real-time Generation:</strong> High-fidelity, real-time voice and video deepfakes will become commonplace, posing a major threat to live communication channels.</li><li><strong>Full-Body Synthesis:</strong> Models will move beyond just the face to generate entire, realistic human avatars.</li><li><strong>Multimodal Generation:</strong> Future models will be able to generate video, audio, and text simultaneously, ensuring all modalities are perfectly consistent and making detection much harder.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1614036125191-dd293108a_3d_3?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "The Future of Detection",
                        "content": "<p>As generative models get better, detectors will need to become smarter. The future of detection will be:</p><ul><li><strong>Multimodal:</strong> Relying on a single signal will be too brittle. Detectors will have to fuse signals from audio, video, physiology, and network context to make a robust decision. [14]</li><li><strong>Real-time:</strong> The threat of live deepfakes requires detectors that can operate with millisecond latency. [14]</li><li><strong>Explainable:</strong> As the stakes get higher, the need for transparent, explainable AI that can show its evidence will become a fundamental requirement. [14]</li><li><strong>Proactive:</strong> A greater emphasis will be placed on proactive defenses like digital provenance (C2PA) to provide a reliable way to verify authenticity at the source.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1533750088891-3b3b55227581?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Research_Paper_Review_Latest_in_Generative_Models.md",
                        "language": "markdown",
                        "code": "# Review of 'Sora: Creating Video from Text' (OpenAI, 2024)\n\n**Key Capabilities:**\n- Generates high-fidelity, minute-long videos from text prompts.\n- Demonstrates a sophisticated understanding of physics and object interaction within the generated world.\n- Capable of generating video in a wide variety of artistic styles.\n\n**Implications for Detection:**\n- **Artifacts:** Traditional, low-level compression artifacts are largely absent. The model shows a high degree of temporal consistency, making flicker detection difficult.\n- **New Weaknesses:** The model still struggles with complex physics and cause-and-effect over long durations. It may generate physically implausible scenarios.\n- **Future Direction:** Detection will need to move away from pixel-level artifacts and towards a higher-level, semantic understanding of whether the content of the video is physically and logically coherent."
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is a key trend in the future of deepfake generation?",
                        "options": [
                            "The models will become less realistic.",
                            "The models will become slower and harder to use.",
                            "The models will become multimodal, generating consistent audio, video, and text simultaneously.",
                            "The technology will become less accessible."
                        ],
                        "correct": 2,
                        "explanation": "Future generative models will be able to create entire, consistent scenes, which will make detection based on single-modality inconsistencies (like lip-sync) much more difficult."
                    },
                    {
                        "id": 2,
                        "question": "As deepfakes become more realistic, what will future detection systems likely need to do?",
                        "options": [
                            "Rely on a single, simple detection method.",
                            "Focus only on low-level pixel artifacts.",
                            "Become more multimodal, fusing signals from many different sources to make a robust decision. [14]",
                            "Ignore the problem."
                        ],
                        "correct": 2,
                        "explanation": "A defense-in-depth approach will be necessary. By cross-referencing many different signals (visual, audio, physiological, provenance), the system as a whole can be more resilient than any single detector."
                    },
                    {
                        "id": 3,
                        "question": "Why is 'proactive defense' like the C2PA content provenance standard so important for the future?",
                        "options": [
                            "It is not important.",
                            "Because as detection becomes harder, the best strategy may be to focus on proving what is *real* rather than trying to detect everything that is *fake*.",
                            "It is a type of deepfake generator.",
                            "It makes detection easier."
                        ],
                        "correct": 1,
                        "explanation": "Provenance flips the problem on its head. It aims to create a world where authentic content is cryptographically signed and verifiable. In such a world, any content that lacks this verifiable provenance can be treated with a high degree of suspicion by default."
                    }
                ]
            }
        },
        {
            "id": "lesson-30",
            "title": "Capstone Project",
            "duration": "120 min",
            "objectives": [
                "Apply the knowledge gained throughout the course to a practical problem",
                "Design a novel deepfake detection system",
                "Train the system on a challenging, real-world dataset",
                "Thoroughly evaluate the system's performance using appropriate metrics",
                "Present the final project, including its strengths, weaknesses, and potential for future improvement"
            ],
            "content": {
                "overview": "This final lesson is the capstone project, where you will bring together everything you have learned to build, train, and evaluate your own deepfake detector. This is your opportunity to tackle a real-world problem, experiment with different techniques, and create a portfolio piece that demonstrates your expertise in this field.",
                "sections": [
                    {
                        "title": "Project Definition",
                        "content": "<p>The goal of the capstone project is to design and implement a complete, end-to-end deepfake detection system. Students will be provided with a challenging dataset (e.g., a subset of the DFDC or Celeb-DF) and will be expected to:</p><ol><li>Perform exploratory data analysis to understand the dataset.</li><li>Design a detection model. This could be based on a single modality or a multimodal approach. Students are encouraged to be creative and to try combining different techniques learned in the course.</li><li>Train the model, being careful to use a proper validation set to tune hyperparameters.</li><li>Evaluate the final model on a hold-out test set, using the appropriate metrics (Accuracy, Precision, Recall, AUC).</li><li>Write a final report that describes your system, presents your results, and analyzes its performance, including a discussion of its limitations and its performance on cross-dataset generalization.</li></ol>",
                        "image": "https://images.unsplash.com/photo-1510915228340-29c85a43dcfe?w=800&h=400&fit=crop"
                    },
                    {
                        "title": "Example Project Ideas",
                        "content": "<ul><li>A CNN-based detector that focuses on a specific type of visual artifact.</li><li>A multimodal detector that fuses lip-sync analysis with head pose analysis.</li><li>An explainable detector that uses Grad-CAM to visualize its findings.</li><li>A comparative study that benchmarks the performance of several different pre-trained models on a challenging dataset.</li></ul>",
                        "image": "https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=400&fit=crop"
                    }
                ],
                "codeExamples": [
                    {
                        "title": "Capstone_Project_End_to_End_Deepfake_Detector.py",
                        "language": "python",
                        "code": "# This file will contain the complete, well-documented code for your final project.\n# It should be structured logically:\n\n# 1. Imports\nimport numpy as np\nimport tensorflow as tf\n# ... and so on\n\n# 2. Data Loading and Preprocessing Functions\ndef load_data(path):\n    # ...\n    pass\n\ndef preprocess(data):\n    # ...\n    pass\n\n# 3. Model Definition\ndef build_my_detector_model():\n    # ... your model architecture ...\n    return model\n\n# 4. Main script logic\nif __name__ == '__main__':\n    # Load data\n    X_train, y_train, X_test, y_test = load_data(...)\n\n    # Preprocess data\n    X_train_processed = preprocess(X_train)\n\n    # Build and compile model\n    my_detector = build_my_detector_model()\n\n    # Train model\n    my_detector.fit(X_train_processed, y_train, ...)\n\n    # Evaluate model\n    results = my_detector.evaluate(preprocess(X_test), y_test)\n    print(f\"Final Test Results: {results}\")\n\n    # Save model\n    my_detector.save('my_deepfake_detector.h5')"
                    }
                ]
            },
            "quiz": {
                "passingScore": 75,
                "questions": [
                    {
                        "id": 1,
                        "question": "What is the primary purpose of a capstone project?",
                        "options": [
                            "To review the content of a single lesson.",
                            "To provide a large, hands-on project that allows a student to apply the knowledge and skills from the entire course.",
                            "To take a final exam.",
                            "To learn a new topic."
                        ],
                        "correct": 1,
                        "explanation": "A capstone project is a culminating experience that integrates all the concepts learned throughout the curriculum into a single, comprehensive project."
                    },
                    {
                        "id": 2,
                        "question": "Which of the following is a critical part of the capstone project's evaluation?",
                        "options": [
                            "Only the final accuracy score matters.",
                            "A thorough evaluation of the model's performance using multiple, appropriate metrics (like AUC and recall) and an analysis of its limitations.",
                            "The visual design of the final report.",
                            "How fast the model trains."
                        ],
                        "correct": 1,
                        "explanation": "A good project is not just about getting a high score. It's about a deep and honest analysis of the system's performance, including understanding where it succeeds and where it fails."
                    },
                    {
                        "id": 3,
                        "question": "What is the final step in the capstone project?",
                        "options": [
                            "Training the model.",
                            "Choosing a dataset.",
                            "Writing the code.",
                            "Presenting the final system, including the methodology, results, and a critical analysis of its strengths and weaknesses."
                        ],
                        "correct": 3,
                        "explanation": "Communicating your work is just as important as doing it. A final report and presentation are essential for demonstrating what you have built and what you have learned."
                    }
                ]
            }
        }
    ]
}

      // =====================================================
      // GLOBAL VARIABLES
      // =====================================================
      let currentUser = null;
      let currentLessonIndex = 0;
      let courseProgress = {};
      let userStats = {};
      let quizState = {
        currentQuestion: 0,
        answers: [],
        score: 0,
        isComplete: false,
      };

      // Enhanced session tracking
      let courseSession = {
        startTime: null,
        totalStudyTime: 0,
        lessonsStarted: 0,
        lessonsCompleted: 0,
        pageLoadTime: new Date(),
        interactionCount: 0,
        lastActivityTime: new Date(),
      };

      // Music Player Variables
      let audioPlayer = new Audio();

      // Achievement notification system
      let notificationQueue = [];
      let isShowingNotification = false;

      // =====================================================
      // INITIALIZATION
      // =====================================================
      document.addEventListener("DOMContentLoaded", async () => {
        try {
          showLoadingScreen();
          await checkAuth();

          if (currentUser) {
            // Initialize session tracking
            startCourseSession();

            // Load course data and progress
            await loadCourseData();
            await loadUserProfile();
            await loadProgress();

            // Initialize UI
            initializeEventListeners();
            renderSidebar();
            loadLesson(currentLessonIndex);

            // Initialize additional features
            initMusicPlayer();
            addMusicIndicator();
            initializeActivityTracking();

            hideLoadingScreen();
          } else {
            openAuthModal();
          }
        } catch (error) {
          console.error("Error initializing course:", error);
          hideLoadingScreen();
          showToast("Failed to initialize course system", "error");
        }
      });

      // =====================================================
      // AUTHENTICATION SYSTEM
      // =====================================================
      async function checkAuth() {
        try {
          const {
            data: { session },
            error,
          } = await supabase.auth.getSession();

          if (error) {
            console.error("Auth error:", error);
            currentUser = null;
            openAuthModal();
            return;
          }

          if (!session) {
            currentUser = null;
            openAuthModal();
            return;
          }

          currentUser = session.user;
          updateUIWithUser();
        } catch (error) {
          console.error("Error checking auth:", error);
          currentUser = null;
          openAuthModal();
        }
      }

      function updateUIWithUser() {
        if (!currentUser) return;

        const name =
          currentUser.user_metadata?.full_name ||
          currentUser.email.split("@")[0];
        const userElements = document.querySelectorAll("[data-user-name]");
        userElements.forEach((el) => (el.textContent = name));
      }

      // =====================================================
      // USER PROFILE & STATS MANAGEMENT
      // =====================================================
      async function loadUserProfile() {
        try {
          // 🔍 Step 1: Try to fetch existing profile
          const { data: profile, error } = await supabase
            .from("profiles")
            .select("*")
            .eq("id", currentUser.id)
            .single();

          if (error && error.code !== "PGRST116") {
            throw error;
          }

          if (!profile) {
            // 🆕 Step 2: Create new profile if missing
            await createUserProfile();
          } else {
            // ✅ Step 3: Use existing profile
            userStats = profile;
            await updateLastActivity();
          }
        } catch (error) {
          console.error("❌ Error loading user profile:", error);
          showToast("Failed to load user profile", "error");
        }
      }
      async function createUserProfile() {
        try {
          const newProfile = {
            id: currentUser.id,
            full_name: currentUser.user_metadata?.full_name || "",
            email: currentUser.email,
            level: "Script Kiddie",
            total_points: 0,
            completed_courses: 0,
            current_streak: 0,
            total_certificates: 0,
            total_achievements: 0,
            sections_visited: 0,
            bonus_points: 0,
            in_progress_courses: 0,
            settings_changed: 0,
            midnight_sessions: 0,
            early_sessions: 0,
            weekend_sessions: 0,
            last_activity: new Date().toISOString(),
            created_at: new Date().toISOString(),
          };

          const { error } = await supabase
            .from("profiles")
            .insert([newProfile]);

          if (!error) {
            userStats = newProfile;
            // Award first login achievement
            setTimeout(() => checkAndUnlockAchievement("first_login"), 1000);
          }
        } catch (error) {
          console.error("Error creating user profile:", error);
        }
      }

      async function updateLastActivity() {
        try {
          const now = new Date();

          // Update activity tracking
          courseSession.lastActivityTime = now;

          // Update database
          await supabase
            .from("profiles")
            .update({
              last_activity: now.toISOString(),
            })
            .eq("id", currentUser.id);

          // Check time-based achievements
          await checkTimeBasedAchievements(now);
        } catch (error) {
          console.error("Error updating last activity:", error);
        }
      }

      // =====================================================
      // COURSE SESSION MANAGEMENT
      // =====================================================
      function startCourseSession() {
        courseSession.startTime = new Date();
        courseSession.pageLoadTime = new Date();

        logUserActivity("course_session_start", {
          course_id: COURSE_DATA.id,
          course_title: COURSE_DATA.title,
          user_agent: navigator.userAgent,
          screen_resolution: `${screen.width}x${screen.height}`,
        });

        // Check daily streak
        checkDailyStreak();
      }

      function initializeActivityTracking() {
        // Track page focus/blur for accurate study time
        document.addEventListener("visibilitychange", handleVisibilityChange);

        // Track user interactions
        ["click", "keydown", "scroll", "mousemove"].forEach((event) => {
          document.addEventListener(event, trackUserInteraction, {
            passive: true,
          });
        });

        // Periodic activity updates
        setInterval(updateStudyTime, 60000); // Every minute

        // Save session data before page unload
        window.addEventListener("beforeunload", saveSessionData);
      }

      function handleVisibilityChange() {
        if (document.hidden) {
          courseSession.lastActivityTime = new Date();
        } else {
          // Page became visible again - update activity
          updateLastActivity();
        }
      }

      function trackUserInteraction() {
        courseSession.interactionCount++;
        courseSession.lastActivityTime = new Date();

        // Throttle activity updates
        if (courseSession.interactionCount % 50 === 0) {
          updateLastActivity();
        }
      }

      function updateStudyTime() {
        if (!courseSession.startTime || document.hidden) return;

        const now = new Date();
        const sessionDuration = now - courseSession.startTime;
        courseSession.totalStudyTime = Math.floor(sessionDuration / 1000 / 60); // in minutes

        // Update progress with study time
        saveProgress();
      }

      function saveSessionData() {
        if (!currentUser || !courseSession.startTime) return;

        const sessionData = {
          user_id: currentUser.id,
          course_id: COURSE_DATA.id,
          session_duration: courseSession.totalStudyTime,
          lessons_viewed: courseSession.lessonsStarted,
          lessons_completed: courseSession.lessonsCompleted,
          interactions: courseSession.interactionCount,
          session_date: courseSession.startTime.toISOString().split("T")[0],
        };

        // Store in localStorage as backup
        localStorage.setItem(
          "course_session_backup",
          JSON.stringify(sessionData)
        );

        // Try to save to database
        logUserActivity("course_session_end", sessionData);
      }

      // =====================================================
      // COURSE DATA & PROGRESS MANAGEMENT
      // =====================================================
      async function loadCourseData() {
        try {
          // Update course info in UI
          document.getElementById("courseTitle").textContent =
            COURSE_DATA.title;
          document.getElementById("totalLessons").textContent =
            COURSE_DATA.lessons.length;

          // Log course access
          logUserActivity("course_access", {
            course_id: COURSE_DATA.id,
            course_title: COURSE_DATA.title,
          });
        } catch (error) {
          console.error("Error loading course data:", error);
        }
      }

      async function loadProgress() {
        try {
          // Get existing progress from database
          const { data, error } = await supabase
            .from("course_progress")
            .select("*")
            .eq("user_id", currentUser.id)
            .eq("course_id", COURSE_DATA.id)
            .single();

          if (error && error.code !== "PGRST116") {
            throw error;
          }

          if (data) {
            courseProgress = JSON.parse(data.lesson_progress || "{}");
            currentLessonIndex = data.current_lesson || 0;

            // Update session tracking
            courseSession.lessonsStarted = Object.keys(courseProgress).length;
            courseSession.lessonsCompleted = Object.values(
              courseProgress
            ).filter((p) => p.completed).length;
          } else {
            // Initialize new progress
            courseProgress = {};
            currentLessonIndex = 0;
            await saveProgress();

            // Award first course start achievement
            await checkAndUnlockAchievement("first_course_start");
          }

          updateProgressDisplay();
        } catch (error) {
          console.error("Error loading progress:", error);
          showToast("Failed to load progress", "error");
        }
      }

      async function saveProgress() {
        try {
          if (!currentUser) return;

          const now = new Date();
          const progressData = {
            user_id: currentUser.id,
            course_id: COURSE_DATA.id,
            course_title: COURSE_DATA.title,
            current_lesson: currentLessonIndex,
            lesson_progress: JSON.stringify(courseProgress),
            progress: calculateOverallProgress(),
            lessons_completed: getCompletedLessonsCount(),
            lessons_started: Object.keys(courseProgress).length,
            study_time_minutes: courseSession.totalStudyTime,
            last_accessed: now.toISOString(),
            updated_at: now.toISOString(),
          };

          // Upsert progress
          const { error } = await supabase
            .from("course_progress")
            .upsert([progressData]);

          if (error) throw error;

          // Update user stats
          await updateUserStats();

          updateProgressDisplay();
        } catch (error) {
          console.error("Error saving progress:", error);
          showToast("Failed to save progress", "error");
        }
      }

      async function updateUserStats() {
        try {
          // Get all course progress for this user
          const { data: allProgress, error } = await supabase
            .from("course_progress")
            .select("progress, course_id, study_time_minutes")
            .eq("user_id", currentUser.id);

          if (error) throw error;

          // Calculate stats
          const completedCourses =
            allProgress?.filter((p) => p.progress >= 100).length || 0;
          const inProgressCourses =
            allProgress?.filter((p) => p.progress > 0 && p.progress < 100)
              .length || 0;
          const totalStudyTime =
            allProgress?.reduce(
              (sum, p) => sum + (p.study_time_minutes || 0),
              0
            ) || 0;

          // Calculate points (500 per completed course + achievement points)
          const coursePoints = completedCourses * 500;
          const bonusPoints = userStats.bonus_points || 0;
          const totalPoints = coursePoints + bonusPoints;

          // Update profile
          const updateData = {
            completed_courses: completedCourses,
            in_progress_courses: inProgressCourses,
            total_points: totalPoints,
            total_study_time: totalStudyTime,
            last_activity: new Date().toISOString(),
          };

          const { error: updateError } = await supabase
            .from("profiles")
            .update(updateData)
            .eq("id", currentUser.id);

          if (updateError) throw updateError;

          // Update local stats
          Object.assign(userStats, updateData);
        } catch (error) {
          console.error("Error updating user stats:", error);
        }
      }

      function calculateOverallProgress() {
        const completedLessons = getCompletedLessonsCount();
        return Math.round(
          (completedLessons / COURSE_DATA.lessons.length) * 100
        );
      }

      function getCompletedLessonsCount() {
        return Object.values(courseProgress).filter(
          (lesson) => lesson.completed
        ).length;
      }

      function updateProgressDisplay() {
        const completedCount = getCompletedLessonsCount();
        const progressPercent = calculateOverallProgress();

        // Update UI elements
        const elements = {
          completedLessons: completedCount,
          courseProgressPercent: `${progressPercent}%`,
        };

        Object.entries(elements).forEach(([id, value]) => {
          const element = document.getElementById(id);
          if (element) element.textContent = value;
        });

        // Update progress bar
        const progressBar = document.getElementById("courseProgressFill");
        if (progressBar) progressBar.style.width = `${progressPercent}%`;
      }

      // =====================================================
      // LESSON MANAGEMENT
      // =====================================================
      function loadLesson(index) {
        if (index < 0 || index >= COURSE_DATA.lessons.length) return;

        const lesson = COURSE_DATA.lessons[index];
        currentLessonIndex = index;

        // Mark lesson as started and track activity
        if (!courseProgress[lesson.id]) {
          courseProgress[lesson.id] = {
            started: true,
            completed: false,
            startedAt: new Date().toISOString(),
            timeSpent: 0,
          };

          courseSession.lessonsStarted++;
          saveProgress();

          // Log lesson start
          logUserActivity("lesson_start", {
            lesson_id: lesson.id,
            lesson_title: lesson.title,
            lesson_index: index,
          });
        }

        // Update lesson start time for time tracking
        courseProgress[lesson.id].currentSessionStart = new Date();

        // Update sidebar and navigation
        renderSidebar();
        updateNavigationButtons();

        // Update header info
        updateLessonHeader(lesson, index);

        // Render lesson content
        renderLessonContent(lesson);

        // Smooth scroll and animations
        window.scrollTo({ top: 0, behavior: "smooth" });
        document.getElementById("contentBody").scrollTop = 0;

        const contentBody = document.getElementById("contentBody");
        contentBody.classList.add("fade-in");
        setTimeout(() => contentBody.classList.remove("fade-in"), 500);

        // Check lesson-based achievements
        checkLessonAchievements(index + 1);
      }

      function updateLessonHeader(lesson, index) {
        const elements = {
          currentLessonNumber: index + 1,
          currentLessonTitle: lesson.title,
          navInfo: `Lesson ${index + 1} of ${COURSE_DATA.lessons.length}`,
        };

        Object.entries(elements).forEach(([id, value]) => {
          const element = document.getElementById(id);
          if (element) element.textContent = value;
        });
      }

      function renderSidebar() {
        const lessonNav = document.getElementById("lessonNav");
        if (!lessonNav) return;

        lessonNav.innerHTML = "";

        COURSE_DATA.lessons.forEach((lesson, index) => {
          const lessonItem = document.createElement("div");
          lessonItem.className = "lesson-item";

          // Determine lesson status
          const lessonProgress = courseProgress[lesson.id];
          const isCompleted = lessonProgress?.completed || false;
          const isInProgress = lessonProgress?.started || false;
          const isLocked = !isCompleted && !canAccessLesson(index);
          const isActive = index === currentLessonIndex;

          // Apply status classes
          if (isCompleted) lessonItem.classList.add("completed");
          if (isLocked) lessonItem.classList.add("locked");
          if (isActive) lessonItem.classList.add("active");

          // Determine status display
          let statusClass = "not-started";
          let statusIcon = index + 1;

          if (isCompleted) {
            statusClass = "completed";
            statusIcon = "✓";
          } else if (isInProgress) {
            statusClass = "in-progress";
            statusIcon = "◐";
          }

          // Create lesson item HTML
          lessonItem.innerHTML = `
            <div class="lesson-status ${statusClass}">${statusIcon}</div>
            <div class="lesson-info">
                <div class="lesson-title">${lesson.title}</div>
                <div class="lesson-meta">
                    ${
                      isCompleted
                        ? "Completed"
                        : isInProgress
                        ? "In Progress"
                        : isLocked
                        ? "Locked"
                        : "Not Started"
                    }
                </div>
            </div>
            <div class="lesson-duration">${lesson.duration}</div>
        `;

          // Add click handler if not locked
          if (!isLocked) {
            lessonItem.addEventListener("click", () => {
              if (index !== currentLessonIndex) {
                // Track lesson time before switching
                trackLessonTime();

                currentLessonIndex = index;
                loadLesson(index);
                closeSidebar();
              }
            });
          }

          lessonNav.appendChild(lessonItem);
        });
      }

      function canAccessLesson(index) {
        if (index === 0) return true; // First lesson always accessible

        // Can access if previous lesson is completed
        const previousLessonId = COURSE_DATA.lessons[index - 1].id;
        return courseProgress[previousLessonId]?.completed || false;
      }

      function trackLessonTime() {
        const currentLesson = COURSE_DATA.lessons[currentLessonIndex];
        const lessonProgress = courseProgress[currentLesson.id];

        if (lessonProgress?.currentSessionStart) {
          const sessionTime = Math.floor(
            (new Date() - new Date(lessonProgress.currentSessionStart)) /
              1000 /
              60
          );
          lessonProgress.timeSpent =
            (lessonProgress.timeSpent || 0) + sessionTime;
          delete lessonProgress.currentSessionStart;
        }
      }

      // =====================================================
      // LESSON CONTENT RENDERING
      // =====================================================
      function renderLessonContent(lesson) {
        const contentContainer = document.getElementById("lessonContent");
        if (!contentContainer) return;

        let contentHTML = `
        <div class="content-section">
            <h2>Learning Objectives</h2>
            <ul>
                ${lesson.objectives.map((obj) => `<li>${obj}</li>`).join("")}
            </ul>
        </div>
        
        <div class="content-section">
            <h2>Overview</h2>
            <p>${lesson.content.overview}</p>
        </div>
    `;

        // Render content sections
        lesson.content.sections.forEach((section) => {
          contentHTML += `
            <div class="content-section">
                <h2>${section.title}</h2>
                ${section.content}
                ${
                  section.image
                    ? `<img src="${section.image}" alt="${section.title}" class="content-image" loading="lazy">`
                    : ""
                }
            </div>
        `;
        });

        // Render code examples
        if (
          lesson.content.codeExamples &&
          lesson.content.codeExamples.length > 0
        ) {
          contentHTML += `<div class="content-section"><h2>Code Examples</h2></div>`;

          lesson.content.codeExamples.forEach((example, index) => {
            const codeId = `code-${lesson.id}-${index}`;
            contentHTML += `
                <div class="code-section">
                    <div class="code-header">
                        <span class="code-title">${example.title}</span>
                        <button class="copy-btn" onclick="copyCode('${codeId}')">
                            <i class="fas fa-copy"></i> Copy
                        </button>
                    </div>
                    <pre class="code-block"><code id="${codeId}" class="language-${
              example.language
            }">${escapeHtml(example.code)}</code></pre>
                </div>
            `;
          });
        }

        // Render quiz
        contentHTML += renderQuiz(lesson.quiz);

        contentContainer.innerHTML = contentHTML;

        // Highlight code syntax if Prism is available
        setTimeout(() => {
          if (window.Prism) {
            Prism.highlightAll();
          }
        }, 100);
      }

      // =====================================================
      // QUIZ SYSTEM
      // =====================================================
      function renderQuiz(quiz) {
        const currentLessonProgress =
          courseProgress[COURSE_DATA.lessons[currentLessonIndex].id];
        const isCompleted = currentLessonProgress?.completed || false;

        let quizHTML = `
        <div class="quiz-section" id="quizSection">
            <div class="quiz-header">
                <h2 class="quiz-title">
                    <i class="fas fa-clipboard-check"></i>
                    Knowledge Check
                </h2>
                <p class="quiz-info">
                    Complete this quiz with ${quiz.passingScore}% or higher to unlock the next lesson.
                </p>
            </div>
    `;

        if (isCompleted) {
          const savedScore = currentLessonProgress.quizScore || 0;
          quizHTML += `
            <div class="quiz-results show">
                <div class="quiz-score pass">${savedScore}%</div>
                <div class="quiz-message">
                    <strong>Lesson Completed!</strong><br>
                    You've successfully passed this lesson's quiz.
                </div>
            </div>
        `;
        } else {
          // Render quiz questions
          quiz.questions.forEach((question, qIndex) => {
            quizHTML += `
                <div class="quiz-question ${
                  qIndex === 0 ? "active" : ""
                }" data-question="${qIndex}">
                    <div class="question-header">
                        <span class="question-number">Question ${
                          qIndex + 1
                        }</span>
                        <span class="question-progress">${qIndex + 1}/${
              quiz.questions.length
            }</span>
                    </div>
                    <div class="question-text">${question.question}</div>
                    <div class="question-options">
                        ${question.options
                          .map(
                            (option, oIndex) => `
                            <div class="option" data-option="${oIndex}" onclick="selectOption(${qIndex}, ${oIndex})">
                                <span class="option-letter">${String.fromCharCode(
                                  65 + oIndex
                                )}</span>
                                <span class="option-text">${option}</span>
                            </div>
                        `
                          )
                          .join("")}
                    </div>
                </div>
            `;
          });

          quizHTML += `
            <div class="quiz-controls">
                <button class="btn btn-secondary" id="prevQuestionBtn" onclick="previousQuestion()" disabled>
                    <i class="fas fa-chevron-left"></i>
                    Previous
                </button>
                <div>
                    <button class="btn btn-secondary" id="nextQuestionBtn" onclick="nextQuestion()" disabled>
                        Next
                        <i class="fas fa-chevron-right"></i>
                    </button>
                    <button class="btn btn-primary" id="submitQuizBtn" onclick="submitQuiz()" style="display: none;">
                        <i class="fas fa-check"></i>
                        Submit Quiz
                    </button>
                </div>
            </div>

            <div class="quiz-results" id="quizResults">
                <div class="quiz-score" id="quizScore">0%</div>
                <div class="quiz-message" id="quizMessage"></div>
                <div class="quiz-actions">
                    <button class="btn btn-primary" id="continueBtn" onclick="completeLesson()" style="display: none;">
                        <i class="fas fa-arrow-right"></i>
                        Continue to Next Lesson
                    </button>
                    <button class="btn btn-secondary" onclick="retakeQuiz()">
                        <i class="fas fa-redo"></i>
                        Retake Quiz
                    </button>
                </div>
            </div>
        `;
        }

        quizHTML += "</div>";
        return quizHTML;
      }

      // Quiz interaction functions
      function selectOption(questionIndex, optionIndex) {
        // Clear previous selections
        document
          .querySelectorAll(`[data-question="${questionIndex}"] .option`)
          .forEach((opt) => {
            opt.classList.remove("selected");
          });

        // Select current option
        const selectedOption = document.querySelector(
          `[data-question="${questionIndex}"] [data-option="${optionIndex}"]`
        );
        selectedOption.classList.add("selected");

        // Store answer
        quizState.answers[questionIndex] = optionIndex;

        // Update navigation
        const isLastQuestion =
          questionIndex ===
          COURSE_DATA.lessons[currentLessonIndex].quiz.questions.length - 1;
        if (isLastQuestion) {
          document.getElementById("submitQuizBtn").style.display =
            "inline-flex";
          document.getElementById("nextQuestionBtn").style.display = "none";
        } else {
          document.getElementById("nextQuestionBtn").disabled = false;
        }
      }

      function nextQuestion() {
        const currentQuestionEl = document.querySelector(
          ".quiz-question.active"
        );
        const nextQuestionEl = currentQuestionEl.nextElementSibling;

        if (
          nextQuestionEl &&
          nextQuestionEl.classList.contains("quiz-question")
        ) {
          currentQuestionEl.classList.remove("active");
          nextQuestionEl.classList.add("active");
          quizState.currentQuestion++;
          updateQuizNavigation();
        }
      }

      function previousQuestion() {
        const currentQuestionEl = document.querySelector(
          ".quiz-question.active"
        );
        const prevQuestionEl = currentQuestionEl.previousElementSibling;

        if (
          prevQuestionEl &&
          prevQuestionEl.classList.contains("quiz-question")
        ) {
          currentQuestionEl.classList.remove("active");
          prevQuestionEl.classList.add("active");
          quizState.currentQuestion--;
          updateQuizNavigation();
        }
      }

      function updateQuizNavigation() {
        const totalQuestions =
          COURSE_DATA.lessons[currentLessonIndex].quiz.questions.length;
        const prevBtn = document.getElementById("prevQuestionBtn");
        const nextBtn = document.getElementById("nextQuestionBtn");
        const submitBtn = document.getElementById("submitQuizBtn");

        prevBtn.disabled = quizState.currentQuestion === 0;

        const hasAnswer =
          quizState.answers[quizState.currentQuestion] !== undefined;
        const isLastQuestion = quizState.currentQuestion === totalQuestions - 1;

        if (isLastQuestion) {
          nextBtn.style.display = "none";
          submitBtn.style.display = hasAnswer ? "inline-flex" : "none";
        } else {
          nextBtn.style.display = "inline-flex";
          nextBtn.disabled = !hasAnswer;
          submitBtn.style.display = "none";
        }
      }

      async function submitQuiz() {
        const lesson = COURSE_DATA.lessons[currentLessonIndex];
        const quiz = lesson.quiz;
        let correctAnswers = 0;

        // Hide questions and controls
        document
          .querySelectorAll(".quiz-question")
          .forEach((q) => (q.style.display = "none"));
        document.querySelector(".quiz-controls").style.display = "none";

        // Calculate score and highlight answers
        quiz.questions.forEach((question, index) => {
          const userAnswer = quizState.answers[index];
          const correctAnswer = question.correct;
          const isCorrect = userAnswer === correctAnswer;

          if (isCorrect) correctAnswers++;

          // Highlight answers
          const questionEl = document.querySelector(
            `[data-question="${index}"]`
          );
          const options = questionEl.querySelectorAll(".option");

          options[correctAnswer].classList.add("correct");
          if (userAnswer !== correctAnswer && userAnswer !== undefined) {
            options[userAnswer].classList.add("incorrect");
          }
        });

        const score = Math.round(
          (correctAnswers / quiz.questions.length) * 100
        );
        const passed = score >= quiz.passingScore;

        // Update quiz results UI
        const quizScore = document.getElementById("quizScore");
        const quizMessage = document.getElementById("quizMessage");
        const quizActions = document.querySelector(".quiz-actions");

        quizScore.textContent = `${score}%`;
        quizScore.className = `quiz-score ${passed ? "pass" : "fail"}`;

        if (passed) {
          quizMessage.innerHTML = `
            <strong>Congratulations!</strong><br>
            You passed with ${score}%. You can now proceed to the next lesson.
        `;

          quizActions.innerHTML = `
            <button class="btn btn-primary" onclick="completeLesson()">
                <i class="fas fa-arrow-right"></i>
                Continue to Next Lesson
            </button>
        `;

          // Mark lesson as completed
          await markLessonComplete(score);
        } else {
          quizMessage.innerHTML = `
            <strong>Not quite there yet.</strong><br>
            You scored ${score}%. You need ${quiz.passingScore}% to pass. Review the material and try again.
        `;

          quizActions.innerHTML = `
            <button class="btn btn-secondary" onclick="retakeQuiz()">
                <i class="fas fa-redo"></i>
                Retake Quiz
            </button>
        `;
        }

        document.getElementById("quizResults").classList.add("show");

        // Log quiz completion
        logUserActivity("quiz_completed", {
          lesson_id: lesson.id,
          lesson_title: lesson.title,
          score: score,
          passed: passed,
          attempts: (courseProgress[lesson.id]?.quizAttempts || 0) + 1,
        });

        // Scroll to results
        document
          .getElementById("quizResults")
          .scrollIntoView({ behavior: "smooth" });
      }

      function retakeQuiz() {
        // Reset quiz state
        quizState = {
          currentQuestion: 0,
          answers: [],
          score: 0,
          isComplete: false,
        };

        // Track quiz retry
        const lessonId = COURSE_DATA.lessons[currentLessonIndex].id;
        if (!courseProgress[lessonId]) courseProgress[lessonId] = {};
        courseProgress[lessonId].quizAttempts =
          (courseProgress[lessonId].quizAttempts || 0) + 1;

        // Reload lesson content
        loadLesson(currentLessonIndex);

        // Scroll to quiz
        setTimeout(() => {
          document
            .getElementById("quizSection")
            .scrollIntoView({ behavior: "smooth" });
        }, 500);
      }

      async function markLessonComplete(score) {
        const lesson = COURSE_DATA.lessons[currentLessonIndex];
        const lessonId = lesson.id;

        // Track lesson completion time
        trackLessonTime();

        // Update lesson progress
        courseProgress[lessonId] = {
          ...courseProgress[lessonId],
          completed: true,
          quizScore: score,
          completedAt: new Date().toISOString(),
          finalScore: score,
        };

        // Update session tracking
        courseSession.lessonsCompleted++;

        // Save progress to database
        await saveProgress();

        // Update UI
        renderSidebar();
        updateNavigationButtons();

        // Check various achievements
        await checkLessonCompletionAchievements();
        await checkPerfectScoreAchievements(score);
        await checkStudyTimeAchievements();

        // Log lesson completion
        logUserActivity("lesson_completed", {
          lesson_id: lessonId,
          lesson_title: lesson.title,
          final_score: score,
          time_spent: courseProgress[lessonId].timeSpent || 0,
          lesson_number: currentLessonIndex + 1,
        });

        showToast("Lesson completed successfully!", "success");
      }

      async function completeLesson() {
        if (currentLessonIndex < COURSE_DATA.lessons.length - 1) {
          // Move to next lesson
          currentLessonIndex++;
          loadLesson(currentLessonIndex);
        } else {
          // Course completion
          await completeCourse();
        }
      }

      async function completeCourse() {
        try {
          const completionTime = courseSession.totalStudyTime;

          // Update course progress to 100% completed
          await supabase
            .from("course_progress")
            .update({
              progress: 100,
              completed_at: new Date().toISOString(),
              completion_time_minutes: completionTime,
            })
            .eq("user_id", currentUser.id)
            .eq("course_id", COURSE_DATA.id);

          // Award course completion achievements
          await checkAndUnlockAchievement("course_completion_1");
          await checkSpeedCompletionAchievements(completionTime);
          await checkCourseStreakAchievements();

          // Update user stats
          await updateUserStats();

          // Show completion message
          showToast(
            "Congratulations! Course completed successfully!",
            "certificate"
          );

          // Log course completion
          logUserActivity("course_completed", {
            course_id: COURSE_DATA.id,
            course_title: COURSE_DATA.title,
            completion_time_minutes: completionTime,
            total_lessons: COURSE_DATA.lessons.length,
            average_quiz_score: calculateAverageQuizScore(),
          });

          // Redirect to dashboard after delay
          setTimeout(() => {
            window.location.href = "/dashboard";
          }, 3000);
        } catch (error) {
          console.error("Error completing course:", error);
          showToast("Error marking course as complete", "error");
        }
      }

      // =====================================================
      // ACHIEVEMENT INTEGRATION SYSTEM
      // =====================================================
      async function checkAndUnlockAchievement(
        achievementId,
        skipNotification = false
      ) {
        try {
          // Prevent duplicate checks
          const cacheKey = `achievement_${achievementId}_${currentUser.id}`;
          if (sessionStorage.getItem(cacheKey)) return false;

          // Check if already unlocked
          const { data: existing, error } = await supabase
            .from("user_achievements")
            .select("id")
            .eq("user_id", currentUser.id)
            .eq("achievement_id", achievementId)
            .single();

          if (existing) {
            sessionStorage.setItem(cacheKey, "true");
            return false;
          }

          // Award achievement
          const achievementData = {
            user_id: currentUser.id,
            achievement_id: achievementId,
            unlocked_at: new Date().toISOString(),
            points_awarded: getAchievementPoints(achievementId),
            context: "course_system",
          };

          const { data, error: insertError } = await supabase
            .from("user_achievements")
            .insert([achievementData])
            .select()
            .single();

          if (insertError) {
            if (insertError.code === "23505") return false; // Already exists
            throw insertError;
          }

          // Update user points
         await supabase.rpc('increment_user_stats', {
  user_id_param: currentUser.id,
  points_to_add: achievementData.points_awarded,
  achievements_to_add: 1
});
          // Cache and queue notification
          sessionStorage.setItem(cacheKey, "true");

          if (!skipNotification) {
            queueAchievementNotification({
              id: achievementId,
              name: getAchievementName(achievementId),
              description: getAchievementDescription(achievementId),
              points: achievementData.points_awarded,
              rarity: getAchievementRarity(achievementId),
              icon: getAchievementIcon(achievementId),
            });
          }

          return true;
        } catch (error) {
          console.error("Error unlocking achievement:", error);
          return false;
        }
      }

      // Helper functions for achievement data (replace with your actual achievement definitions)
      function getAchievementPoints(id) {
        const pointsMap = {
          first_course_start: 75,
          first_lesson: 100,
          course_completion_1: 500,
          perfect_score: 250,
          speed_demon: 750,
          marathon_learner: 500,
          night_owl: 200,
          early_bird: 200,
          weekend_warrior: 150,
          // Add more as needed
        };
        return pointsMap[id] || 100;
      }

      function getAchievementName(id) {
        const nameMap = {
          first_course_start: "Learning Initiated",
          first_lesson: "First Steps",
          course_completion_1: "Course Conqueror",
          perfect_score: "Perfectionist",
          speed_demon: "Speed Demon",
          // Add more as needed
        };
        return nameMap[id] || "Achievement Unlocked";
      }

      function getAchievementDescription(id) {
        const descMap = {
          first_course_start: "Start your first cybersecurity course",
          first_lesson: "Complete your first lesson",
          course_completion_1: "Complete your first course",
          perfect_score: "Score 100% on any quiz",
          speed_demon: "Complete a course in under 2 hours",
          // Add more as needed
        };
        return descMap[id] || "Achievement description";
      }

      function getAchievementRarity(id) {
        const rarityMap = {
          first_course_start: "common",
          first_lesson: "bronze",
          course_completion_1: "silver",
          perfect_score: "gold",
          speed_demon: "legendary",
          // Add more as needed
        };
        return rarityMap[id] || "common";
      }

      function getAchievementIcon(id) {
        const iconMap = {
          first_course_start: "fas fa-play",
          first_lesson: "fas fa-baby",
          course_completion_1: "fas fa-trophy",
          perfect_score: "fas fa-star",
          speed_demon: "fas fa-rocket",
          // Add more as needed
        };
        return iconMap[id] || "fas fa-award";
      }

      async function checkLessonAchievements(lessonNumber) {
        if (lessonNumber === 1) {
          await checkAndUnlockAchievement("first_lesson");
        }

        // Check if user has completed multiple lessons in one day
        const today = new Date().toISOString().split("T")[0];
        const todayCompletions = Object.values(courseProgress).filter(
          (p) => p.completed && p.completedAt?.startsWith(today)
        ).length;

        if (todayCompletions >= 3) {
          await checkAndUnlockAchievement("lesson_marathon");
        }
      }

      async function checkLessonCompletionAchievements() {
        const completedCount = getCompletedLessonsCount();

        // Lesson-based achievements
        const lessonMilestones = [1, 5, 10, 25, 50, 100];
        for (const milestone of lessonMilestones) {
          if (completedCount >= milestone) {
            await checkAndUnlockAchievement(`lessons_${milestone}`);
          }
        }

        // Course completion achievements
        if (completedCount === COURSE_DATA.lessons.length) {
          await checkAndUnlockAchievement("course_completion_1");

          // Check for additional course completion achievements
          const { data: allCourses } = await supabase
            .from("course_progress")
            .select("course_id")
            .eq("user_id", currentUser.id)
            .eq("progress", 100);

          const totalCompleted = allCourses?.length || 0;

          const courseMilestones = [1, 5, 10, 25];
          for (const milestone of courseMilestones) {
            if (totalCompleted >= milestone) {
              await checkAndUnlockAchievement(`course_completion_${milestone}`);
            }
          }
        }
      }

      async function checkPerfectScoreAchievements(score) {
        if (score === 100) {
          await checkAndUnlockAchievement("perfect_score");

          // Check for consecutive perfect scores
          const recentScores = Object.values(courseProgress)
            .filter((p) => p.completed && p.quizScore)
            .slice(-5) // Last 5 completed
            .map((p) => p.quizScore);

          if (
            recentScores.length >= 3 &&
            recentScores.every((s) => s === 100)
          ) {
            await checkAndUnlockAchievement("perfectionist_streak");
          }
        }
      }

      async function checkSpeedCompletionAchievements(completionTime) {
        // Speed demon: Complete course in under 2 hours (120 minutes)
        if (completionTime <= 120) {
          await checkAndUnlockAchievement("speed_demon");
        }

        // Quick learner: Complete course in under 4 hours (240 minutes)
        if (completionTime <= 240) {
          await checkAndUnlockAchievement("quick_learner");
        }
      }

      async function checkStudyTimeAchievements() {
        const totalTime = courseSession.totalStudyTime;

        // Marathon learner: Study for 8+ hours in a course session
        if (totalTime >= 480) {
          // 8 hours
          await checkAndUnlockAchievement("marathon_learner");
        }

        // Dedicated learner: Study for 4+ hours
        if (totalTime >= 240) {
          // 4 hours
          await checkAndUnlockAchievement("dedicated_learner");
        }
      }

      async function checkCourseStreakAchievements() {
        try {
          // Check for consecutive course completions
          const { data: recentCourses, error } = await supabase
            .from("course_progress")
            .select("completed_at, course_id")
            .eq("user_id", currentUser.id)
            .eq("progress", 100)
            .order("completed_at", { ascending: false })
            .limit(10);

          if (error || !recentCourses) return;

          // Check for courses completed on consecutive days
          let consecutiveDays = 1;
          for (let i = 1; i < recentCourses.length; i++) {
            const prevDate = new Date(
              recentCourses[i - 1].completed_at
            ).toDateString();
            const currentDate = new Date(
              recentCourses[i].completed_at
            ).toDateString();
            const dayDiff =
              Math.abs(new Date(prevDate) - new Date(currentDate)) /
              (1000 * 60 * 60 * 24);

            if (dayDiff <= 1) {
              consecutiveDays++;
            } else {
              break;
            }
          }

          if (consecutiveDays >= 3) {
            await checkAndUnlockAchievement("learning_streak");
          }
        } catch (error) {
          console.error("Error checking course streak achievements:", error);
        }
      }

      // =====================================================
      // TIME-BASED ACHIEVEMENTS
      // =====================================================
      async function checkTimeBasedAchievements(timestamp) {
        const hour = timestamp.getHours();
        const dayOfWeek = timestamp.getDay();

        // Night owl (after midnight, before 6 AM)
        if (hour >= 0 && hour < 6) {
          await incrementTimeBasedCounter("midnight_sessions", "night_owl");
        }

        // Early bird (4 AM to 6 AM)
        if (hour >= 4 && hour < 6) {
          await incrementTimeBasedCounter("early_sessions", "early_bird");
        }

        // Weekend warrior (Saturday = 6, Sunday = 0)
        if (dayOfWeek === 0 || dayOfWeek === 6) {
          await incrementTimeBasedCounter(
            "weekend_sessions",
            "weekend_warrior"
          );
        }
      }

      async function incrementTimeBasedCounter(counterType, achievementId) {
        try {
          const dateStr = new Date().toISOString().split("T")[0];
          const cacheKey = `${counterType}_${currentUser.id}_${dateStr}`;

          // Prevent multiple increments per day
          if (sessionStorage.getItem(cacheKey)) return;

          // Update counter
       await supabase.rpc('increment_profile_counter', {
  user_id_param: currentUser.id,
  counter_field: counterType,
  increment_value: 1
});

          // Cache to prevent double counting
          sessionStorage.setItem(cacheKey, "true");

          // Check related achievement
          if (achievementId) {
            await checkAndUnlockAchievement(achievementId, true);
          }
        } catch (error) {
          console.error(`Error incrementing ${counterType}:`, error);
        }
      }

      async function checkDailyStreak() {
        try {
          const { data: profile, error } = await supabase
            .from("profiles")
            .select("last_activity, current_streak, last_streak_date")
            .eq("id", currentUser.id)
            .single();

          if (error) throw error;

          const now = new Date();
          const today = now.toDateString();
          const lastActivity = profile.last_activity
            ? new Date(profile.last_activity)
            : null;
          const lastStreakDate = profile.last_streak_date
            ? new Date(profile.last_streak_date).toDateString()
            : null;

          let newStreak = profile.current_streak || 0;
          let shouldUpdateStreak = false;

          if (!lastActivity) {
            // First time user
            newStreak = 1;
            shouldUpdateStreak = true;
          } else {
            const daysSinceLastActivity = Math.floor(
              (now - lastActivity) / (1000 * 60 * 60 * 24)
            );

            if (lastStreakDate === today) {
              // Already counted today's streak
              return newStreak;
            } else if (
              daysSinceLastActivity === 1 ||
              (daysSinceLastActivity === 0 && lastStreakDate !== today)
            ) {
              // Consecutive day or same day but not counted yet
              newStreak += 1;
              shouldUpdateStreak = true;
            } else if (daysSinceLastActivity > 1) {
              // Streak broken
              newStreak = 1;
              shouldUpdateStreak = true;
            }
          }

          if (shouldUpdateStreak) {
            await supabase
              .from("profiles")
              .update({
                current_streak: newStreak,
                last_activity: now.toISOString(),
                last_streak_date: now.toISOString(),
              })
              .eq("id", currentUser.id);

            userStats.current_streak = newStreak;

            showToast(`Daily streak: ${newStreak} days!`, "success");
            await checkStreakAchievements(newStreak);
          }

          return newStreak;
        } catch (error) {
          console.error("Error checking daily streak:", error);
          return 0;
        }
      }

      async function checkStreakAchievements(currentStreak) {
        const streakMilestones = [3, 7, 14, 30, 100, 365];

        for (const milestone of streakMilestones) {
          if (currentStreak >= milestone) {
            await checkAndUnlockAchievement(`streak_${milestone}`);
          }
        }
      }

      // =====================================================
      // USER ACTIVITY LOGGING
      // =====================================================
      async function logUserActivity(activityType, metadata = {}) {
        try {
          const now = new Date();

          const activityData = {
            user_id: currentUser.id,
            activity_type: activityType,
            timestamp: now.toISOString(),
            course_id: COURSE_DATA.id,
            lesson_index: currentLessonIndex,
            session_duration: courseSession.totalStudyTime,
            metadata: JSON.stringify(metadata),
            created_at: now.toISOString(),
          };

          // Try to log to activities table
          const { error } = await supabase
            .from("user_activities")
            .insert([activityData]);

          if (error && error.code !== "42P01") {
            console.warn("Activity logging failed:", error.message);
          }

          // Always update last activity in profile
          await supabase
            .from("profiles")
            .update({ last_activity: now.toISOString() })
            .eq("id", currentUser.id);
        } catch (error) {
          console.error("Error logging user activity:", error);
        }
      }

      // =====================================================
      // NAVIGATION SYSTEM
      // =====================================================
      function updateNavigationButtons() {
        const prevBtn = document.getElementById("prevLessonBtn");
        const nextBtn = document.getElementById("nextLessonBtn");

        if (!prevBtn || !nextBtn) return;

        // Previous button
        prevBtn.disabled = currentLessonIndex === 0;

        // Next button logic
        const isLastLesson =
          currentLessonIndex === COURSE_DATA.lessons.length - 1;
        const canGoNext =
          currentLessonIndex < COURSE_DATA.lessons.length - 1 &&
          canAccessLesson(currentLessonIndex + 1);

        if (isLastLesson) {
          const isCurrentCompleted =
            courseProgress[COURSE_DATA.lessons[currentLessonIndex].id]
              ?.completed;
          nextBtn.disabled = !isCurrentCompleted;
          nextBtn.innerHTML = '<i class="fas fa-trophy"></i> Complete Course';
        } else {
          nextBtn.disabled = !canGoNext;
          nextBtn.innerHTML = 'Next <i class="fas fa-chevron-right"></i>';
        }
      }

      // Navigation button event handlers
      function goToPreviousLesson() {
        if (currentLessonIndex > 0) {
          trackLessonTime(); // Track time spent on current lesson
          currentLessonIndex--;
          loadLesson(currentLessonIndex);
        }
      }

      function goToNextLesson() {
        if (currentLessonIndex < COURSE_DATA.lessons.length - 1) {
          const canGoNext = canAccessLesson(currentLessonIndex + 1);
          if (canGoNext) {
            trackLessonTime();
            currentLessonIndex++;
            loadLesson(currentLessonIndex);
          } else {
            showToast("Complete the current lesson quiz to proceed", "warning");
          }
        } else {
          // Complete course
          completeCourse();
        }
      }

      // =====================================================
      // SIDEBAR FUNCTIONS
      // =====================================================
      function toggleSidebar() {
        const sidebar = document.getElementById("sidebar");
        const overlay = document.getElementById("sidebarOverlay");

        if (sidebar) sidebar.classList.toggle("open");
        if (overlay) overlay.classList.toggle("active");
      }

      function closeSidebar() {
        const sidebar = document.getElementById("sidebar");
        const overlay = document.getElementById("sidebarOverlay");

        if (sidebar) sidebar.classList.remove("open");
        if (overlay) overlay.classList.remove("active");
      }

      // =====================================================
      // ACHIEVEMENT NOTIFICATION SYSTEM
      // =====================================================
      function queueAchievementNotification(achievement) {
        notificationQueue.push(achievement);
        processNotificationQueue();
      }

      function processNotificationQueue() {
        if (isShowingNotification || notificationQueue.length === 0) return;

        isShowingNotification = true;
        const achievement = notificationQueue.shift();
        showAchievementNotification(achievement);

        const duration =
          achievement.rarity === "mythic"
            ? 8000
            : achievement.rarity === "legendary"
            ? 7000
            : 6000;

        setTimeout(() => {
          isShowingNotification = false;
          processNotificationQueue();
        }, duration + 500);
      }

      function showAchievementNotification(achievement) {
        const notification = document.getElementById("achievementNotification");
        if (!notification) return;

        const icon = document.getElementById("notificationIcon");
        const title = document.getElementById("notificationTitle");
        const description = document.getElementById("notificationDescription");
        const points = document.getElementById("notificationPoints");

        if (icon) {
          icon.innerHTML = `<i class="${achievement.icon}"></i>`;
          icon.className = `notification-icon ${achievement.rarity}`;
        }
        if (title) title.textContent = achievement.name;
        if (description) description.textContent = achievement.description;
        if (points) points.textContent = `+${achievement.points} Points`;

        notification.className = `achievement-notification ${achievement.rarity}`;
        notification.classList.add("show");

        const duration =
          achievement.rarity === "mythic"
            ? 8000
            : achievement.rarity === "legendary"
            ? 7000
            : 6000;

        setTimeout(() => {
          notification.classList.remove("show");
        }, duration);
      }

      // =====================================================
      // UTILITY FUNCTIONS
      // =====================================================
      function calculateAverageQuizScore() {
        const scores = Object.values(courseProgress)
          .filter((p) => p.completed && p.quizScore)
          .map((p) => p.quizScore);

        if (scores.length === 0) return 0;
        return Math.round(
          scores.reduce((sum, score) => sum + score, 0) / scores.length
        );
      }

      async function resetProgress() {
        if (
          !confirm(
            "Are you sure you want to reset your progress? This action cannot be undone."
          )
        ) {
          return;
        }

        try {
          // Track lesson time before reset
          trackLessonTime();

          // Reset local state
          courseProgress = {};
          currentLessonIndex = 0;
          courseSession = {
            startTime: new Date(),
            totalStudyTime: 0,
            lessonsStarted: 0,
            lessonsCompleted: 0,
            pageLoadTime: new Date(),
            interactionCount: 0,
            lastActivityTime: new Date(),
          };

          // Delete from database
          await supabase
            .from("course_progress")
            .delete()
            .eq("user_id", currentUser.id)
            .eq("course_id", COURSE_DATA.id);

          // Log reset activity
          logUserActivity("progress_reset", {
            course_id: COURSE_DATA.id,
            reset_reason: "manual",
          });

          // Reinitialize
          await saveProgress();
          renderSidebar();
          loadLesson(0);

          showToast("Progress reset successfully", "success");
        } catch (error) {
          console.error("Error resetting progress:", error);
          showToast("Failed to reset progress", "error");
        }
      }

      function copyCode(codeId) {
        const codeElement = document.getElementById(codeId);
        if (!codeElement) return;

        const text = codeElement.textContent;

        navigator.clipboard
          .writeText(text)
          .then(() => {
            showToast("Code copied to clipboard!", "success");

            // Track code copy for achievements
            logUserActivity("code_copied", {
              code_section: codeId,
              lesson_id: COURSE_DATA.lessons[currentLessonIndex].id,
            });
          })
          .catch((err) => {
            console.error("Failed to copy code:", err);
            showToast("Failed to copy code", "error");

            // Fallback for older browsers
            const textArea = document.createElement("textarea");
            textArea.value = text;
            document.body.appendChild(textArea);
            textArea.select();
            try {
              document.execCommand("copy");
              showToast("Code copied to clipboard!", "success");
            } catch (fallbackError) {
              showToast(
                "Copy failed - please select and copy manually",
                "error"
              );
            }
            document.body.removeChild(textArea);
          });
      }

      function escapeHtml(text) {
        const div = document.createElement("div");
        div.textContent = text;
        return div.innerHTML;
      }

      // =====================================================
      // UI FEEDBACK SYSTEMS
      // =====================================================
      function showToast(message, type = "success") {
        const toast = document.getElementById("toast");
        const messageEl = document.getElementById("toastMessage");

        if (!toast || !messageEl) {
          console.log(`Toast: ${message} (${type})`);
          return;
        }

        messageEl.textContent = message;
        toast.className = `toast ${type} show`;

        setTimeout(() => {
          toast.classList.remove("show");
        }, 4000);
      }

      function showLoadingScreen() {
        const loadingScreen = document.getElementById("loadingScreen");
        if (loadingScreen) loadingScreen.classList.remove("hidden");
      }

      function hideLoadingScreen() {
        const loadingScreen = document.getElementById("loadingScreen");
        if (loadingScreen) {
          setTimeout(() => {
            loadingScreen.classList.add("hidden");
          }, 1000);
        }
      }

      // =====================================================
      // MUSIC PLAYER INTEGRATION
      // =====================================================
      function initMusicPlayer() {
        const btnText = document.getElementById("music-button-text");
        const icon = document.getElementById("music-icon");
        const dropdown = document.getElementById("music-dropdown-content");

        if (!btnText || !icon || !dropdown) return;

        function setUI(state) {
          btnText.textContent =
            state === "Playing" ? "PAUSE MUSIC" : "STUDY MUSIC";
          icon.className =
            state === "Playing"
              ? "fa-solid fa-circle-pause"
              : "fa-solid fa-music";
        }

        // Load saved music state
        const savedSrc = localStorage.getItem("cybersec_music_src");
        const savedTime = parseFloat(
          localStorage.getItem("cybersec_music_time") || 0
        );

        if (savedSrc) {
          audioPlayer.src = savedSrc;
          audioPlayer.currentTime = savedTime;
          audioPlayer
            .play()
            .then(() => setUI("Playing"))
            .catch(() => setUI("Stopped"));
        }

        // Music button click handler
        document
          .getElementById("music-dropdown")
          .querySelector("button")
          .addEventListener("click", (e) => {
            e.stopPropagation();
            dropdown.style.display =
              dropdown.style.display === "block" ? "none" : "block";

            if (audioPlayer.src && !audioPlayer.paused) {
              audioPlayer.pause();
              setUI("Stopped");
            } else if (audioPlayer.src) {
              audioPlayer.play().then(() => setUI("Playing"));
            }
          });

        // Music selection handlers
        dropdown.querySelectorAll("a[data-src]").forEach((link) => {
          link.addEventListener("click", (e) => {
            e.preventDefault();
            audioPlayer.src = link.dataset.src;
            audioPlayer.play().then(() => setUI("Playing"));
            localStorage.setItem("cybersec_music_src", link.dataset.src);
            dropdown.style.display = "none";
          });
        });

        // Stop music handler
        const stopLink = document.getElementById("stop-music-link");
        if (stopLink) {
          stopLink.addEventListener("click", (e) => {
            e.preventDefault();
            audioPlayer.pause();
            audioPlayer.currentTime = 0;
            audioPlayer.removeAttribute("src");
            setUI("Stopped");
            localStorage.removeItem("cybersec_music_src");
            localStorage.removeItem("cybersec_music_time");
            dropdown.style.display = "none";
          });
        }

        // Save music state before page unload
        window.addEventListener("beforeunload", () => {
          if (!audioPlayer.paused && audioPlayer.src) {
            localStorage.setItem("cybersec_music_src", audioPlayer.src);
            localStorage.setItem(
              "cybersec_music_time",
              audioPlayer.currentTime
            );
          }
        });

        // Close dropdown when clicking outside
        window.addEventListener("click", (e) => {
          if (!e.target.closest("#music-dropdown")) {
            dropdown.style.display = "none";
          }
        });
      }

      // Show initial music indicator
      function addMusicIndicator() {
        const musicBtn = document.querySelector("#music-dropdown");
        if (musicBtn) {
          const indicator = document.createElement("div");
          indicator.className = "music-indicator";
          indicator.innerHTML = `<i class="fa-solid fa-music"></i> Try Study Music!`;

          musicBtn.style.position = "relative";
          musicBtn.appendChild(indicator);

          setTimeout(() => indicator.remove(), 3000);
        }
      }

      // Initialize Event Listeners
      function initializeEventListeners() {
        // Navigation buttons
        document
          .getElementById("prevLessonBtn")
          ?.addEventListener("click", goToPreviousLesson);
        document
          .getElementById("nextLessonBtn")
          ?.addEventListener("click", goToNextLesson);

        // Reset progress button
        document
          .getElementById("resetProgressBtn")
          ?.addEventListener("click", resetProgress);

        // Mobile menu toggle
        document
          .getElementById("menuToggle")
          ?.addEventListener("click", toggleSidebar);
        document
          .getElementById("sidebarOverlay")
          ?.addEventListener("click", closeSidebar);

        // Keyboard shortcuts
        document.addEventListener("keydown", handleKeyboardShortcuts);

        // Window resize handler
        window.addEventListener("resize", handleWindowResize);

        // Content interaction tracking
        document.getElementById("contentBody")?.addEventListener(
          "scroll",
          debounce(() => {
            trackUserInteraction();
          }, 1000)
        );
      }

      // Keyboard Shortcuts
      function handleKeyboardShortcuts(e) {
        if (document.querySelector(".quiz-question.active")) return;

        if (e.key === "ArrowLeft" && e.ctrlKey) {
          e.preventDefault();
          goToPreviousLesson();
        } else if (e.key === "ArrowRight" && e.ctrlKey) {
          e.preventDefault();
          goToNextLesson();
        }
      }

      // Window Resize Handler
      function handleWindowResize() {
        if (window.innerWidth > 768) {
          closeSidebar();
        }
      }

      // Debounce Helper
      function debounce(func, wait) {
        let timeout;
        return function executedFunction(...args) {
          const later = () => {
            clearTimeout(timeout);
            func(...args);
          };
          clearTimeout(timeout);
          timeout = setTimeout(later, wait);
        };
      }

      // Initialize responsive behavior
      if (window.innerWidth <= 768) {
        document.getElementById("menuToggle").style.display = "inline-flex";
      }

      // Auto-save progress periodically
      setInterval(async () => {
        if (currentUser && Object.keys(courseProgress).length > 0) {
          await saveProgress();
        }
      }, 60000); // Save every minute

      // System initialization logging
      console.log("CyberSec Academy Course System Initialized");
      console.log("Current Course:", COURSE_DATA.title);
      console.log("Total Lessons:", COURSE_DATA.lessons.length);

      // Google OAuth Integration
      const googleBtn = document.querySelector(".btn-google");
      if (googleBtn) {
        googleBtn.addEventListener("click", async () => {
          const { data, error } = await supabase.auth.signInWithOAuth({
            provider: "google",
            options: {
              redirectTo: window.location.origin + "/courses/deepfake-detection-course",
            },
          });

          if (error) {
            console.error("Google login error:", error.message);
            showToast("Google login failed: " + error.message, "error");
          }
        });
      }

      // Check for existing session
      supabase.auth.getSession().then(({ data }) => {
        if (data.session) {
          console.log("User already logged in:", data.session.user);
          currentUser = data.session.user;
          startCourseSession();
        }
      });

      // Add missing auth modal functions
      function openAuthModal() {
        const modal = document.getElementById("authModal");
        if (modal) {
          modal.style.display = "flex";
        }
      }

      function closeAuthModal() {
        const modal = document.getElementById("authModal");
        if (modal) {
          modal.style.display = "none";
        }
      }

      // Add auth tab switching functionality
      document.getElementById("tabSignIn")?.addEventListener("click", () => {
        document.getElementById("tabSignIn").classList.add("active");
        document.getElementById("tabSignUp").classList.remove("active");
        document.getElementById("signInForm").style.display = "block";
        document.getElementById("signUpForm").style.display = "none";
      });

      document.getElementById("tabSignUp")?.addEventListener("click", () => {
        document.getElementById("tabSignUp").classList.add("active");
        document.getElementById("tabSignIn").classList.remove("active");
        document.getElementById("signUpForm").style.display = "block";
        document.getElementById("signInForm").style.display = "none";
      });

      document
        .getElementById("closeAuthModal")
        ?.addEventListener("click", closeAuthModal);

      // Improved error handling for auth session
      supabase.auth.onAuthStateChange((event, session) => {
        if (event === "SIGNED_IN") {
          currentUser = session.user;
          closeAuthModal();
          startCourseSession();
        } else if (event === "SIGNED_OUT") {
          currentUser = null;
          openAuthModal();
        }
      });

      // Add form submission handlers
      document
        .getElementById("emailSignInForm")
        ?.addEventListener("submit", async (e) => {
          e.preventDefault();
          const email = document.getElementById("signInEmail").value;
          const password = document.getElementById("signInPassword").value;

          try {
            const { data, error } = await supabase.auth.signInWithPassword({
              email,
              password,
            });

            if (error) throw error;

            closeAuthModal();
          } catch (error) {
            showToast(error.message, "error");
          }
        });

      document
        .getElementById("emailSignUpForm")
        ?.addEventListener("submit", async (e) => {
          e.preventDefault();
          const email = document.getElementById("signUpEmail").value;
          const password = document.getElementById("signUpPassword").value;
          const name = document.getElementById("signUpName").value;

          try {
            const { data, error } = await supabase.auth.signUp({
              email,
              password,
              options: {
                data: {
                  full_name: name,
                },
              },
            });

            if (error) throw error;

            showToast(
              "Account created successfully! Please check your email.",
              "success"
            );
          } catch (error) {
            showToast(error.message, "error");
          }
        });
    </script>
  </body>
</html>

